<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>hqin</title>
  
  
  <link href="https://hqin-2020.github.io/atom.xml" rel="self"/>
  
  <link href="https://hqin-2020.github.io/"/>
  <updated>2021-01-04T04:07:37.774Z</updated>
  <id>https://hqin-2020.github.io/</id>
  
  <author>
    <name>hqin</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Infinite Planning Horizon</title>
    <link href="https://hqin-2020.github.io/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Infinite-Planning-Horizon/"/>
    <id>https://hqin-2020.github.io/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Infinite-Planning-Horizon/</id>
    <published>2021-01-03T14:22:22.000Z</published>
    <updated>2021-01-04T04:07:37.774Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Methodological-Issues-of-Inifinite-Horizon"><a href="#Methodological-Issues-of-Inifinite-Horizon" class="headerlink" title="Methodological Issues of Inifinite Horizon"></a>Methodological Issues of Inifinite Horizon</h2><p>​    </p><ul><li>The convergence problem arises because the objective functional, now in the form of $\int^\infty_0F(t, y, y’)dt$, is an improper integral which may or may not have a finite value. </li><li>In the case where the integral diverges, there may exist more than one $y(t)$ path that yields an infinite value for the objective functional and it would be difficult to determine which among these paths is optimal. </li><li>There are several certain conditions that are sufficient for convergence.</li></ul><h3 id="Condition-I"><a href="#Condition-I" class="headerlink" title="Condition I"></a>Condition I</h3><p>Given the improper integral $\int^\infty_0F(t, y, y’) dt$, if the integrand $F$ is finite throughout the interval of integration, and if $F$ attains a zero value at some finite point of time, say, $t_0$, and remains at zero for all $t &gt; t_0$, then the integral will converge.</p><ul><li>Although the integral nominally has an infinite horizon, the effective upper limit of integration is a finite value, $t_0$. Thus, the given improper integral reduces in effect to a proper one, with assurance that it will integrate to a finite value.</li></ul><h3 id="Condition-III"><a href="#Condition-III" class="headerlink" title="Condition III"></a>Condition III</h3><p>In the integral $\int^\infty_0F(t,y,y’)dt$, if the integrand takes the form of $G(t,y,y’)e^{-\rho t}$, where $\rho$ is a positive rate of discount, and the $G$ function is bounded, then the integral will converge.</p><ul><li><p>A distinguishing feature of this integral is the presence of the discount factor $e^{\rho t}$ which, ceteris paribus, provides a dynamic force to drive the integrand down toward zero over time at a good speed. </p></li><li><p>When the $G(t,y,y’)$ component of the integrand is positive (as in most economic applications) and has an upper hound, say, $\hat{G}$, then the downward force of $e^\rho t$ is sufficient to make the integral converge.</p></li><li><p>More formally, since the value of the $G$ function can never exceed the value of the constant $\hat{G}$, we can write<br>$$</p><pre><code>    \int^\infty_0G(t,y,y&#39;)e^&#123;-\rho t&#125;dt\leq\int^\infty_0\hat&#123;G&#125;e^&#123;-\rho t&#125;dt=\frac&#123;\hat&#123;G&#125;&#125;&#123;\rho&#125;\tag&#123;5.3&#125;</code></pre><p>$$</p></li><li><p>It follows that the first integral must also be convergent.</p></li></ul><h3 id="The-Matter-of-Transversality-Conditions"><a href="#The-Matter-of-Transversality-Conditions" class="headerlink" title="The Matter of Transversality Conditions"></a>The Matter of Transversality Conditions</h3><p>When the planning horizon is infinite, there is no longer a specific terminal $T$ value for us to adhere to. And the terminal state may also be left open. Thus transversality conditions are needed.</p><ul><li><p>Recalling<br>$$<br>[F-y’F_{y’}]<em>{t=T}\Delta T+[F</em>{y’}]<em>{t=T}\Delta y</em>{T}=0\ \ \ \ \ \ \mbox{  [General Transversality Condition]}\tag{3.9}<br>$$</p></li><li><p>In the present context, $(3.9)$ must be modified to<br>$$</p><pre><code>    [F-y&#39;F_&#123;y&#39;&#125;]_&#123;t\to \infty&#125;\Delta T+[F_&#123;y&#39;&#125;]_&#123;t\to \infty&#125;\Delta y_&#123;T&#125;=0\tag&#123;5.4&#125;</code></pre><p>$$</p><ul><li>where each of the two terms must individually vanish.</li></ul></li></ul><p>Since there is no fixed $T$ in the present context, $\Delta T$ is perforce nonzero, and this necessitates the condition<br>$$<br>\lim_{t\to \infty}(F-y’F_{y’})=0\ \ \ \ \ \ [\mbox{Transversalilty Condition for the Infinite Horizon}]\tag{5.5}<br>$$</p><p>As to the second term in $(5.4)$, if an asymptotic terminal state is specified in the problem:<br>$$<br>\lim_{t\to \infty}y(t)=y_{\infty}=\mbox{a given constant}\tag{5.6}<br>$$</p><ul><li><p>then the second term in $(5.4)$ will vanish on its own $(\Delta y_T = 0)$ and no transversality condition is needed. </p></li><li><p>But if the terminal state is free, then we should impose the additional condition<br>$$<br>\lim_{t\to\infty} F_{y’}=0\ \ \ \ \ \ [\mbox{Transversalilty Condition for Free Terminal State}]\tag{5.7}<br>$$</p></li></ul><h2 id="The-Optimal-Social-Saving-Behavior"><a href="#The-Optimal-Social-Saving-Behavior" class="headerlink" title="The Optimal Social Saving Behavior"></a>The Optimal Social Saving Behavior</h2><h3 id="The-Ramsey-Model"><a href="#The-Ramsey-Model" class="headerlink" title="The Ramsey Model"></a>The Ramsey Model</h3><p>The central question addressed by Ramsey is that of intertemporal resource allocation: </p><ol><li>How much of the national output at any point of time should be for current consumption to yield current utility, and </li><li>how much should be saved (and invested) so as to enhance future production and consumption, and hence yield future utility?</li></ol><p>$$<br>\mbox{Maximize}\ \int^\infty_0[U(C)-D(L)]dt\tag{5.26}<br>$$</p><ul><li><p>where<br>$$</p><pre><code>C=Q(K,L)-K&#39;\tag&#123;5.25&#125;</code></pre><p>$$</p></li><li><p>since we assuming away depreciation.<br>$$<br>Q(K,L)=C+S=C+K’<br>$$</p><ul><li>This implies the output can be consumed or saved, but what is saved always results in investment and capital accumulation. </li><li>Utility function $U(C)$ has nonincreasing marginal utility, $U’’(C)\leq 0$</li></ul></li><li><p>In order to produce its consumption goods, society incurs disutility of labor $D(L)$, with nondecreasing marginal utility, $D’’(L)\geq 0$</p></li></ul><h3 id="The-Question-of-Convergence"><a href="#The-Question-of-Convergence" class="headerlink" title="The Question of Convergence"></a>The Question of Convergence</h3><p>The absence of discount factor unfortunately forfeits the opportunity to take advantage of Condition III to establish convergence, even if the integrand has an upper bound. </p><ul><li>Thus replace (5.26)​ with the following substitute problem:</li></ul><p>$$<br>\begin{array}{lll}<br>        \mbox{Minimize} &amp;\int^\infty_0[B-U(C)+D(L)]dt\<br>        \<br>        \mbox{subject to} &amp;K(0)=K_0&amp;(K_0)\ \mbox{given}<br>    \end{array}\tag{5.26’}<br>$$</p><ul><li>where $B$ (for Bliss) is a postulated maximum attainable level of net utility.</li><li>Condition I guarantees the convergence.</li></ul><h3 id="The-Solution-of-the-Model"><a href="#The-Solution-of-the-Model" class="headerlink" title="The Solution of the Model"></a>The Solution of the Model</h3><p>From (5.26’)​ we get<br>$$<br>F=B-U(C)+D(L)\ \ \ \ \ \ \ \mbox{where}\ C=Q(K,L)-K’<br>$$</p><ul><li><p>The derivative with respect to $L$ and $L’$:<br>$$<br>\begin{aligned}</p><pre><code>    &amp;F_L=-U&#39;(C)\frac&#123;\partial C&#125;&#123;\partial L&#125;+D&#39;(L)\equiv-\mu Q_L+D&#39;(L)\\    &amp;F_&#123;L&#39;&#125;=0\end&#123;aligned&#125;</code></pre><p>$$</p></li><li><p>The derivative with respect to $K$ and $K’$:<br>$$</p><pre><code>\begin&#123;aligned&#125;&amp;F_K=-U&#39;(C)\frac&#123;\partial C&#125;&#123;\partial K&#125;\equiv-\mu Q_K\\&amp;F_&#123;K&#39;&#125;=-U&#39;(C)\frac&#123;\partial C&#125;&#123;\partial K&#125;=-U&#39;(C)(-1)=\mu\end&#123;aligned&#125;</code></pre><p>$$</p></li></ul><p>According to the Euler Equation<br>$$<br>F_{y_j}-\frac{d}{dt}F_{y’_j}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.27}<br>$$</p><ul><li><p>we have<br>$$</p><pre><code>    \begin&#123;aligned&#125;        &amp;F_L-\frac&#123;dF_&#123;L&#39;&#125;&#125;&#123;dt&#125;=0\\        &amp;F_K-\frac&#123;dF_&#123;K&#39;&#125;&#125;&#123;dt&#125;=0    \end&#123;aligned&#125;</code></pre><p>$$</p></li><li><p>which means<br>$$</p><pre><code>    D&#39;(L)=\mu Q_L\ \ \ \ \mbox&#123;for all &#125; t\geq0\tag&#123;5.27&#125;</code></pre><p>$$</p><p>$$<br>\frac{d\mu/dt}{\mu}=-Q_K\ \ \ \ \mbox{for all } t\geq0\tag{5.28}<br>$$</p></li><li><p>The marginal disutility of labor must, at each point of time, be equated to the product of the marginal utility of consumption and the marginal product of labor.</p></li><li><p>The marginal utility of consumption, must at every point of time have a growth rate equal to the negative of the marginal product of capital. </p></li></ul><h3 id="A-Look-at-the-Transversality-Conditions"><a href="#A-Look-at-the-Transversality-Conditions" class="headerlink" title="A Look at the Transversality Conditions"></a>A Look at the Transversality Conditions</h3><p>$$<br>\lim_{t\to \infty}(F-y’F_{y’})=0\ \ \ \ \ \ [\mbox{Transversalilty Condition for the Infinite Horizon}]\tag{5.5}<br>$$</p><ul><li>When it is applied to two state variables, then we have </li></ul><p>$$<br>\lim_{t\to \infty}(F-L’F_{L’})=0\ \ \ \ \ \ \ \ \ and\ \ \ \ \ \ \ \         \lim_{t\to \infty}(F-K’F_{K’})=0<br>$$</p><ul><li><p>In view of the fact that $F_{L’} = 0$, the first of these conditions reduces to the condition that $F\to 0$ as $t\to\infty$. </p><ul><li>This would mean the net utility $U(C) - D(L)$ must tend to Bliss. </li><li>Note that, by itself, this condition still leaves the constant in (5.29)​ uncertain. </li><li>However, the other condition will fix the constant at zero because $F - K’F_{K’}$ is nothing but the left-hand-side expression in (5.29)​.</li></ul></li><li><p>The present problem implicitly specifies the terminal state at Bliss. Consequently, the transversality condition (5.7) is not needed.</p></li></ul><h3 id="The-Optimal-Investment-and-Capital-Paths"><a href="#The-Optimal-Investment-and-Capital-Paths" class="headerlink" title="The Optimal Investment and Capital Paths"></a>The Optimal Investment and Capital Paths</h3><p>Of greater interest to us is the optimal $K$ path and the related investment (and savings) path $K’$.</p><ul><li>Instead of deducing these from the previous results, let us find this information by taking advantage of the fact that the present problem also falls under the Special Case II: $F=F(y,y’)$, which means</li></ul><p>$$<br>F-K’F_{K’}=constant<br>$$</p><ul><li><p>or<br>$$<br>B-U(C)+D(L)-K’\mu=constant\ \ \ \ \ \ \ \ \mbox{for all } t\geq0\tag{5.29}<br>$$</p></li><li><p>This equation can be solved for $K’$ as soon as the (arbitrary) constant on the right-hand side can be assigned a specific value. </p><ul><li>To find this value, we note that this constant is to hold for all $t$, including $t\to\infty$.</li><li>We can thus make use of the fact that as $t\to\infty$, the economic objective of the model is to have $U(C) - D(L)$ tend to Bliss. </li></ul></li></ul><p>As $t\to\infty$, $B-U(C)+D(L)\to0$, $\mu\to0$,  then we have the arbitrary constant has to be $0$. </p><ul><li>Thus</li></ul><p>$$<br>    {K^*}’=\frac{B-U(C)+D(L)}{\mu}\tag{5.30}<br>$$</p><ul><li><p>With the time argument explicitly written out,<br>$$<br>{K^*}’(t)=\frac{B-U[C(t)]+D[L(t)]}{\mu(t)}\tag{5.30’}<br>$$</p></li><li><p>This result is known as the Ramsey rule. </p><ul><li>It stipulates that, optimally, the rate of capital accumulation must at any point of time be equal to the ratio of the shortfall of net utility from Bliss to the marginal utility of consumption. </li></ul></li></ul><p>From the Ramsey rule, it is possible to go one step further to find the $K^*(t)$ path by integrating (5.30’)​. </p><ul><li><p>For that, however, we need specific forms of $U(C)$ and $D(L)$ functions. </p></li><li><p>The general solution of (5.30’)​ will contain one arbitrary constant, which can be definitized by the initial condition $K(0) = K_0$. </p><ul><li>And that would complete the solution of the model. </li></ul></li></ul><h2 id="The-Concavity-Convexity-Sufficient-Condition-Again"><a href="#The-Concavity-Convexity-Sufficient-Condition-Again" class="headerlink" title="The Concavity / Convexity Sufficient Condition Again"></a>The Concavity / Convexity Sufficient Condition Again</h2><h3 id="Sufficient-Conditions"><a href="#Sufficient-Conditions" class="headerlink" title="Sufficient Conditions"></a>Sufficient Conditions</h3><p>​    </p><p>Recall that if the integrand function $F(t,y,y’)$ in a fixed-endpoint problem is concave / convex in the variables $(y, y’)$, then the Euler equation is sufficient for an absolute maximum / minimum of $V[y]$. </p><ul><li><p>Moreover, this sufficiency condition remains applicable when the terminal time is fixed but the terminal state is variable, provided that the supplementary condition<br>$$</p><pre><code>[F_&#123;y&#39;&#125;(y-y^*)]_&#123;t=&#123;T&#125;&#125;\leq 0</code></pre><p>$$</p><ul><li>is satisfied. </li></ul></li><li><p>For the infinite-horizon case, this supplementary becomes<br>$$</p><pre><code>\lim_&#123;t\to \infty&#125;[F_&#123;y&#39;&#125;(y-y^*)]\leq 0\tag&#123;5.44&#125;</code></pre><p>$$</p></li><li><p>In this condition, $F_{y’}$ is to be evaluated along the optimal path, and $(y-y^*)$ represents the deviation of any admissible neighboring path $y(t)$ from the optimal path $y^*(t)$.</p></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Methodological-Issues-of-Inifinite-Horizon&quot;&gt;&lt;a href=&quot;#Methodological-Issues-of-Inifinite-Horizon&quot; class=&quot;headerlink&quot; title=&quot;Methodol</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.4 Dynamic Optimization" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-4-Dynamic-Optimization/"/>
    
    <category term="1.4.B The Calculus of Variations" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/"/>
    
    
    <category term="Control Theory" scheme="https://hqin-2020.github.io/tags/Control-Theory/"/>
    
  </entry>
  
  <entry>
    <title>Second-Order Conditions</title>
    <link href="https://hqin-2020.github.io/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Second-Order-Conditions/"/>
    <id>https://hqin-2020.github.io/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Second-Order-Conditions/</id>
    <published>2021-01-03T14:19:22.000Z</published>
    <updated>2021-01-03T14:23:57.955Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Second-Order-Conditions"><a href="#Second-Order-Conditions" class="headerlink" title="Second-Order Conditions"></a>Second-Order Conditions</h2><h3 id="Second-Order-Necessary-Conditions"><a href="#Second-Order-Necessary-Conditions" class="headerlink" title="Second-Order Necessary Conditions"></a>Second-Order Necessary Conditions</h3><p>$$<br>\frac{d^2 V}{d\epsilon ^2}\leq 0\ \ \ \ \ \ [\mbox{for maximization of  } V]<br>$$</p><p>$$<br>\frac{d^2 V}{d\epsilon ^2}\geq 0\ \ \ \ \ \ [\mbox{for minimization of } V]<br>$$</p><h3 id="Second-Order-Sufficient-Conditions"><a href="#Second-Order-Sufficient-Conditions" class="headerlink" title="Second-Order Sufficient Conditions"></a>Second-Order Sufficient Conditions</h3><p>$$<br>\frac{d^2 V}{d\epsilon ^2}&lt; 0\ \ \ \ \ \ [\mbox{for maximization of } V]<br>$$</p><p>$$<br>\frac{d^2 V}{d\epsilon ^2}&gt; 0\ \ \ \ \ \ [\mbox{for minimization of } V]<br>$$</p><h3 id="The-Second-Derivative-of-V"><a href="#The-Second-Derivative-of-V" class="headerlink" title="The Second Derivative of $V$"></a>The Second Derivative of $V$</h3><p>Recalling the first derivative of $V$<br>$$<br>\begin{aligned}<br>        \frac{dV}{d\epsilon}&amp;=\int^T_0\frac{\partial F}{\partial \epsilon}dt=\int^T_0\left(\frac{\partial F}{\partial y}\frac{dy}{d\epsilon}+\frac{\partial F}{\partial y’}\frac{dy’}{d\epsilon}\right)dt\<br>        &amp;=\int^T_0[F_yp(t)+F_{y’}p’(t)]dt<br>        \end{aligned}\tag{2.13}<br>$$</p><ul><li><p>Since all the partial derivatives of $F(t, y, y’)$ are, like $F$ itself, functions of $t$, $y$, and $y’$</p></li><li><p>And recalling<br>$$<br>\begin{matrix}</p><pre><code>    y(t)=y^&#123;*&#125;(t)+\epsilon p(t)    &amp; \ &amp;    y&#39;(t)=&#123;y^*&#125;&#39;(t)+\epsilon p&#39;(t)    \end&#123;matrix&#125;\tag&#123;2.3&#125;</code></pre><p>$$</p><ul><li>So $y$ and $y’$ are, in turn, both functions of $\epsilon$, with derivatives<br>$$<pre><code>    \frac&#123;dy&#125;&#123;d\epsilon&#125;=p(t)\ \ \ \ and\ \ \ \ \ \ \frac&#123;dy&#39;&#125;&#123;d\epsilon&#125;=p&#39;(t)\tag&#123;4.1&#125;</code></pre>$$</li></ul></li><li><p>Thus we have<br>$$</p><pre><code>\begin&#123;aligned&#125;    \frac&#123;d^2V&#125;&#123;d\epsilon^2&#125;&amp;=\frac&#123;d&#125;&#123;d\epsilon&#125;\left(\frac&#123;dV&#125;&#123;d\epsilon&#125;\right)=\frac&#123;d&#125;&#123;d\epsilon&#125;\int^T_0[F_yp(t)]+F_&#123;y&#39;&#125;p&#39;(t)]dt    \\&amp;=\int^T_0\left[p(t)\frac&#123;d&#125;&#123;d\epsilon&#125;F_y+p&#39;(t)\frac&#123;d&#125;&#123;d\epsilon&#125;F_&#123;y&#39;&#125;\right]dt\end&#123;aligned&#125;\tag&#123;4.2&#125;</code></pre><p>$$</p><ul><li>In view of fact that</li></ul><p>$$<br>\frac{d}{d\epsilon}F_y=F_{yy}\frac{dy}{d\epsilon}+F_{yy’}\frac{dy’}{d\epsilon}=F_{yy}p(t)+F_{y’y}p’(t)<br>$$</p><ul><li><p>and similarly<br>$$</p><pre><code>\frac&#123;d&#125;&#123;d\epsilon&#125;F_&#123;y&#39;&#125;=F_&#123;yy&#39;&#125;p(t)+F_&#123;y&#39;y&#39;&#125;p&#39;(t)</code></pre><p>$$</p><ul><li>The second derivative (4.2)​ can be simplified as<br>$$<pre><code>    \frac&#123;d^2V&#125;&#123;d\epsilon^2&#125;=\int^T_0\left[F_&#123;yy&#125;p^2(t)+2F_&#123;yy&#39;&#125;p(t)p&#39;(t)+F_&#123;y&#39;y&#39;&#125;&#123;p&#39;&#125;^2(t)\right]dt\tag&#123;4.2&#39;&#125;</code></pre>$$</li></ul></li></ul></li></ul><h3 id="The-Quadratic-Form-Test"><a href="#The-Quadratic-Form-Test" class="headerlink" title="The Quadratic-Form Test"></a>The Quadratic-Form Test</h3><p>The second derivative in (4.2’)​ is a definite integral with a quadratic form as its integrand.</p><ul><li>Since $t$ spans the interval $[0, T ]$, we have, of course, not one, but an infinite number of quadratic forms in the integral.</li><li>Nevertheless, if it can be established that the quadratic form-with $F_{yy’}$, $F_{yy’}$, and $F_{y’y’}$ evaluated on the extremal-is negative definite for every $t$, then $\frac{d^2V}{d\epsilon^2} &lt; 0$, and the extremal maximizes $V$. </li><li>Similarly, positive definiteness of the quadratic form for every $t$ is sufficient for minimization of $V$.</li><li>Even if we can only establish sign semidefiniteness, we can at least have the second-order necessary conditions checked.</li></ul><p>For some reason, however, the quadratic-form test was totally ignored in the historical development of the classical calculus of variations.</p><ul><li>In a more recent development, however, the concavity / convexity of the integrand function $F$ is used in a sufficient condition.</li><li>While concavity / convexity does not per se require differentiability, it is true that if the $F$ function does possess continuous second derivatives, then its concavity / convexity can be checked by means of the sign semidefiniteness of the second-order total differential of $F$. </li><li>So the quadratic-form test definitely has a role to play in the calculus of variations.</li></ul><h2 id="The-Concavity-Convexity-Sufficient-Condition"><a href="#The-Concavity-Convexity-Sufficient-Condition" class="headerlink" title="The Concavity / Convexity Sufficient Condition"></a>The Concavity / Convexity Sufficient Condition</h2><h3 id="A-Sufficiency-Theorem-for-Fixed-Endpoint-Problems"><a href="#A-Sufficiency-Theorem-for-Fixed-Endpoint-Problems" class="headerlink" title="A Sufficiency Theorem for Fixed-Endpoint Problems"></a>A Sufficiency Theorem for Fixed-Endpoint Problems</h3><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[y]=\int^T_0 F[t,y(t),y’(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=Z\ &amp;(T,Z\ \mbox{given})<br>\end{array}\tag{2.1}<br>$$</p><p>Just as a concave / convex objective function in a static optimization prob­ lem is sufficient to identify an extremum as an absolute maximum / mini­mum, a similar sufficiency theorem holds in the calculus of variations:<br>$$<br>\begin{aligned}<br>            &amp;\mbox{For the fixed-endpoint problem } (2.1), \mbox{if the integrand function}\ F(t,y,y’)\ \mbox{is concave} \&amp;\mbox{in the variables}\ (y,y’), \mbox{then the Euler equation is sufficient for an absolute}\&amp;\mbox{maximum of}\ V[y].\mbox{ Similarly, if}\ F(t, y, y’)\ \mbox{is convex in}\ (y, y’), \mbox{then the Euler}\&amp; \mbox{equation is sufficient for an absolute minimum of}\ V[y].<br>        \end{aligned}\tag{4.3}<br>$$</p><ul><li>It should be pointed out that concavity / convexity in $(y,y’)$ means concavity / convexity in the two variables $y$ and $y’$ jointly, not in each variable separately.</li></ul><p>The proof of this theorem for the concave case.<br>$$<br>\begin{aligned}<br>            F(t,y,y’)-F(t,y^*,{y^*}’)&amp;\leq F_y(t,y^*,{y^*}’) (y-y^*)+F_{y’}(t,y^*,{y^*}’)(y’-{y^*}’)\<br>            &amp;= F_y(t,y^*,{y^*}’)\epsilon p(t)+F_{y’}(t,y^*,{y^*})’\epsilon p’(t)<br>        \end{aligned}\tag{4.4}<br>$$</p><ul><li><p>Recalling that<br>$$</p><pre><code>\begin&#123;aligned&#125;\int^T_0F_&#123;y&#39;&#125;p&#39;(t)dt&amp;=[F_&#123;y&#39;&#125;p(t)]^T_0-\int^T_0 p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt\\&amp;=-\int^T_0p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt\end&#123;aligned&#125;\tag&#123;2.16&#125;</code></pre><p>$$</p></li><li><p>And the Euler Equation<br>$$</p><pre><code>    F_y-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;=0\ \ \ \ \ \  \mbox&#123;for all&#125;\ t\in[0,T]</code></pre><p>$$</p></li><li><p>Integrate both sides of (4.4) with respect to $t$ over the interval $[0,T]$<br>$$</p><pre><code>    \begin&#123;aligned&#125;    V[y]-V[y^*]&amp;\leq \epsilon\int^T_0\left[F_y(t,y^*,&#123;y^*&#125;&#39;)p(t)+F_&#123;y&#39;&#125;(t,y^*,&#123;y^*&#125;&#39;)p&#39;(t)\right]dt\\    &amp;=\epsilon \int^T_0 p(t)\left[F_y(t,y^*,&#123;y^*&#125;&#39;)-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;(t,y^*,&#123;y^*&#125;&#39;)\right]dt\\    &amp;=0     \end&#123;aligned&#125;\tag&#123;4.5&#125;</code></pre><p>$$</p><ul><li>The first equation follows from (2.16)​, the last equation follows from the fact that $y^*(t)$ satisfies the Euler Equation ​(2.18)​</li><li>Note that if the $F$ function is strictly concave in $(y, y’)$, then the weak inequality $\leq$ in (4.4)​ and (4.5)​ will become the strict inequality $&lt;$. <ul><li>The result, $V[y] &lt; V[y^*]$, will then establish $V[y^*]$ to be a unique absolute maximum of $V$. </li></ul></li></ul></li><li><p>By the same token, a strictly convex $F$ will make $V[y^*]$ a unique absolute minimum.    </p></li></ul><h3 id="Generalization-to-Variable-Terminal-Point"><a href="#Generalization-to-Variable-Terminal-Point" class="headerlink" title="Generalization to Variable Terminal Point"></a>Generalization to Variable Terminal Point</h3><p>Recalling<br>$$<br>\begin{aligned}<br>        \int^{T}<em>0F</em>{y’}p’(t)dt&amp;=[F_{y’}p(t)]^{T}<em>0-\int^{T}<em>0 p(t)\frac{d}{dt}F</em>{y’}dt\<br>        &amp;=[F</em>{y’}]_{t={T}} p(T)-\int^{T}<em>0p(t)\frac{d}{dt}F</em>{y’}dt<br>        \end{aligned}\tag{2.16’}<br>$$</p><ul><li><p>Thus the Equation (4.5)​ becomes<br>$$<br>\begin{aligned}</p><pre><code>    V[y]-V[y^*]&amp;\leq \epsilon\int^T_0\left[F_y(t,y^*,&#123;y^*&#125;&#39;)p(t)+F_&#123;y&#39;&#125;(t,y^*,&#123;y^*&#125;&#39;)p&#39;(t)\right]dt\\    &amp;=\epsilon \int^T_0 p(t)\left[F_y(t,y^*,&#123;y^*&#125;&#39;)-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;(t,y^*,&#123;y^*&#125;&#39;)\right]dt+\epsilon[F_&#123;y&#39;&#125;p(t)]_&#123;t=&#123;T&#125;&#125;\\    &amp;=\epsilon[F_&#123;y&#39;&#125;p(t)]_&#123;t=&#123;T&#125;&#125;    \\    &amp;=[F_&#123;y&#39;&#125;(y-y^*)]_&#123;t=&#123;T&#125;&#125;\end&#123;aligned&#125;\tag&#123;4.5&#39;&#125;</code></pre><p>$$</p></li><li><p>To ensure the maximum, $F(t, y, y’)$ in (4.3)​ only needs to be supplemented in the present case by a nonpositivity condition on the expression $[F_{y’}(y-y^*)]_{t={T}}$.</p><ul><li>But this supplementary condition is automatically met when the transversality condition is satisfied for the vertical-terminal-line problem, namely, $[F_{y’}]=0$. </li><li>As for the truncated case, the transversality condition calls for either  $[F_{y’}]=0$ (when the minimum acceptable terminal value is nobinding), or $y^*=y_{\min}$ (when that terminal value is binding), thereby in effect turning the problem into one with a fixed terminal point).</li><li>Either way, the supplementary condition is met. </li><li>Thus, if the integrand function $F$ is concave / convex in the variables $(y,y’)$ in a problem with a vertical terminal lime or truncated vertical terminal line, then the Euler equation plus the transversality condition are sufficient for an absolute maximum / minimum of $V[y]$.    </li></ul></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Second-Order-Conditions&quot;&gt;&lt;a href=&quot;#Second-Order-Conditions&quot; class=&quot;headerlink&quot; title=&quot;Second-Order Conditions&quot;&gt;&lt;/a&gt;Second-Order Cond</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.4 Dynamic Optimization" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-4-Dynamic-Optimization/"/>
    
    <category term="1.4.B The Calculus of Variations" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/"/>
    
    
    <category term="Control Theory" scheme="https://hqin-2020.github.io/tags/Control-Theory/"/>
    
  </entry>
  
  <entry>
    <title>Transversality Conditions for Variable-Endpoint Problems</title>
    <link href="https://hqin-2020.github.io/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Transversality-Conditions-for-Variable-Endpoint-Problems/"/>
    <id>https://hqin-2020.github.io/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Transversality-Conditions-for-Variable-Endpoint-Problems/</id>
    <published>2021-01-03T14:17:22.000Z</published>
    <updated>2021-01-03T13:18:32.215Z</updated>
    
    <content type="html"><![CDATA[<h2 id="The-General-Transversality-Condition"><a href="#The-General-Transversality-Condition" class="headerlink" title="The General Transversality Condition"></a>The General Transversality Condition</h2><h3 id="The-Variable-Terminal-Point-Problem"><a href="#The-Variable-Terminal-Point-Problem" class="headerlink" title="The Variable-Terminal-Point Problem"></a>The Variable-Terminal-Point Problem</h3><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[y]=\int^T_0 F[t,y(t),y’(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=y_T\ &amp;(T,y_T\ \mbox{free})<br>\end{array}\tag{3.1}<br>$$</p><ul><li><p>Using the variable $\epsilon$ to express any value of $T$ in the neighborhood of $T^*$.<br>$$</p><pre><code>T=T^*+\epsilon\Delta T\tag&#123;3.2&#125;</code></pre><p>$$</p></li><li><p>Consider $T$ as a function of $\epsilon$, with derivative:<br>$$</p><pre><code>\frac&#123;dT&#125;&#123;d\epsilon&#125;=\Delta T\tag&#123;3.3&#125;</code></pre><p>$$</p></li><li><p>Generating neighboring paths of the extremal $y^*(t)$ with the same $\epsilon$<br>$$<br>\begin{matrix}</p><pre><code>    y(t)=y^&#123;*&#125;(t)+\epsilon p(t)    &amp; \mbox&#123; [implying&#125;&amp;    y&#39;(t)=&#123;y^*&#125;&#39;(t)+\epsilon p&#39;(t)]    \end&#123;matrix&#125;\tag&#123;3.4&#125;</code></pre><p>$$</p><ul><li>However, although the $p(t)$ curve must still satisfy the condition $p(0) = 0$, to force the neighboring paths to pass through the fixed·initial point, the other condition-$p(T)= 0$-should now be dropped, because $Y_T$ is free.</li></ul></li><li><p>Consider $V$ as a function of the variable $\epsilon$, the upper limit of integration in the $V$ function will also vary with $\epsilon$<br>$$<br>V(\epsilon)=\int^{T(\epsilon)}_0 F[y, y^{<em>}(t)+\epsilon p(t), {y^</em>}’(t)+\epsilon p’(t)]dt\tag{3.5}<br>$$</p></li></ul><h3 id="Deriving-the-General-Transversality-Condition"><a href="#Deriving-the-General-Transversality-Condition" class="headerlink" title="Deriving the General Transversality Condition"></a>Deriving the General Transversality Condition</h3><p>The <u>necessary condition</u>:<br>$$<br>\frac{dV}{d\epsilon}=\int^{T(\epsilon)}_0\frac{\partial F}{\partial \epsilon}dt+F[T,y(T),y’(T)]\frac{dT}{d\epsilon}=0\tag{3.6}<br>$$</p><ul><li><p>Recalling previous equations<br>$$</p><pre><code>\begin&#123;aligned&#125;\int^&#123;T&#125;_0\frac&#123;\partial F&#125;&#123;\partial \epsilon&#125;dt&amp;=\int^&#123;T&#125;_0\left(\frac&#123;\partial F&#125;&#123;\partial y&#125;\frac&#123;dy&#125;&#123;d\epsilon&#125;+\frac&#123;\partial F&#125;&#123;\partial y&#39;&#125;\frac&#123;dy&#39;&#125;&#123;d\epsilon&#125;\right)dt\\&amp;=\int^&#123;T&#125;_0[F_yp(t)+F_&#123;y&#39;&#125;p&#39;(t)]dt\end&#123;aligned&#125;\tag&#123;2.13&#125;</code></pre><p>$$</p></li><li><p>Breaking the integral into two parts<br>$$<br>\int^{T}<em>0 F_yp(t)dt+\int^{T}_0F</em>{y’}p’(t)dt=0\tag{2.14}<br>$$</p></li><li><p>Integration by parts<br>$$</p><pre><code>    \begin&#123;aligned&#125;        \int^&#123;T&#125;_0F_&#123;y&#39;&#125;p&#39;(t)dt&amp;=[F_&#123;y&#39;&#125;p(t)]^&#123;T&#125;_0-\int^&#123;T&#125;_0 p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt\\            &amp;=[F_&#123;y&#39;&#125;]_&#123;t=&#123;T&#125;&#125;p(T)-\int^&#123;T&#125;_0p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt    \end&#123;aligned&#125;\tag&#123;2.16&#39;&#125;</code></pre><p>$$</p></li><li><p>Thus we have<br>$$<br>\mbox{First term in } (3.6)=\int^{T}<em>0p(t)\left[F_y-\frac{d}{dt}F_{y’}\right]dt+[F</em>{y’}]_{t={T}}p(T)<br>$$</p></li><li><p>According to (3.3) we also have<br>$$<br>\mbox{Second term in } (3.6)=[F]_{t=T}\Delta T<br>$$</p></li><li><p>Then we can transform (3.6)​ into<br>$$</p><pre><code>\int^&#123;T&#125;_0p(t)\left[F_y-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;\right]dt+[F_&#123;y&#39;&#125;]_&#123;t=&#123;T&#125;&#125;p(T)+[F]_&#123;t=T&#125;\Delta T =0\tag&#123;3.7&#125;</code></pre><p>$$</p></li></ul><p>Of the three terms on the left-hand side of $(3. 7)$, each contains its own independent arbitrary element: </p><ul><li>$p(t)$ (the entire perturbing curve) in the first term</li><li>$p(T)$ (the terminal value on the perturbing curve) in the second term</li><li>$\Delta T$ (the arbitrarily chosen change in $T$) in the third term<ul><li>Thus we cannot presume any offsetting or cancellation of terms. </li><li>Consequently, in order to satisfy the condition (3. 7)​, each of the three terms must individually be set equal to zero.</li></ul></li></ul><p>When the first term in (3.7)​ is set equal to zero, the Euler equation emerges.<br>$$<br>F_y-\frac{d}{dt}F_{y’}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.18}<br>$$</p><ul><li>This establish the fact that the Euler equation remains valid as a necessary condition in the variable-endpoint problem.</li></ul><p>Getting rid of the arbitrary quantity $p(T)$ by transforming it into terms of $\Delta T$ and $\Delta y_T$<br>$$<br>\Delta y(T)=p(T)+y’(T)\Delta T<br>$$</p><ul><li>This can be done with the help of Fig. 3.1. </li></ul><img src="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Transversality-Conditions-for-Variable-Endpoint-Problems/Fig_3.1.png" class title="[title]"><ul><li><p>Rearranging it and get<br>$$</p><pre><code>p(T)=\Delta y_T-y&#39;(T)\Delta T\tag&#123;3.8&#125;</code></pre><p>$$</p><ul><li>The result in (3.8)​ is build on condition that $\epsilon=1$ while deriving, but it is still valid if $\epsilon$ is not set equal to one.</li></ul></li><li><p>Eliminating $p(T)$ in (3.7)​, and dropping the first term in ​(3.7)​, we get another <u>necessary condition</u><br>$$</p><pre><code>    \begin&#123;aligned&#125;        &#123;[F_&#123;y&#39;&#125;]&#125;_&#123;t=&#123;T&#125;&#125;p(T)+[F]_&#123;t=T&#125;\Delta T&amp;=[F_&#123;y&#39;&#125;]_&#123;t=&#123;T&#125;&#125;[\Delta y_T-y&#39;(T)\Delta T]+[F]_&#123;t=T&#125;\Delta T         \\        &amp;=[F_&#123;y&#39;&#125;]_&#123;t=T&#125;\Delta y_&#123;T&#125;+[F-y&#39;F_&#123;y&#39;&#125;]_&#123;t=T&#125;\Delta T    \end&#123;aligned&#125;</code></pre><p>$$</p></li><li><p>Rearranging it and get<br>$$<br>[F-y’F_{y’}]<em>{t=T}\Delta T+[F</em>{y’}]<em>{t=T}\Delta y</em>{T}=0\ \ \ \ \ \ \mbox{  [General Transversality Condition]}\tag{3.9}<br>$$</p><ul><li>This condition, unlike the Euler equation, is relevant only to one point of time, $T$.</li><li>Its role is to take the place of the missing terminal condition in the present problem. </li></ul></li></ul><h2 id="Specialized-Transversality-Conditions"><a href="#Specialized-Transversality-Conditions" class="headerlink" title="Specialized Transversality Conditions"></a>Specialized Transversality Conditions</h2><p>Depending on the exact specification of the terminal line or curve, however, the general condition (3.9) can be written in various specialized forms.</p><h3 id="Vertical-Terminal-Line"><a href="#Vertical-Terminal-Line" class="headerlink" title="Vertical Terminal Line"></a>Vertical Terminal Line</h3><p>For a fixed $T$, $\Delta T=0$, so we only need<br>$$<br>[F_{y’}]_{t=T}=0\ \ \ \ \ \ \mbox{  [Transversality Condition for Vertical Terminal Line]}\tag{3.10}<br>$$</p><h3 id="Horizontal-Terminal-Line"><a href="#Horizontal-Terminal-Line" class="headerlink" title="Horizontal Terminal Line"></a>Horizontal Terminal Line</h3><p>For a fixed $y_T$, $\Delta y_T=0$, so we only need<br>$$<br>[F-y’F_{y’}]_{t=T}=0\ \ \ \ \ \ \mbox{  [Transversality Condition for Hoizontal Terminal Line]}\tag{3.11}<br>$$</p><h3 id="Terminal-Curve"><a href="#Terminal-Curve" class="headerlink" title="Terminal Curve"></a>Terminal Curve</h3><p>With a terminal curve $y_T=\phi(T)$, we also have $\Delta y_T=\phi’\Delta T$. </p><ul><li><p>Transform (3.9)​, we have<br>$$<br>[F-y’F_{y’}+F_{y’}\phi’]_{t=T}\Delta T=0<br>$$</p></li><li><p>For an arbitrary $\Delta T$, we have</p></li></ul><p>$$<br>[F-y’F_{y’}+F_{y’}\phi’]_{t=T}=0\ \ \ \ \ \ \mbox{  [Transversality Condition for Terminal Curve]}\tag{3.12}<br>$$</p><h3 id="Truncated-Vertical-Terminal-Line"><a href="#Truncated-Vertical-Terminal-Line" class="headerlink" title="Truncated Vertical Terminal Line"></a>Truncated Vertical Terminal Line</h3><p>The usual case ofvertical terminal line, with $\Delta T =0$, specializes (3.9) to<br>$$<br>[F_{y’}]<em>{t=T}\Delta</em>{y_T}=0<br>$$</p><ul><li>When the line is truncated-restricted by the terminal condition $y_T&gt;y_{\min}$ where $y_{\min}$ is a minimum permissible level of $y$<ul><li>the optimal solution can have two possible types of outcome: $y_T^*&gt;y_{\min}$ or  $y_T^*=y_{\min}$ </li></ul></li></ul><p>For $y_T^*&gt;y_{\min}$, the constraint is not binding, we have<br>$$<br>[F_{y’}]<em>{t=T}=0\ \ \ for\ \ \ y_T^*&gt;y_{\min}\tag{3.14}<br>$$<br>For $y_T^*=y_{min}$, the constraint is binding, with Kuhn-Tucker condition, we have<br>$$<br>[F</em>{y’}]<em>{t=T}\leq0\ \ \ for\ \ \ y_T^*=y_{min}\tag{3.16}<br>$$<br>Combing (3.14)​ and (3.16)​ we have<br>$$<br>[F</em>{y’}]<em>{t=T}\leq0\ \ \ for\ \ \ y_T^<em>\geq y_{min}\ \ \ \ \ \ \ (y_T^</em>-y_{min})[F</em>{y’}]_{t=T}=0\   \ \ \mbox{ [Transversality Condition for Truncated Vertical Terminal Line in the Maximization Problem]}\tag{3.17}<br>$$</p><p>$$<br>[F_{y’}]<em>{t=T}\geq0\ \ \ for\ \ \ y_T^<em>\geq y_{min}\ \ \ \ \ \ \ (y_T^</em>-y_{min})[F</em>{y’}]_{t=T}=0\   \ \ \mbox{[Transversality Condition for Truncated Vertical Terminal Line in the Minimization Problem]}\tag{3.18}<br>$$</p><h2 id="Three-Generalizations"><a href="#Three-Generalizations" class="headerlink" title="Three Generalizations"></a>Three Generalizations</h2><h3 id="The-Case-of-Several-State-Variables"><a href="#The-Case-of-Several-State-Variables" class="headerlink" title="The Case of Several State Variables"></a>The Case of Several State Variables</h3><p>$$<br>\left[F-(y_1’F_{y_1’}+\cdots+y_n’F_{y_n’})\right]<em>{t=T}\Delta T+\left[F</em>{y_1’}\right]<em>{t=T}\Delta</em>{y_{1T}}+\cdots+\left[F_{y_n’}\right]<em>{t=T}\Delta y</em>{nT}=0\   \ \ \mbox{[The General (Terminal) Transversality Condition]}\tag{3.27}<br>$$</p><ul><li>When $n=2$<br>$$<br>\left[F-(y’F_{y’}+z’F_{z’})\right]<em>{t=T}\Delta T+\left[F</em>{y’}\right]<em>{t=T}\Delta</em>{y_{T}}+\left[F_{z’}\right]<em>{t=T}\Delta z</em>{T}=0\tag{3.27’}<br>$$</li></ul><h3 id="The-Case-of-Higher-Derivatives"><a href="#The-Case-of-Higher-Derivatives" class="headerlink" title="The Case of Higher Derivatives"></a>The Case of Higher Derivatives</h3><p>The general transversality condition for the case of $F(t,y’,y’’,y’’’)$ is<br>$$<br>\begin{aligned}<br>            &amp;\left[F-y’F_{y’}-y’’F_{y’’}+y’\frac{d}{dt}F_{y’’}\right]<em>{t=T}\Delta T\&amp;+\left[F</em>{y’}-\frac{d}{dt}F_{y’’}\right]<em>{t=T}\Delta y_T+[F_{y’’}]</em>{t=T}\Delta y’_{T}=0<br>        \end{aligned}\tag{3.28}<br>$$</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;The-General-Transversality-Condition&quot;&gt;&lt;a href=&quot;#The-General-Transversality-Condition&quot; class=&quot;headerlink&quot; title=&quot;The General Transver</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.4 Dynamic Optimization" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-4-Dynamic-Optimization/"/>
    
    <category term="1.4.B The Calculus of Variations" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/"/>
    
    
    <category term="Control Theory" scheme="https://hqin-2020.github.io/tags/Control-Theory/"/>
    
  </entry>
  
  <entry>
    <title>The Fundamental Problem of the Calculus of Variations</title>
    <link href="https://hqin-2020.github.io/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/The-Fundamental-Problem-of-the-Calculus-of-Variations/"/>
    <id>https://hqin-2020.github.io/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/The-Fundamental-Problem-of-the-Calculus-of-Variations/</id>
    <published>2021-01-03T14:17:21.000Z</published>
    <updated>2021-01-03T13:17:32.345Z</updated>
    
    <content type="html"><![CDATA[<h2 id="The-Fundamental-Problem"><a href="#The-Fundamental-Problem" class="headerlink" title="The Fundamental Problem"></a>The Fundamental Problem</h2><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[y]=\int^T_0 F[t,y(t),y’(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=Z\ &amp;(T,Z\ \mbox{given})<br>\end{array}\tag{2.1}<br>$$</p><ul><li>The maximization and minimization problems differ from each other in the second-order conditions, but they share the same first-order conditions.</li></ul><p>The task of variational calculus is to select from a set of admissible $y$ paths (or trajectories) the one that yields an extreme value of $V[y]$.</p><ul><li><p>Since the calculus of variations is based on the classical methods of calculus, requiring the use of first and second derivatives, we shall restrict the set of admissible paths to those continuous curves with continuous derivatives.</p></li><li><p>A smooth $y$ path that yields an extremum of $V[y]$ is called an <strong>extremal</strong>.</p></li><li><p>We shall also assume that the integrand function $F$ is twice differentiable.</p></li></ul><h2 id="The-Euler-Equation"><a href="#The-Euler-Equation" class="headerlink" title="The Euler Equation"></a>The Euler Equation</h2><h3 id="Differentiating-a-Definite-Integral"><a href="#Differentiating-a-Definite-Integral" class="headerlink" title="Differentiating a Definite Integral"></a>Differentiating a Definite Integral</h3><p> Considering the definite integral<br>$$<br>I(x)\equiv \int^b_aF(t,x)dt<br>$$</p><ul><li>We have the Leibnniz’s Rule</li></ul><p>$$<br>\frac{dI}{dx}=\int^b_aF_x(t,x)dt<br>$$</p><p>Considering another definite integral<br>$$<br>K(x)\equiv\int^{b(x)}_{a(x)}F(t,x)dt<br>$$</p><ul><li>We have </li></ul><p>$$<br>\frac{dK}{dx}=\int^{b(x)}_{a(x)}F_x(t,x)dt+F[b(x),x]b’(x)-F[a(x),x]a’(x)<br>$$</p><h3 id="Development-of-the-Euler-Equation"><a href="#Development-of-the-Euler-Equation" class="headerlink" title="Development of the Euler Equation"></a>Development of the Euler Equation</h3><p>With reference to Fig. 2.1, let the solid path $y^*(t)$ be a known extremal. </p><ul><li>We seek to find some property of the extremal that is absent in the (nonextremal) neighboring paths. </li><li>Such a property would constitute a necessary condition for an extremal. </li><li>To do this, we need for comparison purposes a family of neighboring paths which, by specification in (2.1), must pass through the given endpoints $(0, A)$ and $(T, Z)$. </li><li>A simple way of generating such neighboring paths is by using a <strong>perturbing curve</strong>, chosen arbitrarily except for the restrictions that it be smooth and pass through the points $0$ and $T$ on the horizontal axis in Fig. 2.1.</li></ul><img src="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/The-Fundamental-Problem-of-the-Calculus-of-Variations/Fig_2.1.png" class title="[title]"><p>Consider a perturbing curve:<br>$$<br>p(0)=p(T)=0\tag{2.2}<br>$$<br>Using the perturbing curve, we can generate neighboring paths of the extremal $y^*(t)$:<br>$$<br>\begin{matrix}<br>        y(t)=y^{<em>}(t)+\epsilon p(t)<br>        &amp; \mbox{ [implying}&amp;<br>        y’(t)={y^</em>}’(t)+\epsilon p’(t)]<br>        \end{matrix}\tag{2.3}<br>$$</p><ul><li>Instead of considering $V$ as a functional of $y$ path, now consider it as <u>a function of the variable $\epsilon$</u></li></ul><p>$$<br>V(\epsilon)=\int^T_0 F[y, y^{<em>}(t)+\epsilon p(t), {y^</em>}’(t)+\epsilon p’(t)]dt\tag{2.12}<br>$$</p><p>The <u>necessary condition</u> for the extremal is:<br>$$<br>\frac{dV}{d\epsilon}\bigg\vert_{\epsilon=0}=0\tag{2.4}<br>$$</p><ul><li><p>Then we have<br>$$<br>\begin{aligned}</p><pre><code>    \frac&#123;dV&#125;&#123;d\epsilon&#125;&amp;=\int^T_0\frac&#123;\partial F&#125;&#123;\partial \epsilon&#125;dt=\int^T_0\left(\frac&#123;\partial F&#125;&#123;\partial y&#125;\frac&#123;dy&#125;&#123;d\epsilon&#125;+\frac&#123;\partial F&#125;&#123;\partial y&#39;&#125;\frac&#123;dy&#39;&#125;&#123;d\epsilon&#125;\right)dt\\    &amp;=\int^T_0[F_yp(t)+F_&#123;y&#39;&#125;p&#39;(t)]dt    \end&#123;aligned&#125;\tag&#123;2.13&#125;</code></pre><p>$$</p></li><li><p>Breaking the integral into two parts<br>$$<br>\int^T_0 F_yp(t)dt+\int^T_0F_{y’}p’(t)dt=0\tag{2.14}<br>$$</p><ul><li>While this form of necessary condition is already <u>free of the arbitrary variable</u> $\epsilon$, the arbitrary perturbing curve $p(t)$ is still present along with its derivative $p’(t)$. </li><li>To make the necessary condition fully operational, we must also eliminate $p(t)$ and $p’(t)$.</li></ul></li><li><p>Integration by parts<br>$$<br>\begin{aligned}</p><pre><code>        \int^T_0F_&#123;y&#39;&#125;p&#39;(t)dt&amp;=[F_&#123;y&#39;&#125;p(t)]^T_0-\int^T_0 p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt\\        &amp;=-\int^T_0p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt    \end&#123;aligned&#125;\tag&#123;2.16&#125;</code></pre><p>$$</p></li><li><p>Another version of necessary condition for the extremal:<br>$$<br>\int^T_0 p(t)\left[F_y-\frac{d}{dt}F_{y’}\right]dt=0\tag{2.17}<br>$$</p><ul><li><u>Precisely because $p(t)$ enters in an arbitrary way</u>, we may conclude that the condition $(2.17)$ can only be satisfied only if the bracketed expression is made to vanish for every value of $t$ on the extremal.</li></ul></li><li><p>Consequently, it is a necessary condition for an extremal that<br>$$<br>F_y-\frac{d}{dt}F_{y’}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.18}<br>$$</p><ul><li><p>Break the second-order derivative into three parts:<br>$$</p><pre><code>    \frac&#123;dF_&#123;y&#39;&#125;&#125;&#123;dt&#125;=\frac&#123;\partial F_&#123;y&#39;&#125;&#125;&#123;\partial t&#125;+\frac&#123;\partial F_&#123;y&#39;&#125;&#125;&#123;\partial y&#125;\frac&#123;dy&#125;&#123;dt&#125;+\frac&#123;\partial F_&#123;y&#39;&#125;&#125;&#123;\partial y&#39;&#125;\frac&#123;dy&#39;&#125;&#123;dt&#125;=F_&#123;ty&#39;&#125;+F_&#123;yy&#39;&#125;y&#39;(t)+F_&#123;y&#39;y&#39;&#125;y&#39;&#39;(t)</code></pre><p>$$</p></li><li><p>We get the expanded version of Euler Equation:<br>$$<br>F_{y’y’}y’’(t)+F_{yy’}y’(t)+F_{ty’}-F_y=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.19}<br>$$</p><ul><li>The Euler equation is in general a second-order nonlinear differential equation. Its general solution will thus contain two arbitrary constants.</li><li>Since our problem in (2.1) comes with two boundary conditions (one initial and one terminal), we should normally possess sufficient information to definitize the two arbitrary constants and obtain the definite solution.</li></ul></li></ul></li></ul><h2 id="Some-Special-Cases"><a href="#Some-Special-Cases" class="headerlink" title="Some Special Cases"></a>Some Special Cases</h2><h3 id="Special-Case-I-F-F-t-y’"><a href="#Special-Case-I-F-F-t-y’" class="headerlink" title="Special Case I: $F=F(t,y’)$"></a>Special Case I: $F=F(t,y’)$</h3><p>$$<br>F_y-\frac{d}{dt}F_{y’}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.18}<br>$$</p><ul><li>In this special case, the $F$ function is free of $y$, implying that $F_y=0$. Hence the Euler equation reduces  to $\frac{dF_{y’}}{dt}=0$, Hence the solution is<br>$$<pre><code>F_&#123;y&#39;&#125;=\mbox&#123;constant&#125;\tag&#123;2.20&#125;</code></pre>$$</li></ul><h3 id="Special-Case-II-F-F-y-y’"><a href="#Special-Case-II-F-F-y-y’" class="headerlink" title="Special Case II: $F=F(y,y’)$"></a>Special Case II: $F=F(y,y’)$</h3><p>$$<br>F_{y’y’}y’’(t)+F_{yy’}y’(t)+F_{ty’}-F_y=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.19}<br>$$</p><ul><li><p>Since $F$ is free of $t$ in this case,  we have $F_{ty’}=0$, so the Euler equations reduces to<br>$$<br> F_{y’y’}y’’(t)+F_{yy’}y’(t)-F_y=0<br>$$</p></li><li><p>Note that<br>$$</p><pre><code>  \begin&#123;aligned&#125;      \frac&#123;d&#125;&#123;dt&#125;(y&#39;F_&#123;y&#39;&#125;-F)&amp;=\frac&#123;d&#125;&#123;dt&#125;(y&#39;F_&#123;y&#39;&#125;)-\frac&#123;d&#125;&#123;dt&#125;F(y,y&#39;)\\      &amp;=F_&#123;y&#39;&#125;&#123;y&#39;&#39;&#125;+y&#39;(F_&#123;yy&#39;&#125;y&#39;+F_&#123;y&#39;y&#39;&#125;y&#39;&#39;)-(F_yy&#39;+F_&#123;y&#39;&#125;y&#39;&#39;)\\      &amp;=y&#39;(F_&#123;y&#39;y&#39;&#125;y&#39;&#39;+F_&#123;yy&#39;&#125;y&#39;-F_y)  \end&#123;aligned&#125;</code></pre><p>$$</p></li><li><p>So the Euler equation can be written as $\frac{d}{dt}(y’F_{y’}-F)=0$, which means<br>$$<br>F-y’F_{y’}=\mbox{constant}\tag{2.21}<br>$$</p></li></ul><h3 id="Special-Case-III-F-F-y’"><a href="#Special-Case-III-F-F-y’" class="headerlink" title="Special Case III: $F=F(y’)$"></a>Special Case III: $F=F(y’)$</h3><p>$$<br>F_{y’y’}y’’(t)+F_{yy’}y’(t)+F_{ty’}-F_y=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.19}<br>$$</p><ul><li><p>When $F$ depends on $y’$ alone, $F_{yy’}=F_{ty’}=F_y=0$. And the Euler equation reduces to<br>$$</p><pre><code>F_&#123;y&#39;y&#39;&#125;y&#39;&#39;(t)=0\tag&#123;2.24&#125;</code></pre><p>$$</p></li><li><p>If $y’’(t)=0$, then $y’(t)=c_1$ and $y(t)=c_1t+c_2$.</p></li><li><p>If $F_{y’y’}=0$, then since $F_{y’y’}$ is, like $F$ itself, a function of $y’$ alone, the solution of $F_{y’y’}=0$ should appear as specific values pf $y’$. </p><ul><li>Suppose there are one or more real solutions $y’=k_i$, then we can deduce that $y=k_it+c$, which again represents a family of straight lines.</li><li>Consequently, given an integrand function that depends on $y’$ alone, we can always take its extermal to be a straight line. </li></ul></li></ul><h3 id="Special-Case-IV-F-F-t-y"><a href="#Special-Case-IV-F-F-t-y" class="headerlink" title="Special Case IV: $F=F(t,y)$"></a>Special Case IV: $F=F(t,y)$</h3><p>$$<br>F_y-\frac{d}{dt}F_{y’}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.18}<br>$$</p><ul><li><p>Since $y’$ is missing from the $F$ function, we have $F_{y’}=0$, the Euler equation reduces to<br>$$</p><pre><code>    F_y(t,y)=0</code></pre><p>$$</p></li><li><p>The fact that the derivative $y’$ does not appear in this equation means that the Euler equation is not a differential equation.</p></li><li><p>Since there are no arbitrary constraints in its solution to be definitized in accordance with the given boundary conditions, the extremal may not satisfy the boundary conditions except by sheer coincidence.</p></li><li><p>Thus, $F(t,y)$ is, in a special sense, “linear” in $y’$.</p></li></ul><h2 id="Two-generalizations-of-the-Euler-Euqation"><a href="#Two-generalizations-of-the-Euler-Euqation" class="headerlink" title="Two generalizations of the Euler Euqation"></a>Two generalizations of the Euler Euqation</h2><h3 id="The-Case-of-Several-State-Variables"><a href="#The-Case-of-Several-State-Variables" class="headerlink" title="The Case of Several State Variables"></a>The Case of Several State Variables</h3><p>The objective functional with $n&gt;1$ state variables<br>$$<br>V[y_1,…,y_n]=\int^T_0 F(t,y_1,…,y_n,y_1’,…,y_n’)dt\tag{2.26}<br>$$</p><ul><li><p>Then we have<br>$$<br>F_{y_j}-\frac{d}{dt}F_{y’_j}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.27}<br>$$</p><ul><li>When $n=2$, we have<br>$$<br>\begin{aligned}<pre><code>    F_&#123;y&#39;y&#39;&#125;y&#39;&#39;(t)+F_&#123;z&#39;y&#39;&#125;z&#39;&#39;(t)+F_&#123;yy&#39;&#125;y&#39;(t)+F_&#123;zy&#39;&#125;z&#39;(t)+F_&#123;ty&#39;&#125;-F_y=0\\    F_&#123;y&#39;z&#39;&#125;y&#39;&#39;(t)+F_&#123;z&#39;z&#39;&#125;z&#39;&#39;(t)+F_&#123;yz&#39;&#125;y&#39;(t)+F_&#123;zz&#39;&#125;z&#39;(t)+F_&#123;tz&#39;&#125;-F_z=0\\&amp; \mbox&#123;for all&#125;\ t\in[0,T]    \end&#123;aligned&#125;\tag&#123;2.28&#125;</code></pre>$$</li></ul></li></ul><h3 id="The-Case-of-Higher-Order-Derivatives"><a href="#The-Case-of-Higher-Order-Derivatives" class="headerlink" title="The Case of Higher-Order Derivatives"></a>The Case of Higher-Order Derivatives</h3><p>The objective functional contains high-order derivatives<br>$$<br>V[y]=\int^T_0 F(t,y,y’’,…,y^{(n)})dt\tag{2.29}<br>$$</p><ul><li>Then we have<br>$$<br>F_y-\frac{d}{dt}F_{y’}+\frac{d^2}{dt^2}F_{y’}-\cdots+(-1)^n\frac{d^n}{dt^n}F_{y^{(n)}}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler-Poisson Equation]}    \tag{2.30}<br>$$</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;The-Fundamental-Problem&quot;&gt;&lt;a href=&quot;#The-Fundamental-Problem&quot; class=&quot;headerlink&quot; title=&quot;The Fundamental Problem&quot;&gt;&lt;/a&gt;The Fundamental P</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.4 Dynamic Optimization" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-4-Dynamic-Optimization/"/>
    
    <category term="1.4.B The Calculus of Variations" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/"/>
    
    
    <category term="Control Theory" scheme="https://hqin-2020.github.io/tags/Control-Theory/"/>
    
  </entry>
  
  <entry>
    <title>The Nature of Dynamic Optimization</title>
    <link href="https://hqin-2020.github.io/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/"/>
    <id>https://hqin-2020.github.io/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/</id>
    <published>2021-01-02T14:17:12.000Z</published>
    <updated>2021-01-02T15:59:56.877Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Salient-Features-of-Dynamic-Optimization-Problems"><a href="#Salient-Features-of-Dynamic-Optimization-Problems" class="headerlink" title="Salient Features of Dynamic Optimization Problems"></a>Salient Features of Dynamic Optimization Problems</h2><p>Dynamic optimization can be viewed as a problem of multistage decision making.</p><h3 id="The-Discrete-Variable-Version"><a href="#The-Discrete-Variable-Version" class="headerlink" title="The Discrete-Variable Version"></a>The Discrete-Variable Version</h3><p>Suppose that a firm engages in transforming a certain substance from an initial state $A$ (raw material state) into a termi­nal state $Z$ (finished product state) through a five-stage production process. </p><ul><li>In every stage, the firm faces the problem of choosing among several possible alternative subprocesses, each entailing a specific cost. </li><li>The ques­tion is: How should the firm select the sequence of subprocesses through the five stages in order to minimize the total cost?</li></ul><p>In Fig. 1.1, our problem is transformed to choose a connected sequence of arcs going from left to right, starting at $A$ and terminating at $Z$, such that the sum of the values of the component arcs is minimized. </p><img src="/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/Fig_1.1.png" class title="[title]"><ul><li>The optimal solution for the present example is the path $ACEHJZ$, with $14 as the minimum cost of production. </li></ul><h3 id="The-Continuous-Variable-Version"><a href="#The-Continuous-Variable-Version" class="headerlink" title="The Continuous-Variable Version"></a>The Continuous-Variable Version</h3><p>We can visualize Fig. 1.2 to be a map of an open terrain, with the stage variable representing the longitude, and the state variable representing the latitude. </p><ul><li>Our assigned task is to transport a load of cargo from location $A$ to location $Z$ at minimum cost by selecting an appropriate travel path. </li><li>The cost associated with each possible path de­pends, in general, not only on the distance traveled, but also on the topography on that path. </li></ul><img src="/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/Fig_1.2.png" class title="[title]"><p>Regardless of whether the variables are discrete or continuous, a simple type of dynamic optimization problem would contain the following basic ingredients:</p><ol><li>a given <strong>initial point</strong> and a given <strong>terminal point</strong>;</li><li>a set of <strong>admissible paths</strong> from the initial point to the terminal point;</li><li>a set of <strong>path values</strong> serving as performance indices (cost, profit, etc.) associated with the various paths; and</li><li>a specified objective-either to maximize or to minimize the path value or performance index by choosing the <strong>optimal path</strong>.<ul><li>For most of the problems, the stage variable will represent time; then the optimal path is actually optimal <strong>time path</strong>.</li></ul></li></ol><h3 id="The-Concept-of-a-Functional"><a href="#The-Concept-of-a-Functional" class="headerlink" title="The Concept of a Functional"></a>The Concept of a Functional</h3><p>The relationship between paths and path values represents a special sort of mapping</p><ul><li>not a mapping from real numbers to real numbers as in the usual function</li><li>but a mapping from <u>paths (curves)</u> to <u>real numbers</u> (performance indices). </li></ul><p>Let us denote time paths by $y_I(t), y_{II}(t)$, and so on. Then the mapping is as shown in Fig. 1.3, where $V_I$, $V_{II}$ represent the associated path values. </p><ul><li>The general notation for the mapping should there­ fore be $V[y(t)]$. But this symbol fundamentally differs from the composite-function symbol $g[f(x)]$. </li><li>In the latter, $g$ is a function of $f$, and $f$ is in turn a function of $x$; thus, $g$ is in the final analysis a function of $x$. </li><li>In the symbol $V[y(t)]$, on the other hand, the $y(t)$ component comes as an integral unit-to indicate time paths-and there­fore $V$ is actually a <strong>functional</strong> of $y(t)$</li></ul><img src="/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/Fig_1.3.png" class title="[title]"><h2 id="Variable-Endpoints-and-Transversality-Conditions"><a href="#Variable-Endpoints-and-Transversality-Conditions" class="headerlink" title="Variable Endpoints and Transversality Conditions"></a>Variable Endpoints and Transversality Conditions</h2><h3 id="Types-of-Variable-Terminal-Points"><a href="#Types-of-Variable-Terminal-Points" class="headerlink" title="Types of Variable Terminal Points"></a>Types of Variable Terminal Points</h3><p>In Fig. 1.5a, while the planning horizon is fixed at time $T$, any point on the vertical line $t = T$ is acceptable as a terminal point, such as $Z_1$, $Z_2$, and $Z_3$• </p><ul><li>This type of problem is commonly referred to in the literature as a <strong>fixed-time-horizon problem</strong>, <strong>fixed-time problem</strong>, or the <strong>vertical-terminal-line</strong> problem, meaning that the termi­nal time of the problem is fixed rather than free. </li></ul><p>In Fig. 1.5b, the horizontal line $y = Z$ constitutes the set of admissible terminal points. Each of these, depending on the path chosen, may be associated with a different terminal time, as exemplified by $T_1$, $T_2$, and $T_3$•</p><ul><li>This type of problem is commonly referred to as a <strong>fixed-endpoint problem</strong>, or the <strong>horizontal-terminal-line problem</strong>.</li></ul><p>In the third type, neither the termi­nal time $T$ nor the terminal state $Z$ is individually preset, but the two are tied together via a constraint equation of the form $Z = \phi(T)$, as illustrated in Fig. 1.5c.</p><ul><li>We shall call this type of problem the <strong>terminal-curve</strong> (or <strong>terminal-surface</strong>) <strong>problem</strong> .</li></ul><img src="/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/Fig_1.5.png" class title="[title]"><h3 id="Transversality-Condition"><a href="#Transversality-Condition" class="headerlink" title="Transversality Condition"></a>Transversality Condition</h3><p>The common feature of <strong>variable-terminal-point</strong> problems is that the planner has <u>one more degree of freedom</u> than in the <strong>fixed-terminal-point</strong> case. </p><ul><li><p>But this fact automatically implies that, in deriving the optimal solution, <u>an extra condition is needed to pinpoint the exact path chosen</u>. </p></li><li><p>In the former, the optimal path must satisfy the boundary (initial and terminal) conditions<br>$$<br>y(0) =A\mbox{ and }y(T)=Z\ \ \ \  (T,A, \mbox{ and }Z \mbox{ all given})<br>$$</p></li><li><p>In the latter case, the initial condition $y(O)=A$ still applies by assumption.</p></li><li><p>But since $T$ and / or $Z$ are now variable, the terminal condition $y(T) = Z$ is no longer capable of pinpointing the optimal path for us. </p></li><li><p>As Fig. 1.5 shows, all admissible paths, ending at $Z_1, Z_2$, or other possible terminal positions, equally satisfy the condition $y(T) = Z$.</p></li></ul><p>What is needed, therefore, is a terminal condition that can conclusively distinguish the optimal path from the other admissible paths. </p><ul><li>Such a condition is referred to as a <strong>transversal­ity condition</strong>, because it normally appears as a description of <u>how the optimal path crosses the terminal line or the terminal curve</u> (to “transverse” means to “to go across”).</li></ul><h2 id="The-Objective-Functional"><a href="#The-Objective-Functional" class="headerlink" title="The Objective Functional"></a>The Objective Functional</h2><h3 id="The-Integral-Form-of-Functional"><a href="#The-Integral-Form-of-Functional" class="headerlink" title="The Integral Form of Functional"></a>The Integral Form of Functional</h3><p><strong>The standard form with one state variable</strong>:<br>$$<br>V[y]=\int^T_0 F[t,y(t),y’(t)]dt<br>$$</p><p><strong>The standard form with two state variables</strong>:<br>$$<br>V[y,z]=\int^T_0 F[t,y(t),z(t),y’(t),z’(t)]dt<br>$$</p><h3 id="Other-Forms-of-Functional"><a href="#Other-Forms-of-Functional" class="headerlink" title="Other Forms of Functional"></a>Other Forms of Functional</h3><p><strong>Rely exclusively on the terminal point with one state variable</strong>:</p><ul><li>Problem of Mayer:</li></ul><p>$$<br>V[y]=G[T,y(T)]<br>$$</p><p><strong>Rely both on path values and the terminal point</strong>:</p><ul><li>Problem of Bolza:</li></ul><p>$$<br>V[y]=\int^T_0 F[t,y(t),y’(t)]dt+G[T,y(T)]<br>$$</p><p>Although the problem of Bolza may seem to be the more general formulation, the truth is that the three types of problems-standard, Mayer, and Bolza-are all convertible into one another.</p><p><strong>Rely exclusively on the terminal point with two state variables</strong>:</p><ul><li>Problem of Mayer with two state variables</li></ul><p>$$<br>V[y,z]=G[T, y(T),z(T)]<br>$$</p><h2 id="Alternative-Approaches-to-Dynamic-Optimization"><a href="#Alternative-Approaches-to-Dynamic-Optimization" class="headerlink" title="Alternative Approaches to Dynamic Optimization"></a>Alternative Approaches to Dynamic Optimization</h2><h3 id="The-Calculus-of-Variations"><a href="#The-Calculus-of-Variations" class="headerlink" title="The Calculus of Variations"></a>The Calculus of Variations</h3><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[y]=\int^T_0 F[t,y(t),y’(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=Z\ &amp;(T,Z\ \mbox{given})<br>\end{array}<br>$$</p><p>In order to make such problems meaningful, it is necessary that the functional be <u>integrable</u> (i.e., the integral must be convergent).</p><p>Furthermore, we shall assume that all the functions that appear in the problem are <u>continuous</u> and <u>continuously differentiable</u>.</p><ul><li>This assumption is needed because the basic methodology underlying the calcu­lus of variations closely parallels that of the classical differential calculus.<ul><li>The main difference is that, instead of dealing with the differential $dx$ that changes the value of $y=f(x)$, we will now deal with <strong>the</strong> <strong>“variation” of an entire curve $y(t)$ that affects the value of the functional $V[y]$.</strong> </li></ul></li></ul><h3 id="Optimal-Control-Theory"><a href="#Optimal-Control-Theory" class="headerlink" title="Optimal Control Theory"></a>Optimal Control Theory</h3><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[u]=\int^T_0 F[t,y(t),u(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y’(t)=f[t,y(t),u(t))]\<br>                \<br>                &amp;y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=Z\ &amp;(T,Z\ \mbox{given})<br>\end{array}<br>$$</p><ul><li>Additional constraint:<br>$$<br>u(t)\in\mathscr{U}\mbox{ for }0\leq t\leq T<br>$$</li></ul><p>Aside from the time variable $t$ and the state variable $y(t)$, consideration is given to a control variable $u(t)$. </p><ul><li>Indeed, it is the latter type of variable that gives optimal control theory its name and occupies the center of stage in this new approach to dynamic optimiza­tion.</li></ul><p>To <u>focus attention on the control variable</u> implies that <u>the state variable is relegated to a secondary status</u>.</p><ul><li><p>This would be acceptable only if the decision on a control path $u(t)$ will, once given an initial condition on $y$, unambiguously determine a state-variable path $y(t)$ as a by-product. </p></li><li><p>For this reason, an optimal control problem must contain an equation that relates $y$ to $u$:<br>$$<br>y’(t)=f[t,y(t),u(t))]<br>$$</p><ul><li>Such an equation, called an <strong>equation of motion</strong> (or <strong>transition equation</strong> or <strong>state equation</strong>), shows how, at any moment of time, given the value of the state variable, the planner’s choice of $u$ will drive the state variable y over time.</li><li>Once we have found the optimal control-variable path $u^*(t)$, the equation of motion would make it possible to construct the related optimal state-variable path $y^*(t)$.</li></ul></li></ul><h3 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming:"></a>Dynamic Programming:</h3><p>The most important distinguishing characteristics of this approach are two: </p><ul><li>First, it <u>embeds the given control problem in a family of control problems</u>, with the consequence that in solving the given problem, we are actually solving the entire family of problems. </li><li>Second, for each member of this family of problems, <u>primary attention is focused on the optimal value of the functional</u>, $V^*$, rather than on the properties of the optimal state path $y^*(t)$ (as in the calculus of variations) or the optimal control path $u^*(t)$ (as in optimal control theory).</li></ul><p>Referring to Fig. 1.6 (adapted from Fig. 1.1), given the original problem of finding the least-cost path from point $A$ to point $Z$, we consider the larger problem of finding the least-cost path from each point in the set ${A, B, C, . . . , Z}$ to the terminal point $Z$. </p><ul><li><p>Since every component problem has a unique optimal path value, it is possible to write an <strong>optimal value function</strong><br>$$<br>V^* = V^*(i)\ \ \ \  ( i = A, B, .. . , Z)<br>$$</p><ul><li>which says that we can determine an optimal path value for every possible initial point.</li></ul></li><li><p>From this, we can also construct an <strong>optimal policy function</strong>, which will tell us how best to proceed from any specific initial point $i$, in order to attain $V^*(i)$ by the proper selection of a sequence of arcs leading from point $i$ to the terminal point $Z$.</p></li></ul><img src="/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/Fig_1.6.png" class title="[title]"><p>Returning to Fig. 1.6, imagine that our immediate problem is merely that of determining the optimal values for stage 5, associated with the three initial points $I$, $J$, and $K$. </p><ul><li><p>The answer is easily seen to be<br>$$<br>V^*(I) = 3,\ V^*(J) = 1,\ V^*(K) =2\tag{1.10}<br>$$</p></li><li><p>Having found the optimal values for $I$, $J$, and $K$, the task of finding the least-cost values $V^*(G)$ and $V^*(H)$ becomes easier. </p></li><li><p>Moving back to stage 4 and utilizing the previously obtained optimal-value information in (1.10), we can determine $V^*(G)$ as well as the optimal path $GZ$ (from $G$ to $Z$) as follows:</p></li></ul><p>$$<br>\begin{array}{}</p><p>V^*(G)<br>&amp;= \min{\mbox{value of arc }GI + V^*(I),\mbox{ value of arc }GJ + V^*(J)}<br>\<br>&amp;=\min{2+ 3,8+1}=5 \mbox{ [The optimal path GZ is GIZ.]}<br>\end{array}\tag{1.11}<br>$$</p><ul><li><p>By the same token, we find<br>$$<br>\begin{array}{}</p><p>V^*(H)<br>&amp;= \min{\mbox{value of arc }HJ + V^*(J),\mbox{ value of arc }HK + V^*(K)}<br>\<br>&amp;=\min{4+ 1,6+2}=5 \mbox{ [The optimal path HZ is HJZ.]}<br>\end{array}\tag{1.12}<br>$$</p></li></ul><p>With the knowledge of $V^*(G)$ and $V^*(H)$, we can then move back one more stage to calculate $V^*(D), V^*(E)$, and $V^*(F)$-and the optimal paths $DZ, EZ$, and $FZ$-in a similar manner. </p><ul><li>And, with two more such steps, we will be back to stage 1, where we can determine $V^*(A)$ and the optimal path $AZ$, that is, solve the original given problem.</li></ul><p>The essence of the iterative solution procedure is captured in <strong>Bellman’s principle of optimality</strong>, which states, roughly, that if you chop off the first arc from an optimal sequence of arcs, the remaining abridged sequence must still be optimal in its own right-as an optimal path from its own initial point to the terminal point. </p><ul><li>If $EHJZ$ is the optimal path from $E$ to $Z$, for example, then $HJZ$ must be the optimal path from $H$ to $Z$. </li><li>Conversely, if $HJZ$ is already known to be the optimal path from $H$ to $Z$, then a longer optimal path that passes through $H$ must use the sequence $HJZ$ at the tail end. <ul><li>This reasoning is behind the calculations in (1.11) and (1.12). </li><li>But note that in order to apply the principle of optimality and the iterative procedure to delineate the optimal path from $A$ to $Z$, we must find the optimal value associated with every possible point in Fig. 1.6. </li><li>This explains why we must embed the original problem.</li></ul></li></ul><p>Even though the essence of dynamic programming is sufficiently clari­fied by the discrete example in Fig. 1.6, the full version of dynamic program­ ming includes the continuous-time case. </p><ul><li>Unfortunately, the solution of continuous-time problems of dynamic programming involves the more ad­vanced mathematical topic of partial differential equations. </li><li>Besides, partial differential equations <u>often do not yield analytical solutions</u>. </li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Salient-Features-of-Dynamic-Optimization-Problems&quot;&gt;&lt;a href=&quot;#Salient-Features-of-Dynamic-Optimization-Problems&quot; class=&quot;headerlink&quot; t</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.4 Dynamic Optimization" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-4-Dynamic-Optimization/"/>
    
    <category term="1.4.A Preliminaries of Dynamic Optimization" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-A-Preliminaries-of-Dynamic-Optimization/"/>
    
    
    <category term="Control Theory" scheme="https://hqin-2020.github.io/tags/Control-Theory/"/>
    
  </entry>
  
  <entry>
    <title>Geometric Linear Algebra</title>
    <link href="https://hqin-2020.github.io/2020/12/24/Mathematics/Algebra/2%20Linear%20Algebra/Geometric-Linear-Algebra/"/>
    <id>https://hqin-2020.github.io/2020/12/24/Mathematics/Algebra/2%20Linear%20Algebra/Geometric-Linear-Algebra/</id>
    <published>2020-12-24T14:52:59.000Z</published>
    <updated>2021-01-01T15:48:13.811Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Vectors"><a href="#Vectors" class="headerlink" title="Vectors"></a>Vectors</h2><p>In physics, the vectors can be arrows in the space.</p><p>In computer science, the vectors can be ordered list of numbers.</p><p>Mathematicians consider a more general definition form of vectors.</p><ul><li>Anything where there is a sensible notion of <strong>addition</strong> and <strong>scalar multiplication</strong> can be viewed as vectors.</li></ul><p>Consider vectors as arrows with its tails sitting at the origin, the coordinates of vectors tells how to get from the tails of vectors to its tip.</p><ul><li>The addition of vectors is moving along the sum of vectors</li><li>The scalar multiplication of vectors is scaling the vectors.</li></ul><p>Consider vectors as ordered lists of numbers, each component of ordered lists of numbers describe how to move parallel to the axes, getting from the tails of vectors to its tip.</p><ul><li>The addition of vectors is matching up their trems and add each one together.</li><li>The scalar muliplication of vectors is multiplying each one of their component by that scalar.</li></ul><p>We usually think of individual vectors as arrows, and think of a set of vectors as points.</p><h2 id="Linear-Combinations"><a href="#Linear-Combinations" class="headerlink" title="Linear Combinations"></a>Linear Combinations</h2><p>The <strong>linear combination</strong> refers to the vector as an <strong>operation result</strong> of <strong>addition</strong> and <strong>scalar multiplication</strong> between multiple vectors.</p><p><strong><font color=941751>For an arbitrary vector, the vector this coordinate describe is actually the sum of multiple scaled vectors / bases of vectors / unit vectors.</font></strong></p><ul><li><strong><font color=941751>The coordinate as scalars describe how each one stretches and squishes vectors / bases of vectors / unit vectors.</font></strong></li><li>As a vector, it also has the orginal meaning like the arrow movement or the ordered list of numbers.</li><li>Any linear combination is relevant to its basis vectors.</li></ul><p>How to understand linearity?</p><ul><li>In 2-D linear spaces, fix one of these scalars, and let the other one change its value freely, the tip of resulting vector draws a straight line.</li></ul><p>The <strong>span</strong> of vectors refers to what are all the possible vectors we can reach using only these (two) vectors.</p><ul><li><strong>Linear dependent</strong> <ul><li>Whenever we have multiple vectors and we can remove one without reducing the span.</li><li>One of these vectors could be represented as a linear combination of others, since it is already in the span of the others.</li></ul></li><li><strong>Linear independent</strong><ul><li>Each vector add a new dimension to the span.</li></ul></li></ul><h2 id="Matrix-as-Linear-Transformations"><a href="#Matrix-as-Linear-Transformations" class="headerlink" title="Matrix as Linear Transformations"></a>Matrix as Linear Transformations</h2><p><strong>Linear transformation</strong> is a function from a linear space to another linear space.</p><ul><li><p>Linear transformation is not called as function because <strong>it is to be suggestive of a way to visualize the input-output relation</strong>, that is, the movement – move the input vector to output vector.</p></li><li><p>Linear transformation is also called the <strong>linear operator</strong>, since it can be represented by matrix multiplication, which is an binary operation defined on an algebra.</p></li><li><p><strong>Linear function</strong> / Linear funtional refers to a real map which maps a linear space to $\mathbb{R}$.</p></li></ul><p>The movement decribed by the linear transformation has two characteristics.</p><ol><li>All lines must remain lines, without getting curved.</li><li>The origin remain fixed.<ul><li>The affine transformation is a movement where the origin moves.</li></ul></li></ol><p>In other words, the linear transformation <strong>keeps grid lines paraller and evenly spaced</strong>.</p><p>Consider taking every possible input vector to some output vector, it is equivalent to consider every point in space move to some other point.</p><p>How to decribe one of these linear transformations numerically?</p><ul><li>Only need to record where the (two) basis vectors each land.</li></ul><p>Consider a linear transformation<br>$$<br>\mathbf{i}=\left[\begin{matrix}1 \\ 0\end{matrix}\right]\to L(\mathbf{i})=\left[\begin{matrix}1 \\ -2\end{matrix}\right]<br>$$</p><p>$$<br>\mathbf{j}=\left[\begin{matrix}0 \\ 1\end{matrix}\right]\to L(\mathbf{j})=\left[\begin{matrix}3 \\ 0\end{matrix}\right]<br>$$</p><p>Since every vector can be decribed as the sum of scaled bases of vectors. Since the linear transfomation moves the basis vectors, then the vector is the sum of scaled new bases of vectors. </p><p>let’s write this linear transformation in a linear combination form.<br>$$<br>\left[\begin{matrix}x \\ y\end{matrix}\right]\to L(\left[\begin{matrix}x \\ y\end{matrix}\right])=x\left[\begin{matrix}1 \\ -2\end{matrix}\right]+y\left[\begin{matrix}3 \\ 0\end{matrix}\right]=\left[\begin{matrix}1x+3y \\ -2x+0y\end{matrix}\right]<br>$$<br>Intuitively, we can write it as a matrix multiplication form for convenience<br>$$<br>\left[\begin{matrix}x \\ y\end{matrix}\right]\to L(\left[\begin{matrix}x \\ y\end{matrix}\right])=\left[\begin{matrix}L(\mathbf{i}) &amp;  L(\mathbf{j})\end{matrix}\right]\left[\begin{matrix}x \\ y\end{matrix}\right]=\left[\begin{matrix}1 &amp; 3\\ -2 &amp; 0\end{matrix}\right]\left[\begin{matrix}x \\ y\end{matrix}\right]=\left[\begin{matrix}1x+3y \\ -2x+0y\end{matrix}\right]<br>$$<br>In conclusion, linear transformation can be decribed using only the coordinates of where each basis vector lands.</p><ul><li>Matrices give us a language to describe these transformation where the column represent those coordinates.</li><li>Any metrix-vector multiplication is just a way to compute what that trnsformation does to a given vector.</li><li><strong><font color=941751>Every matrix can be interpreted as a certain transformation of space.</font></strong></li></ul><h2 id="Matrix-Multiplication-as-Composition"><a href="#Matrix-Multiplication-as-Composition" class="headerlink" title="Matrix Multiplication as Composition"></a>Matrix Multiplication as Composition</h2><p>Matrix multiplcation is a <strong>compostion</strong> of linear transformation.</p><p>Multipliy two matrices has the geometric meaning of applying one transformation then another.</p><h2 id="The-Determinant"><a href="#The-Determinant" class="headerlink" title="The Determinant"></a>The Determinant</h2><p>The linear transformation is accompanied by streching or squishing space. </p><p>To measure exactly how much are things being scretched is equivalent to measure how much are areas scaled.</p><p>Similarly, we only need to consider the unit square, take 2-D linear space as an example.</p><p>The very special scaling vector, by whcih a linear transformation changes any area are called the <strong>determinant</strong> of a transformation.</p><ul><li><p>The determinant of a 2-D transformation is 0, if it squishes all sapce onto a line or even onto a single point.</p></li><li><p>Negative determinant imlies orientation-flipping.</p></li></ul><h2 id="Linear-Equation-Systems"><a href="#Linear-Equation-Systems" class="headerlink" title="Linear Equation Systems"></a>Linear Equation Systems</h2><p>Any linear equation system can be written as the form $\mathbf{Ax}=\mathbf{y}$, it is equivalent to find a vector $\mathbf{x}$ after linear transformation $\mathbf{A}$ lands on $\mathbf{y}$. </p><ul><li>To solve the linear equation system, we need to find a inverse linear transformation, which is to find the inverse matrix $\mathbf{A}^{-1}$.</li></ul><p>If the determinant of $\mathbf{A}$ doesn’t equal to 0, then we can find a unique $\mathbf{x}$, which is the unique solution, implying the existence of $\mathbf{A}^{-1}$.</p><p>If the determinant of $\mathbf{A}$ equal to 0, for a 2-D linear space, it implies this linear transformation squish the whole space into a line, the solution exists iff $\mathbf{y}$ lies on this line. </p><ul><li>$\mathbf{A}^{-1}$ doesn’t exist if the determinant of $\mathbf{A}$ equal to 0, because we cannot find a inverse linear transformation map a low dimensional space to a high dimensional space, since the linear transformation is a function. <ul><li>For example, we cannot map a point into a line.</li></ul></li><li>Let’s review the definition of function:<ul><li>By a <strong><font color=0096FF>function / map</font></strong> $f$ that maps $X$ into $Y$, denoted as $f:X→Y$, we mean a relation $f∈X×Y$ such that<ol><li>For every $x∈X$, there exists a $y∈Y$ such that $xfy$,</li><li>For every $y,z∈Y$ with $xfy$ and $xfz$, we have $y=z$.</li></ol></li><li>The second condition implies we cannot unsquish a linear space.</li></ul></li></ul><p>The <strong>rank</strong> of a matrix is the number of the dimension in the output vector.</p><ul><li>Since the columns tell us where the basis vector lands after the transformation.<ul><li>The set of all possible outputs of $\mathbf{Ax}$ is the span of column vectors, called the <strong>column space</strong> of $\mathbf{A}$</li><li>Rank is actually the dimension of the column space.<ul><li>$\mathbf{0}$ must lie in the column space since the origin doesn’t change in the linear transformation.</li><li>For the full rank matrix, only $\mathbf{0}$ in the input space is mapped into $\mathbf{0}$.</li><li>For the non-full rank matrix, multiple vector lands in $\mathbf{0}$ finally, due to the squishing.<ul><li>These vectors are called the <strong>null space</strong> or the <strong>kernel</strong> of the matrix.</li><li>The <strong>null space</strong> gives all possible solution to $\mathbf{Ax=0}$</li></ul></li></ul></li></ul></li></ul><h2 id="Eigenvectors-and-Eigenvalues"><a href="#Eigenvectors-and-Eigenvalues" class="headerlink" title="Eigenvectors and Eigenvalues"></a>Eigenvectors and Eigenvalues</h2><p>The <strong>eigenvectors</strong> refer to vectors which are not knocked off from its span, or still remain in its span after the linear transformation.</p><p>The <strong>eigenvalues</strong> refer to how much the eigenvectors being screched or squished after the liner transformation.</p><p>This can be represented as equations.<br>$$<br>\mathbf{Av}=\lambda\mathbf{v}\Longleftrightarrow(\mathbf{A}-\lambda\mathbf{I})\mathbf{v}=\mathbf{0}<br>$$<br>We are interested in non-zero eigenvectors $\mathbf{v}$, which requires $\mbox{det}(\mathbf{A}-\lambda\mathbf{I})=0$.</p><ul><li>It means $\mathbf{A}-\lambda\mathbf{I}$ squish non-zero vectors onto $\mathbf{0}$</li><li>Otherwise the null space, which is the solution of $(\mathbf{A}-\lambda\mathbf{I})\mathbf{v}=\mathbf{0}$ is unique $\mathbf{0}$.</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://www.3blue1brown.com/">https://www.3blue1brown.com</a></li><li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Vectors&quot;&gt;&lt;a href=&quot;#Vectors&quot; class=&quot;headerlink&quot; title=&quot;Vectors&quot;&gt;&lt;/a&gt;Vectors&lt;/h2&gt;&lt;p&gt;In physics, the vectors can be arrows in the space</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.2 Algebra" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-2-Algebra/"/>
    
    <category term="1.2.B Linear Algebra" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-2-Algebra/1-2-B-Linear-Algebra/"/>
    
    
    <category term="Linear Algebra" scheme="https://hqin-2020.github.io/tags/Linear-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Elements of Abstract Algebra</title>
    <link href="https://hqin-2020.github.io/2020/12/24/Mathematics/Algebra/1%20Abstract%20Algebra/Elements-of-Abstract-Algebra/"/>
    <id>https://hqin-2020.github.io/2020/12/24/Mathematics/Algebra/1%20Abstract%20Algebra/Elements-of-Abstract-Algebra/</id>
    <published>2020-12-24T14:52:00.000Z</published>
    <updated>2021-01-01T15:48:07.911Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Binary Operation</strong></p><ul><li>A <strong>binary operation</strong> on a set $G$ is a function </li></ul><p>$$<br>∗ : G ×G → G<br>$$</p><blockquote><p>As with any function, a binary operation is well-defined; when one says this explicitly, it is usually called the <strong>law of substitution</strong>:</p><p>If $x = x’$ and $y = y’$, then $x ∗ y = x’ ∗ y’$.</p></blockquote><p><strong>Group</strong></p><ul><li>A <strong>group</strong> is a set $G$ equipped with a binary operation $∗$ such that<ol><li>the <strong>associative law</strong> holds: for every $x, y, z ∈ G$, $x ∗ (y ∗ z) = (x ∗ y) ∗ z$</li><li>there is an element $e∈G$, called the <strong>identity</strong>, with $e∗x=x=x∗e$ for all $x∈G$</li><li>every $x∈G$ has an <strong>inverse</strong>; there is $x’∈G$ with $x∗x’=e=x’∗x$.</li></ol></li></ul><p><strong>Abelian Group</strong></p><ul><li>A group $G$ is called <strong>abelian</strong> if it satisfies the <strong>commutative law</strong>: $x∗y=y∗x$ holds for every <em>x</em>, <em>y</em> ∈ <em>G</em>.</li></ul><blockquote><p>We will usually denote the product $x ∗ y$ in a group by $xy$, and we will denote the identity by $1$ instead of by $e$. </p><p>When a group is abelian, however, we will often use the <strong>additive notation</strong> $x + y$; in this case, we will denote the identity by $0$, and we will denote the inverse of an element $x$ by $−x$ instead of by $x^{−1}$.</p></blockquote><p><strong>Ring</strong>:</p><ul><li>A <strong>ring</strong> $(R, +, ·)$ is an abelian group $(R, +)$ endowed with a second binary operation $·$, satisfying on its own the requirements of being associative and having a two-sided identity and further interacting with $+$ via the distributive properties:, i.e.,<ol><li>$∀r,s,t∈R$, $(r·s)·t=r·(s·t)$,</li><li>$∃1_R ∈ R, ∀r ∈ R$, $r · 1_R = r = 1_R · r$ </li><li>$∀r,s,t∈R$, $(r+s)·t=r·s+r·t$ and $t·(r+s)=t·r+t·s$.</li></ol></li></ul><blockquote><p>What we are calling a ring, others may call a <strong>ring with identity</strong> or a <strong>ring with 1</strong>: it is not uncommon to exclude the axiom of existence of a multiplicative identity from the list of axioms defining a ring.</p></blockquote><p><strong>Commutative Ring</strong></p><ul><li>A <strong>commutative ring</strong> $R$ is a set with two binary operations, addition and multiplication, such that<ol><li>$R$ is an abelian group under addition;</li><li>(<strong>commutativity</strong>) $ab = ba$ for all $a, b ∈ R$;</li><li>(<strong>associativity</strong>) $a(bc) = (ab)c$ for every $a,b, c ∈ R$;</li><li>there is an element $1∈R$ with $1a=a$ for every $a∈R$;</li><li>(<strong>distributivity</strong>) $a(b + c) = ab + ac$ for every $a, b, c ∈ R$.</li></ol></li></ul><p><strong>Subring</strong></p><ul><li>A subset <em>S</em> of a commutative ring <em>R</em> is a <strong>subring</strong> of <em>R</em> if<ol><li>$1 ∈ S$;</li><li>if $a,b∈S$, then $a−b∈S$;</li><li>if $a,b∈S$, then $ab∈S$.</li></ol></li></ul><p><strong>Domain / Integral Domain</strong></p><ul><li>A <strong>domain / Integral domain</strong> is a commutative ring $R$ that satisfies two extra axioms: <ol><li>$1\neq 0$;</li><li>The <strong>cancellation law</strong> for multiplication: For all $a, b, c ∈ R$, if $ca = cb$ and $c\neq 0$, then $a = b$.</li></ol></li></ul><p><strong>Divide</strong>:</p><ul><li>Let $a$ and $b$ be elements of a commutative ring $R$. Then $a$ <strong>divides</strong> $b$ <strong>in $R$</strong> (or $a$ is a <strong>divisor</strong> of $b$ or $b$ is a <strong>multiple</strong> of $a$), denoted by $a | b$, if there exists an element $c ∈ R$ with $b = ca$.</li></ul><p><strong>Unit, Inverse</strong>:</p><ul><li>An element $u$ in a commutative ring $R$ is called a <strong>unit</strong> if $u | 1$ in $R$</li><li>that is, if there exists $v ∈ R$ with $uv = 1$; the element $v$ is called the <strong>inverse</strong> of $u$ and $v$ is often denoted by $u^{−1}$.</li></ul><p><strong>Group of Units</strong>:</p><ul><li>If $R$ is a commutative ring, then the <strong>group of units</strong> of $R$ is $U(R) = {\mbox{all units in }R}$.</li></ul><p><strong>Field</strong>:</p><ul><li>A <strong>field</strong> $F$ is a commutative ring in which $1\neq 0$ and every nonzero element $a$ is a unit; that is, there is $a^{−1} ∈ F$ with $a^{−1}a = 1$.</li></ul><p><strong>Subfield</strong>:</p><ul><li>A <strong>subfield</strong> of a field $K$ is a subring $k$ of $K$ that is also a field.</li></ul><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Lemma 2.16.</strong> Let G be a group.</p><ol><li>The <strong>cancellation laws</strong> hold: If either $x∗a=x∗b$ or $a∗x=b∗x$, then $a=b$.</li><li>The element $e$ is the unique element in $G$ with $e∗x=x=x∗e$ for all $x∈G$.</li><li>Each $x ∈ G$  has a unique inverse: There is only one element $x’∈ G$ with $x ∗ x’ = e = x′ ∗ x$  (henceforth, this element will be denoted by $x^{−1}$).</li><li>$(x^{-1})^{−1} = x$ for all $x ∈ G$.</li></ol><p><strong>Proposition 3.2.</strong> </p><ul><li>Let $R$ be a commutative ring.<ol><li>$0·a=0$ for every $a∈R$.</li><li>If $1 = 0,$ then $R$ consists of the single element $0$. In this case, $R$ is called the zero ring.</li><li>If $−a$ is the additive inverse of $a$, then $(−1)(−a) = a$.</li><li>$(−1)a = −a$ for every $a ∈ R$.</li><li>If $n∈N$ and $n1=0$, then $na=0$ for all $a∈R$.</li></ol></li></ul><p><strong>Proposition 3.3.</strong> </p><ul><li>A subring $S$ of a commutative ring $R$ is itself a commutative ring.</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li><li>Paolo Aluffi, 2009, “Algebra: Chapter 0”;</li><li>Michael Artin, 2010, “Algebra”;</li><li>Joseph J. Rotman, 2003, “Advanced Modern Algebra”;</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Definitions&quot;&gt;&lt;a href=&quot;#Definitions&quot; class=&quot;headerlink&quot; title=&quot;Definitions&quot;&gt;&lt;/a&gt;Definitions&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Binary Operation&lt;/strong&gt;&lt;</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.2 Algebra" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-2-Algebra/"/>
    
    <category term="1.2.A Abstract Algebra" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-2-Algebra/1-2-A-Abstract-Algebra/"/>
    
    
    <category term="Abstract Algebra" scheme="https://hqin-2020.github.io/tags/Abstract-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Propertities of Conditional Expectation</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/15%20Stochastic%20Dependence/Propertities-of-Conditional-Expectation/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/15%20Stochastic%20Dependence/Propertities-of-Conditional-Expectation/</id>
    <published>2020-12-15T14:50:59.000Z</published>
    <updated>2021-01-02T05:49:25.638Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 2.1</strong>: </p><ul><li><p>Let $x$ and $y$ be integrable random variables on a probability space $(X,\Sigma, \mathbf{p})$ and $\Sigma_0$ a sub-$\sigma$-algebra of $\Sigma$. Then, for every real number $\alpha$<br>$$<br>\mathbb{E}(\alpha x|\Sigma_0)=<em>{a.s.}\alpha\mathbb{E}(x|\Sigma_0)<br>$$<br>and<br>$$<br>\mathbb{E}(x+y|\Sigma_0)=</em>{a.s.}\mathbb{E}(x|\Sigma_0)+\mathbb{E}(y|\Sigma_0)<br>$$</p></li><li><p>Moreover,<br>$$<br>x=_{a.s.}y\mbox{ implies }\mathbb{E}(x|\Sigma_0)+\mathbb{E}(y|\Sigma_0)<br>$$</p></li></ul><p><strong>Proposition 2.2</strong>: </p><ul><li>Let $x$ and $y$ be integrable random variables on a probability space $(X,\Sigma, \mathbf{p})$ and $\Sigma_0$ a sub-$\sigma$-algebra of $\Sigma$. Then,<br>$$<br>\mathbb{E}(\mathbb{E}(x|\Sigma_0))=\mathbb{E}(x)<br>$$<br>And if $\Sigma_1$ is another sub-$\sigma$-algebra of $\Sigma$, then<br>$$<br>\Sigma_0\subseteq \Sigma_1\mbox{ implies }\mathbb{E}(\mathbb{E}(x|\Sigma_1)|\Sigma_0)=\mathbb{E}(x|\Sigma_0)<br>$$</li></ul><p><strong>Proposition 2.4</strong>:</p><ul><li><p>Let $x$ and $y$ be integrable random variables on a probability space $(X,\Sigma, \mathbf{p})$ and $\Sigma_0$ a sub-$\sigma$-algebra of $\Sigma$.</p></li><li><p>If $y\in\mathcal{L}^0(X,\Sigma_0)$ and $\mathbb{E}(|xy|)&lt;\infty$,then<br>$$<br>\mathbb{E}(xy|\Sigma_0)=_{a.s.}y\mathbb{E}(x|\Sigma_0)<br>$$</p></li></ul><p><strong>The Conditional Monotone Convergence Theorem</strong>: </p><ul><li><p>Let $x, x_1,x_2,\dots$ be nonnegative integrable random variables on a probability space $(X,\Sigma, \mathbf{p})$ such that $x_m\nearrow_{a.s.} x$</p></li><li><p>If $\Sigma_0$ is a sub-$\sigma$-algebra of $\Sigma$, then<br>$$<br>\mathbb{E}(x_m|\Sigma_0)\nearrow_{a.s.}\mathbb{E}(x|\Sigma_0)<br>$$</p></li></ul><p><strong>The Conditional Dominated Convergence Theorem</strong>: </p><ul><li><p>Let $x, y,x_1,x_2,\dots$ be nonnegative integrable random variables on a probability space $(X,\Sigma, \mathbf{p})$ such that $x_m\leq_{a.s.} x$ for each $m$</p></li><li><p>If $\Sigma_0$ is a sub-$\sigma$-algebra of $\Sigma$, then<br>$$<br>\mathbb{E}(x_m|\Sigma_0)\to_{a.s.}\mathbb{E}(x|\Sigma_0)<br>$$</p></li></ul><p><strong>The Conditional Jensen’s Inequality</strong>:</p><ul><li>Let $x$ be an integrable random variable on a probability space $(X,\Sigma, \mathbf{p})$, and $\Sigma_0$ a sub-$\sigma$-algebra of $\Sigma$. If $\varphi$ is a concave self-map on $\mathbb{R}$ such that $\mathbb{E}(|\varphi\circ x|) &lt;\infty$, then<br>$$<br>\varphi(\mathbb{E}(x|\Sigma_0))\geq_{a.s.}\mathbb{E}(\varphi\circ x|\Sigma_0)<br>$$</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Propositions-and-Theorems&quot;&gt;&lt;a href=&quot;#Propositions-and-Theorems&quot; class=&quot;headerlink&quot; title=&quot;Propositions and Theorems&quot;&gt;&lt;/a&gt;Proposition</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.H Stochastic Independence and Dependence" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
  <entry>
    <title>Conditional Expectation</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/15%20Stochastic%20Dependence/Conditional-Expectation/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/15%20Stochastic%20Dependence/Conditional-Expectation/</id>
    <published>2020-12-15T14:49:59.000Z</published>
    <updated>2021-01-02T05:49:20.400Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Conditional Expectation on a Known Event</strong>:</p><ul><li><p>Let $x$ be an integrable random variable on a probability space $(X,\Sigma,\mathbf{p})$ </p></li><li><p>That is, $x\in\mathcal{L}^0(X,\Sigma$) and $\mathbb{E}(|x|) &lt; \infty$, or put differently, $x\in\mathcal{L}^1(X,\Sigma,\mathbf{p})$</p></li><li><p>For any event $C\in\Sigma$ with $\mathbf{p}(C) &gt; 0$, the <strong>conditional expectation</strong> <strong>of $x$ given</strong> $C$ is defined as<br>$$<br>\mathbb{E}(x|C):=\frac{1}{\mathbf{p}(C)}\int_Cxd\mathbf{p}<br>$$</p></li></ul><blockquote><p>When it is known that the event $C$ has occurred, one is less uncertain about the outcome of the underlying experiment. </p><p>The relevant probability space in this case can be thought of as $(C,\Sigma\cap C,\mathbf{q})$, where $\Sigma\cap C := {A\cap C:A\in\Sigma}$ and<br>$$<br>\mathbf{q}:=\frac{1}{\mathbf{p}(C)}\mathbf{p}|_{\Sigma\cap C}<br>$$<br>Consequently, we compute the conditional expectation of $x$ given $C$ simply as the expectation of $x|_C$ on this space.</p></blockquote><p><strong>Conditional Probability on a Known Event</strong>:</p><ul><li>The <strong>conditional probability</strong> of an event $A\in\Sigma$ given $C$ is $\mathbf{p}(A|C) := \frac{\mathbf{p}(A \cap C)}{\mathbf{p}(C)}$</li></ul><blockquote><p>Just as the probability of an event is the expectation of the indicator function of that event, the conditional probability of an event is the conditional expectation of the indicator function of that event:<br>$$<br>\mathbb{p}(A|C)=\frac{\mathbb{p}(A\cap C)}{\mathbf{p}(C)}\mathbb{E}(\mathbf{1}_A|C)<br>$$<br>Note that these notions apply only when we know that an event has already occurred.</p></blockquote><p><strong>Countable (finite) $\Sigma$-decomposition</strong>:</p><ul><li>By a <strong>countable (finite) $\Sigma$-decomposition</strong> of $X$ we mean a countable (finite) partition $\mathcal{C}$ of $X$ such that $\mathcal{C}\subseteq \Sigma$ and $\mathbf{p}(C) &gt; 0$ for all $C\in\mathcal{C}$.</li></ul><p><strong>Conditional Expectation on a Countable Decomposition</strong>:</p><ul><li><p>We define the <strong>conditional expectation of $x$ given $\mathcal{C}$</strong> as the <u>discrete random variable</u> $\mathbb{E}(x|{\mathcal{C}}) : X \to \mathbb{R}$ with<br>$$<br>\mathbb{E}(x|\mathcal{C})(\omega):=\mathbb{E}(x|C_\omega)<br>$$<br>or<br>$$<br>\int_{C_\omega}\mathbb{E}(x|\mathcal{C})d\mathbf{p}=\int_{C_\omega}xd\mathbf{p}\mbox{ for every }\omega\in X<br>$$</p></li><li><p>where $C_\omega$ is the member of $\mathcal{C}$ that contains the outcome $\omega$.</p></li><li><p>More compactly, we may write<br>$$<br>\mathbb{E}(x|\mathcal{C}):=\sum_{C\in\mathcal{C}}\mathbb{E}(x|C)\mathbf{1}_C<br>$$</p></li></ul><blockquote><p>Given that we will observe which member of $\mathcal{C}$ has occurred once the experiment is performed, we then calculate the conditional expectation.</p><p>A gambler, for instance, would like to have a strategy which is contingent on what will happen through the sequence of games she will repeatedly participate in.</p></blockquote><p><strong>Conditional Probability on a Countable Decomposition</strong>:</p><ul><li><p>The <strong>conditional probability of an event $A$ in $\Sigma$ given $\mathcal{C}$</strong> is similarly defined as the <u>discrete random variable</u> $\mathbf{p}(A|\mathcal{C}) : X\to\mathbb{R}$ with<br>$$<br>\mathbf{p}(A|\mathcal{C}) (\omega):=\frac{\mathbf{p}(A\cap C_\omega)}{\mathbf{p}(C_\omega)}<br>$$</p></li><li><p>Or equivalently,<br>$$<br>\mathbf{p}(A|\mathcal{C}):=\sum_{C\in\mathcal{C}}\mathbf{p}(A\cap C)\mathbf{1}_C<br>$$</p></li></ul><blockquote><p>Of course, we have $\mathbf{p}(A|\mathcal{C})=\mathbb{E}(\mathbf{1}_A|\mathcal{C})$</p></blockquote><p><strong>Sub-$\sigma$-algebra</strong>:</p><ul><li>Let $(X,\Sigma,\mathbf{p})$ be a probability space. By a <strong>sub-$\sigma$-algebra</strong> of $\Sigma$, we mean a subset of $\Sigma$ which is itself a $\sigma$-algebra.</li></ul><p><strong>Conditional Expectation on an Arbitrary $\sigma$-algebra, Conditional Probability on an Arbitrary $\sigma$-algebra</strong>:</p><ul><li><p>For any sub-$\sigma$-algebra $\Sigma_0$ of $\Sigma$, and any $x\in\mathcal{L}^1(X,\Sigma,\mathbf{p})$, the <strong>conditional expectation of $x$ given $\Sigma_0$</strong> is defined as <u>an extended real function</u> $\mathbb{E}(x|\Sigma_0)$ on $X$ which satisfies<br>$$<br>\mathbb{E}(x | \Sigma_0)\mbox{ is }\Sigma_0\mbox{-measurable}<br>$$</p></li><li><p>and<br>$$<br>\int_B\mathbb{E}(x|\Sigma_0)d\mathbf{p}=\int_Bxd\mathbf{p}\mbox{ for every }B\in\Sigma_0<br>$$</p></li><li><p>For any $y \in \mathcal{L}^0(X,\Sigma)$ we define the <strong>conditional expectation of $x$ given $y$,</strong> denoted $\mathbb{E}(x| y)$ as $\mathbb{E}(x | y) := \mathbb{E}(x |\sigma (y))$ </p></li><li><p>For any $A\in\Sigma$, we define <strong>the conditional probability of $A$ given $\Sigma_0$</strong>, denoted $\mathbf{p}(A|\Sigma_0)$ as <u>an extended real function</u> on $X$ that equals $\mathbb{E}(\mathbf{1}_A| \Sigma_0)$ or equivalently, that satisfies<br>$$<br>\mathbf{p}(x | \Sigma_0)\mbox{ is }\Sigma_0\mbox{-measurable}<br>$$</p></li><li><p>and<br>$$<br>\int_B\mathbf{p}(x|\Sigma_0)d\mathbf{p}=\mathbf{p}(A\cap B)\mbox{ for every }B\in\Sigma_0<br>$$</p></li></ul><p><strong>Conditional Density</strong>:</p><ul><li><p>Let $x$ and $y$ be two integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$ and let $f$ be a joint density for $x$ and $y$ </p></li><li><p>The <strong>conditional density of $x$ given $y$</strong> is defined as<br>$$<br>f_{x|y}(s,t):=\left{\begin{matrix}\frac{f(s,t)}{f_y(t)}&amp;\mbox{if }f_y(t)&gt;0\0&amp;\mbox{otherwise}\end{matrix}\right.<br>$$</p></li><li><p>where $f_y$ is the marginal density of $y$.</p></li></ul><blockquote><p>We can show that<br>$$<br>\mathbb{E}(x|y)=<em>{a.s.}\int^\infty</em>{-\infty}sf_{x|y}(s,y(\cdot))ds<br>$$</p></blockquote><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Therorem 1.1</strong>:</p><ul><li>If $x$ is an integrable random variable on the probability space $(X,\Sigma,\mathbf{p})$, and $\Sigma_0$ is a sub-$\sigma$-algebra of $\Sigma$, then $\mathbb{E}(x|\Sigma_0)$ exists.</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Definitions&quot;&gt;&lt;a href=&quot;#Definitions&quot; class=&quot;headerlink&quot; title=&quot;Definitions&quot;&gt;&lt;/a&gt;Definitions&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Conditional Expectation on</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.H Stochastic Independence and Dependence" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
  <entry>
    <title>Law of Large Numbers</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/14%20A%20Primer%20on%20Probability%20Limit%20Theorems/Law-of-Large-Numbers/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/14%20A%20Primer%20on%20Probability%20Limit%20Theorems/Law-of-Large-Numbers/</id>
    <published>2020-12-15T14:48:59.000Z</published>
    <updated>2021-01-02T05:48:34.538Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Random Sample, Statistic</strong>:</p><ul><li>Let $x$ be a random variable on a probability space $(X,\Sigma,\mathbf{p})$. </li><li>A <strong>random sample</strong> for $x$ is a finite collection $x_1,\dots,x_m$ of i.i.d. random variables on $(X,\Sigma,\mathbf{p})$ such that $x_1 =_{a.s.} x$. </li><li>A (real-valued) <strong>statistic</strong> based on such a random sample is a <u>random variable</u> of the form $\varphi(x_1,\dots, x_m)$, where $\varphi$ is <u>a Borel measurable real function</u> on $\mathbb{R}\cup \mathbb{R}^2\cdots $ </li><li>Notice that $\varphi$ can accomodate any random sample regardless of its size.</li></ul><p><strong>Unbiased estimator</strong>:</p><ul><li><p>The statistics based on a random sample are used to derive inferences about the characteristics of the random variable of interest. We would then surely wish them to satisfy certain properties. </p></li><li><p>For instance, a desirable property in this regard is that of unbiasedness: We say that a statistic $\varphi(x_1,\dots,x_m)$ based on the random sample $x_1,\dots, x_m$ is an <strong>unbiased estimator</strong> of $x$ if<br>$$<br>\mathbb{E}(\varphi(x_1,\dots,x_m))=\theta_x<br>$$</p></li><li><p>where $\theta_x$ is a characteristic of $x$, such as its mean or another moment. </p></li></ul><blockquote><p>The property of unbiasedness is well-defined for any random sample, regardless of its size. As such, it is said to be a small-sample property. </p></blockquote><p><strong>Consistent estimator, Strongly consistent esitimator</strong>:</p><ul><li>We say that a statistic $\varphi(x_1,\dots,x_m)$ based on the random sample $x_1,\dots, x_m$ is a <strong>consistent estimator</strong> of a characteristic $\theta_x$ of $x$ if<br>$$<br>\mathbf{p}\mbox{-}\lim\varphi(x_1,\dots,x_m)=\theta_x<br>$$<br>And that it is a <strong>strongly consistent estimator</strong> of $\theta_x$ if<br>$$<br>\mathbf{p}{\varphi(x_1,\dots,x_m)\to\theta_x}=1<br>$$</li></ul><blockquote><p>A large-sample property of a statistic would instead be based on the limiting properties of this statistic as the sample size gets large. </p><p>Of particular interest in this regard are the properties of consistency.</p></blockquote><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>The Weak Law of Large Numbers</strong>: (Chebyshev) </p><ul><li>Let $(x_m)$ be a sequence of independent random variables on a probability space $(X,\Sigma, \mathbf{p})$ with $\mathbb{E}(x_1) = \mathbb{E}(x_2) = \cdots\in \mathbb{R}$ and $\sup \mathbb{V}(x_m) &lt; \infty$. Then,<br>$$<br>\mathbb{E}\left(\left|\frac{1}{m}\sum_{i\in[m]}x_i-\mathbb{E}(x_1)\right|\right)\to 0\mbox{ and }\frac{1}{m}\sum_{i\in[m]}x_i\to\mathbb{E}(x_1)\mbox{ in probability}<br>$$</li></ul><p><strong>Corollary 2.1</strong>: </p><ul><li>Let $(x_m)$ be a sequence of i.i.d. random variables with finite expectation and variance. Then,<br>$$<br>\frac{1}{m}\sum_{i\in[m]}x_i\to\mathbb{E}(x_1)\mbox{ in probability}<br>$$</li></ul><p><strong>Khinchine’s Weak Law of Large Numbers</strong>:</p><ul><li>Let $(x_m)$ be a sequence of i.i.d. random variables with finite expectation. Then,</li></ul><p>$$<br>\frac{1}{m}\sum_{i\in[m]}x_i\to\mathbb{E}(x_1)\mbox{ in probability}<br>$$</p><p><strong>Markov’s Weak Law of Large Numbers</strong>: </p><ul><li><p>Let $(x_m)$ be a sequence of uncorrelated random variables on a probability space $(X,\Sigma,\mathbf{p})$ such that $\sup\mathbb{E}(x_m) &lt; 1$ and $\sup \mathbb{V}_(x_m) &lt; 1$.</p></li><li><p>Assume further that<br>$$<br>\lim\frac{1}{m}\sum_{i\in[m]}\mathbb{E}(x_i)\in\mathbb{R}\mbox{  and }\frac{1}{m^2}\sum_{i\in[m]}\mathbb{V}(x_i)\to0<br>$$</p></li><li><p>Then<br>$$<br>\frac{1}{m}\sum_{i\in[m]}x_i\to\lim\frac{1}{m}\sum_{i\in[m]}\mathbb{E}(x_i)\mbox{ in probability}<br>$$</p></li></ul><p><strong>The Strong Law of Large Numbers</strong>: (Kolmogorov) </p><ul><li>For any sequence $(x_m)$ of integrable i.i.d. random variables, we have<br>$$<br>\frac{1}{m}\sum_{i\in[m]}x_i\to_{a.s.}\mathbb{E}(x_1)<br>$$</li></ul><p><strong>Corollary 2.2</strong>: (Kolmogorov) </p><ul><li>For any sequence $(x_m)$ of integrable i.i.d. random variables such that $\mathbb{E}(x_1)\in{-\infty,\infty}$, we have<br>$$<br>\frac{1}{m}\sum_{i\in[m]}x_i\to_{a.s.}\mathbb{E}(x_1)<br>$$</li></ul><h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Sample Mean, Sample Variance</strong>:</p><ul><li><p>For a random sample with a finite collection $a_1,\dots,a_m$ of i.i.d. random variables on $(X,\Sigma,\mathbf{p})$ such that $a_1 =_{a.s.} a$. </p></li><li><p>If<br>$$<br>\varphi(a_1,\dots,a_m):=\frac{1}{m}\sum_{i\in[m]}a_i<br>$$<br>Then $\varphi(a_1,\dots,a_m)$ corresponds to the statistic of the sample mean</p></li><li><p>If<br>$$<br>\varphi(a_1,\dots,a_m):=\frac{1}{m}\sum_{i\in[m]}\left(a_i-\frac{1}{m}\sum_{i\in[m]}a_i\right)^2<br>$$<br>Then $\varphi(a_1,\dots,a_m)$ corresponds to the statistic of the sample variance.</p></li><li><p>Sample mean is an unbiased estimator of $\mathbb{E}(x)$, for any postive integer $m$.<br>$$<br>\mathbb{E}\left(\frac{1}{m}\sum_{i\in[m]}x_i\right)=\frac{1}{m}\sum_{i\in[m]}\mathbb<a href="x_i">E</a>=\mathbb{E}(x)<br>$$<br>as $\mathbb{E}(x_i) = \mathbb{E}(x)$ for each $i$.</p></li><li><p>By contrast, the sample variance is not an unbiased estimator of $\mathbb{V}(x)$.</p></li></ul><p><strong>Example 2.2</strong>:</p><ul><li><p>The sample mean is a strongly consistent estimator of $\mathbb{E}(x)$ provided that $\mathbb{E}(x)$ is finite. This is the same thing as saying that<br>$$<br>\frac{1}{m}\sum_{i\in[m]}x_i\to_{a.s.}\mathbb{E}(x)<br>$$</p></li><li><p>when $\mathbb{E}(x)$ is finite. </p></li></ul><p><strong>Example 2.3</strong>:</p><ul><li>The sample variance is a strongly consistent estimator of $\mathbb{V}(x)$ provided that both $\mathbb{E}(x)$ and $\mathbb{E}(x^2)$ are finite. This is the same thing as saying that<br>$$<br>\frac{1}{m}\sum_{i\in[m]}\left(x_i-\frac{1}{m}\sum_{i\in[m]}x_i\right)^2\to_{a.s.}\mathbb{V}(x)<br>$$</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Definitions&quot;&gt;&lt;a href=&quot;#Definitions&quot; class=&quot;headerlink&quot; title=&quot;Definitions&quot;&gt;&lt;/a&gt;Definitions&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Random Sample, Statistic&lt;/</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.G Weak Convergence and Probability Limit" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-G-Weak-Convergence-and-Probability-Limit/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
  <entry>
    <title>Preliminaries of Probability Limit Theorems</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/14%20A%20Primer%20on%20Probability%20Limit%20Theorems/Preliminaries-of-Probability-Limit-Theorems/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/14%20A%20Primer%20on%20Probability%20Limit%20Theorems/Preliminaries-of-Probability-Limit-Theorems/</id>
    <published>2020-12-15T14:47:59.000Z</published>
    <updated>2021-01-02T05:48:41.271Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Converge in Probability</strong>:</p><ul><li><p>Let $Y$ be a separable metric space, and $x,x_1,x_2,\dots$ be $Y$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$. </p></li><li><p>We say that $(x_m)$ <strong>converges to $x$ in probability</strong>, and write $x_m\to x$ in probability, or $\mathbf{p}$-$\lim x_m = x$ if<br>$$<br>\mathbf{p}{d_Y(x_m,x)&gt;\varepsilon}\to 0 \mbox{ for every }\varepsilon&gt;0<br>$$</p></li><li><p>That is, $x_m \to x$ in probability iff, for every positive real numbers $\varepsilon$ and $\delta$ there exists a positive integer $M$ such that<br>$$<br>\mathbf{p}{d_Y(x_m,x)&gt;\varepsilon}&lt;\delta \mbox{ for all }m&gt;M<br>$$</p></li></ul><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1.1</strong>：</p><ul><li>Let $Y$ be a separable metric space, and $x,x_1,x_2,\dots$ be $Y$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$. </li><li>If $(x_m)$ converges to $x$ in probability, then $x_m\stackrel{D}\to x$.</li></ul><p><strong>Proposition 1.2</strong>: (Kolmogorov) </p><ul><li>Let $Y$ be a separable metric space, and $x,x_1,x_2,\dots$ be $Y$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$. </li><li>If $x_m\to_{a.s.}x$, then $(x_m)$ converges to $x$ in probability.</li></ul><blockquote><p>$$<br>\mbox{almost sure convergence}\Longrightarrow\mbox{convergence in probability}\Longrightarrow\mbox{convergence in distribution}<br>$$</p></blockquote><p><strong>Theorem 3.8</strong>:</p><ul><li>Let $Y$ be a separable metric space</li><li>For any given real number $p\geq1$, let $x, x_1,x_2,\dots$ be $Y$-valued random variables on $\mathcal{L}^p(X,\Sigma,\mathbf{p})$</li><li>If $(x_m)$ is $\mathcal{L}^p$-convergent, then $(x_m)$ <strong>converges to $x$ in probability</strong>.</li></ul><blockquote><p>$$<br>\mathcal{L}^p\mbox{-convergence}\Longrightarrow\mbox{convergence in probability}\Longrightarrow\mbox{convergence in distribution}<br>$$</p></blockquote><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Definitions&quot;&gt;&lt;a href=&quot;#Definitions&quot; class=&quot;headerlink&quot; title=&quot;Definitions&quot;&gt;&lt;/a&gt;Definitions&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Converge in Probability&lt;/s</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.G Weak Convergence and Probability Limit" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-G-Weak-Convergence-and-Probability-Limit/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
  <entry>
    <title>Independence of Random Variables</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/13%20Stochastic%20Independence/Independence-of-Random-Variables/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/13%20Stochastic%20Independence/Independence-of-Random-Variables/</id>
    <published>2020-12-15T14:46:59.000Z</published>
    <updated>2021-01-02T05:49:08.691Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Independence of Random Variables</strong>:</p><ul><li><p>For any positive integer $i$, let $Y_i$ be a metric space and $x_i$ a $Y_i$-valued random variable on a probability space $(X,\Sigma,\mathbf{p})$. </p></li><li><p>For any $m &gt; 1$, we say that $x_1,\dots,x_m$ are (stochastically) <strong>independent</strong> when $(x_1),\dots,(x_m)$ are independent, that is,<br>$$<br>\mathbf{p}\left(\bigcap_{i\in[m]}{x_i\in A_i}\right)=\prod_{i\in[m]}\mathbf{p}{x_i\in A_i}\mbox{ for every }A_i\in\mathcal{B}(Y_i),\ i=1,\dots,m<br>$$</p></li><li><p>and we denote this situation by writting $\coprod [x_1,\dots,x_m]$ </p></li><li><p>If $\coprod [x,x]$ for any $Y$-valued random variable $x$ on $(x,\Sigma,\mathbf{p})$, we say that $x$ is <strong>independent of itself</strong>.</p></li><li><p>More generally, if $\coprod [\sigma(x_1),\sigma(x_2),\dots]$ we say that $x_1,x_2,\dots$ are <strong>independent</strong> or $(x_m)$ is a <strong>sequence of independent of random variables</strong>, and write $\coprod [x_1,x_2,\dots]$ </p></li></ul><p><strong>Uncorrelated</strong>:</p><ul><li>Let $x$ and $y$ be two integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$.</li><li>Clearly, $xy$ is also a random variable on $(X,\Sigma,\mathbf{p})$</li><li>We say that $x$ and $y$ are <strong>uncorrelated</strong> if $\mathbb{E}(xy) = \mathbb{E}(x)\mathbb{E}(y)$.</li></ul><p><strong>Independently and Identically Distributed Random Variables</strong>:</p><ul><li>Let $Y$ be a metric space, and $(x_m)$ a sequence of $Y$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$. </li><li>For any $m &gt; 1$, we say that $x_1,\dots, x_m$ are <strong>independently and identically distributed</strong>, or simply <strong>i.i.d.</strong>, if $\coprod [x_1,\dots,x_m]$ and $\mathbf{p}_{x_i} = \mathbf{p}_{x_j}$ for every $i,j\in[m]$.</li><li>More generally, we say that $x_1,x_2,\dots$ are <strong>i.i.d</strong>. (or that $(x_m)$ is a <strong>sequence of i.i.d. random variables</strong> when $\coprod [x_1,\dots,x_m]$ and $\mathbf{p}_{x_i} = \mathbf{p}_{x_j}$ for every postive integers $i$ and $j$.</li></ul><blockquote><p>If $Y=\mathbb{R}$, then $\mathbf{p}_{x_i} = \mathbf{p}_{x_j}$ implies they have the same distribution and distribution functions. </p><p>Take the normal distribution as an example, the $\mu$ and $\sigma$ should be the same for all i.i.d. random variables.</p></blockquote><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 2.1</strong>: </p><ul><li>Let $m$ be a positive integer, and $x_1,\dots,x_m$ integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$. Then,<br>$$<br>\coprod [x_1,\dots,x_m]\mbox{ implies }\mathbb{E}\left(\prod_{i\in[m]}x_i\right)=\prod_{i\in[m]}\mathbb{E}(x_i)<br>$$</li></ul><blockquote><p>The expectation of the product of a finite number of independent random variables (each with finite expectation) equals the product of the expectations of these random variables. </p><p>In particular, any two independent random variables are uncorrelated.</p></blockquote><p><strong>Proposition 2.2</strong>: </p><ul><li>Let $m$ be a positive integer, and $x_1,\dots,x_m$ random variables on a probability space $(X,\Sigma, \mathbf{p})$. Then, $\coprod [x_1,\dots,x_m]$ iff<br>$$<br>F_{x_1,\dots,x_m}(a_1,\dots,a_m)=\prod_{i\in[m]}F_{x_i}(a_i)\mbox{ for every }(a_1,\dots,a_m)\in\mathbb{R}^m.<br>$$</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Definitions&quot;&gt;&lt;a href=&quot;#Definitions&quot; class=&quot;headerlink&quot; title=&quot;Definitions&quot;&gt;&lt;/a&gt;Definitions&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Independence of Random Var</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.H Stochastic Independence and Dependence" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
  <entry>
    <title>Independence of Collection of Events</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/13%20Stochastic%20Independence/Independence-of-Collection-of-Events/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/13%20Stochastic%20Independence/Independence-of-Collection-of-Events/</id>
    <published>2020-12-15T14:45:59.000Z</published>
    <updated>2021-01-02T05:49:13.792Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Independence of Events</strong>:</p><ul><li><p>Let $(X,\Sigma,\mathbf{p})$ be a probability space. </p></li><li><p>The events $A,B\in\Sigma$ are said to be <strong>(stochastically) independent</strong> if $\mathbf{p}(A \cap B) = \mathbf{p}(A)\mathbf{p}(B)$.</p></li><li><p>If $A = B$ here, we say that $A$ is independent of itself.</p></li><li><p>In turn, a nonempty collection $\mathcal{A}\subseteq \Sigma$ is called <strong>independent</strong> – we denote this by writing $\coprod \mathcal{A}$ – if<br>$$<br>\mathbf{p}(\bigcap \mathcal{S})=\prod_{A\in\mathcal{S}}\mathbf{p}(A)<br>$$</p></li><li><p>for every nonempty finite subset $\mathcal{S}$ of $\mathcal{A}$.</p></li></ul><p><strong>Independence of Finitely Many Familites of Events</strong>:</p><ul><li><p>Let $(X,\Sigma,\mathbf{p})$ be a probability space and $m$ an integrer with $m&gt;1$. </p></li><li><p>The nonempty collection $\mathcal{A}_1,\cdots,\mathcal{A}_m\subseteq \Sigma$ are said to be <strong>(stochastically) independent</strong> – we denote this by writing $\coprod [\mathcal{A}_1,\dots,\mathcal{A}_m]$ – if<br>$$<br>\coprod {A_1,\dots,A_m}\mbox{ for every }(A_1,\dotsm,A_m)\in\mathcal{A}_1\times\cdots\times\mathcal{A}_m<br>$$</p></li></ul><blockquote><p>The definition of independence of events is a special case of that of independence of collections of events.</p></blockquote><p><strong>Independence of Infinitely Many Familites of Events</strong>:</p><ul><li>Let $(X,\Sigma,\mathbf{p})$ be a probability space, $I$ any (index) set with $|I|&gt;1$, and $\mathcal{A}_i$ a nonempty subset of $\Sigma$ for each $i\in I$. </li><li>We say that $\mathcal{A}_i$s are <strong>independent</strong> – we denote this by writing $\coprod [\mathcal{A}<em>i, i\in I]$ – if  $\coprod [\mathcal{A}</em>{i_1},\dots,\mathcal{A}_{i_m}]$ for every integrer with $m&gt;1$ and every distinct $i_1,\dots,i_m\in I$.</li><li>If $I=\mathbb{N}$ here, we write $\coprod[\mathcal{A}_1,\mathcal{A}_2,\dots]$ to denote $\coprod[\mathcal{A}_i:i\in I]$.</li></ul><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1.1</strong>: </p><ul><li><p>Let $(X,\Sigma,\mathbf{p})$ be a probabilty space, and $m$ an integer with $m &gt; 1$.</p></li><li><p>If $\mathcal{A}<em>i\subseteq\Sigma$ contains $X$ for each $i\in[m]$, then $\coprod [\mathcal{A}_1,\dots,\mathcal{A}_m]$ iff<br>$$<br>\mathbf{p}\left(\bigcap</em>{i\in[m]}A_i\right)=\prod_{i\in[m]}\mathbf{p}(A_i)\mbox{ for every }(A_1,\dotsm,A_m)\in\mathcal{A}_1\times\cdots\times\mathcal{A}_m<br>$$</p></li></ul><p><strong>Proposition 1.2</strong>: </p><ul><li><p>Let $(X,\Sigma,\mathbf{p})$ be a probabilty space, and $m$ an integer with $m &gt; 1$.</p></li><li><p>If $\mathcal{A}_i\subseteq\Sigma$ contains $X$, and is closed under taking finite intersections, for each $i\in[m]$, then<br>$$<br>\coprod [\mathcal{A}_1,\dots,\mathcal{A}_m]\mbox{ implies }\coprod [\sigma(\mathcal{A}_1),\dots,\sigma(\mathcal{A}_m)]<br>$$</p></li></ul><p><strong>Proposition 1.3</strong>: </p><ul><li><p>Let $(X,\Sigma,\mathbf{p})$ be a probabilty space, $I$ any (index) set with $|I|&gt;1$, and $\mathcal{A}_i$ a nonempty subset of $\Sigma$ for each $i\in I$. </p></li><li><p>If $\mathcal{A}_i\subseteq\Sigma$ contains $X$, and is closed under taking finite intersections, for each $i\in[m]$, then<br>$$<br>\coprod[\mathcal{A}_i:i\in I]\mbox{ implies }\coprod[\sigma(\mathcal{A}_i):i\in I]<br>$$</p></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Definitions&quot;&gt;&lt;a href=&quot;#Definitions&quot; class=&quot;headerlink&quot; title=&quot;Definitions&quot;&gt;&lt;/a&gt;Definitions&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Independence of Events&lt;/st</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.H Stochastic Independence and Dependence" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
  <entry>
    <title>Convergence of Random Variables</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/12%20Weak%20Convergence/Convergence-of-Random-Variables/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/12%20Weak%20Convergence/Convergence-of-Random-Variables/</id>
    <published>2020-12-15T14:44:59.000Z</published>
    <updated>2021-01-02T05:48:49.226Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Converge in distribution</strong>: </p><ul><li>Let $Y$ be a metric space. </li><li>For any sequence of $Y $-valued random variables $(x_m)$ and a $Y $-valued random variable $x$, we say that $x_m$ <strong>converges to $x$ in distribution</strong>, denoted $x_m\stackrel{D}\to x$, if $\mathbf{p}<em>{x_m}\stackrel{w}{\to} \mathbf{p}_x$, where $\mathbf{p}</em>{x_m}$ and $\mathbf{p}_x$ are the distributions of $x_m$ and $x$, respectively, $m = 1,2,\cdots$.</li></ul><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 2.1</strong>: </p><ul><li>Let $Y$ be a metric space. </li><li>For any $Y $-valued random variables $x, x_1, x_2,\cdots$, we have $x_m \stackrel{D}\to x$ iff,</li></ul><p>$$<br>\mathbb{E}(\varphi\circ x_m)\to \mathbb{E}(\phi\circ x)\ \mbox{ for every } \in\mathbf{CB}(Y).<br>$$</p><p><strong>Corollary 2.2</strong>: </p><ul><li>Let $Y$ be a metric space. </li><li>For any $Y $-valued random variables $x, x_1,x_2,\cdots$, we have $x_m \stackrel{D}\to x$ iff,</li></ul><p>$$<br>E(\phi\circ x_m) \to \mathbb{E}(\phi\circ x)<br>$$</p><ul><li>for every bounded and Lipschitz continuous real map $\phi$ on $Y$.</li></ul><p><strong>Proposition 2.3</strong>: </p><ul><li>Let $Y$ be a metric space, and let $x, x_1, x_2,\cdots$ be $Y $-valued random variables on a common probability space. Then</li></ul><p>$$<br>x_m\to_{a.s.} x\ \mbox{ implies } x_m \stackrel{D}{\to} x,<br>$$</p><ul><li>but not conversely.</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Definitions&quot;&gt;&lt;a href=&quot;#Definitions&quot; class=&quot;headerlink&quot; title=&quot;Definitions&quot;&gt;&lt;/a&gt;Definitions&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Converge in distribution&lt;/</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.G Weak Convergence and Probability Limit" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-G-Weak-Convergence-and-Probability-Limit/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
  <entry>
    <title>Weak Convergence</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/12%20Weak%20Convergence/Weak-Convergence/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/12%20Weak%20Convergence/Weak-Convergence/</id>
    <published>2020-12-15T14:44:39.000Z</published>
    <updated>2021-01-02T05:48:53.741Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Converge Weakly, Weak Limit</strong>:  </p><ul><li>Let $X$ be a metric space and $(\mathbf{p}_m)$ a sequence in $\Delta(X)$. </li><li>For any $\mathbf{p}\in \Delta(X)$, we say that  $(\mathbf{p}_m)$ <strong>converges weakly</strong> to $\mathbf{p}$, denoted $\mathbf{p}_m \stackrel{w}{\to} \mathbf{p}$, if</li></ul><p>$$<br>\int_X\varphi d\mathbf{p}_m\to\int_X\varphi d\mathbf{p}\ \ \mbox{ for every } \varphi\in\mathbf{CB}(X)<br>$$</p><ul><li><p>In this case $\mathbf{p}$ is said to be the <strong>weak limit</strong> of $(\mathbf{p}_m)$.</p></li><li><p>Put differently, we have<br>$$<br>\mathbf{p}<em>m \stackrel{w}{\to} \mathbf{p} \mbox{ iff }\mathbb{E}</em>{\mathbf{p}_m}(x)\to\mathbb{E}_\mathbf{p}(x)<br>$$</p></li><li><p>for every $x\in\mathcal{L}^0(X,\mathcal{B}(X))$.</p></li></ul><p><strong>Converge Weakly, Weak Limit</strong>:  </p><ul><li>The Lebesgue-Stieltjes measure induced by a distribution function $F$ is denoted by $\mathbf{p}_F$. And the class of all distribution functions is denoted by $\mathfrak{F}$.</li><li>For any distribution functions $F, F_1, F_2,\cdots$, if $F_m(t)\to F(t)$ holds for every $t$ at which $F$ is continuous, we say the sequence $(F_m)$ <strong>weakly converges</strong> to an $F\in\mathfrak{F}$.</li><li>$F$ is said to be the <strong>weak limit</strong> of $(F_m)$.</li><li>Naturally enough, we denote this situation by writing $F_m\stackrel{w}\to F$.</li></ul><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1.1</strong>: </p><ul><li>Let $X$ be a metric space and ($\mathbf{p}_m$) a sequence in $\Delta(X)$. </li><li>If $\mathbf{p}_m \stackrel{w}{\to} \mathbf{p}$ and $\mathbf{p}_m\stackrel{w}{\to} \mathbf{q} $ for some $\mathbf{p},\mathbf{q}\in\Delta(X)$, then $\mathbf{p}=\mathbf{q}$.</li></ul><p><strong>Corollary 1.2</strong>: </p><ul><li>Given any metric space $X$ and $\mathbf{p}, \mathbf{q}\in \Delta(X)$, we have</li></ul><p>$$<br>\int_X\varphi d\mathbf{p}=\int_X\varphi d\mathbf{q}\ \mbox{ for all } \varphi\in\mathbf{CB}(X),\ \mbox{ iff } \mathbf{p}=\mathbf{q}<br>$$</p><blockquote><p>If two Borel probability measures are distinct, then the expectations of at least one continuous and bounded random variable with respect to these measures are not equal.</p></blockquote><p><strong>Proposition 1.7</strong>: </p><ul><li>For any distribution functions $F, F_1, F_2,\cdots$, we have $\mathbf{p}_{F_m}\stackrel{w}{\to} \mathbf{p}_F$ iff $F_m\stackrel{w}\to F$.</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Definitions&quot;&gt;&lt;a href=&quot;#Definitions&quot; class=&quot;headerlink&quot; title=&quot;Definitions&quot;&gt;&lt;/a&gt;Definitions&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Converge Weakly, Weak Limi</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.G Weak Convergence and Probability Limit" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-G-Weak-Convergence-and-Probability-Limit/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
  <entry>
    <title>Integration By Parts</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/11%20Expectation%20via%20the%20Stieltjes%20Integral/Integration-By-Parts/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/11%20Expectation%20via%20the%20Stieltjes%20Integral/Integration-By-Parts/</id>
    <published>2020-12-15T14:43:59.000Z</published>
    <updated>2021-01-02T05:47:22.110Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Integration by Parts Formula</strong>: (Stieltjes) </p><ul><li>Let $F$ and $\varphi$ be two increasing real functions on $[a,b]$. If $\varphi$ is $F$-integrable, then $F$ is $\varphi$-integrable, and</li></ul><p>$$<br>\int^b_a\varphi dF=\varphi(b)F(b)-\varphi(a)F(a)-\int^b_aFd\varphi<br>$$</p><p><strong>Proposition 3.1</strong>: </p><ul><li>Let $x$ be a nonnegative random variable on a probability space $(X,\Sigma, \mathbf{p})$. If $\varphi$ is an increasing and $F_x$-integrable self-map on $\mathbb{R} $, then</li></ul><p>$$<br>\mathbb{E}(\varphi\circ x)=\varphi(0)+\int^\infty_0(1-F_x)d\varphi<br>$$</p><ul><li>In particular,<br>$$<br>\mathbb{E}(x)=\int^\infty_0(1-F_x(t))dt<br>$$</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Propositions-and-Theorems&quot;&gt;&lt;a href=&quot;#Propositions-and-Theorems&quot; class=&quot;headerlink&quot; title=&quot;Propositions and Theorems&quot;&gt;&lt;/a&gt;Proposition</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
  <entry>
    <title>Expectation as a Stieltjes Integral</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/11%20Expectation%20via%20the%20Stieltjes%20Integral/Expectation-as-a-Stieltjes-Integral/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/11%20Expectation%20via%20the%20Stieltjes%20Integral/Expectation-as-a-Stieltjes-Integral/</id>
    <published>2020-12-15T14:42:59.000Z</published>
    <updated>2021-01-02T05:47:27.006Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Theorem 2.1</strong>: </p><ul><li>Let $x$ be a random variable and $\varphi$ an $F_x$-integrable self-map on $\mathbb{R} $. </li><li>If $\mathbb{E}(\varphi\circ x)$ exists, then</li></ul><p>$$<br>\mathbb{E}(\varphi\circ x)=\int_X(\varphi\circ x)d\mathbf{p}=\int_\mathbb{R}\varphi d\mathbf{p}<em>x=\int^\infty</em>{-\infty}\varphi dF_x<br>$$</p><p><strong>Corollary 2.2</strong>: </p><ul><li>Let $x$ be a nonnegative random variable and $\varphi$ an $F_x$-integrable self-map on $\mathbb{R} $. If $\mathbb{E}(\varphi\circ x)$ exists, then</li></ul><p>$$<br>\mathbb{\varphi\circ x}=\varphi(0)F_x(0)+\int^\infty_0\varphi dF_x<br>$$</p><p><strong>Corollary 2.3</strong>: </p><ul><li>Let $x$ be a random variable with a continuously differentiable distribution function $F_x$, and $f$ a density function for $F_x$. </li><li>Let $\varphi$ be an almost everywhere continuous and locally bounded self-map on $\mathbb{R} $. </li><li>If $\varphi$ is $F_x$-integrable and $\mathbb{E}(\varphi\circ x)$ exists, then</li></ul><p>$$<br>\mathbb{E}(\varphi\circ x)=\int^\infty_{-\infty}\varphi dF_x=\int^\infty_{-\infty}\varphi(t)f(t)dt<br>$$</p><ul><li><p>In particular, above equation holds for every continuous $\varphi : \mathbb{R}\to \mathbb{R}_+$. </p></li><li><p>Moreover, if $\mathbb{E}(x)$ exists,<br>$$<br>\mathbb{E}(x)=\int_Xxd\mathbf{p}=\int^\infty_{-\infty}tdF_x(t)=\int^\infty_{-\infty}tf(t)dt<br>$$</p></li></ul><p><strong>Proposition 2.4</strong>: </p><ul><li>Let $f : \mathbb{R}\to \mathbb{R}_+$ be a Borel measurable function. </li><li>If $f$ is Riemann integrable, then</li></ul><p>$$<br>\int_{\mathbb{R}}fd\ell=\int^\infty_{-\infty}f(t)dt<br>$$</p><ul><li>In particular, above equation holds when $f$ is bounded and continuous almost everywhere.</li></ul><p><strong>Corollary 2.5</strong>: </p><ul><li>Let $a$ and $b$ be real numbers with $a &lt; b$. </li><li>If $f : \mathbb{R}\to \mathbb{R}_+$ is Borel measurable and Riemann integrable (or bounded and continuous almost everywhere), then</li></ul><p>$$<br>\int_{[a,b]}fd\ell=\int^b_af(t)dt<br>$$</p><ul><li><p>Similarly, we have<br>$$<br>\int_{(-\infty,a]}fd\ell=\int^a_{-\infty}f(t)dt, \ \ \ and \ \ \ \int_{[b,\infty)}fd\ell=\int^\infty_bf(t)dt<br>$$</p></li><li><p>The Riemann and Lebesgue integrals of a Riemann integrable and nonnegative Borel measurable real function (defined on an interval) are equal.</p></li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Propositions-and-Theorems&quot;&gt;&lt;a href=&quot;#Propositions-and-Theorems&quot; class=&quot;headerlink&quot; title=&quot;Propositions and Theorems&quot;&gt;&lt;/a&gt;Proposition</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
  <entry>
    <title>The Stieltjes Integral</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/11%20Expectation%20via%20the%20Stieltjes%20Integral/The-Stieltjes-Integral/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/11%20Expectation%20via%20the%20Stieltjes%20Integral/The-Stieltjes-Integral/</id>
    <published>2020-12-15T14:41:59.000Z</published>
    <updated>2021-01-02T05:47:33.719Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Dissection, Subinterval, Division Point, Mesh</strong>: </p><ul><li><p>For two arbitrarily fixed real numbers $a$ and $b$ with $a &lt; b$, for any positive integer $m$ and real numbers $a_0,\cdots,a_m$ with $a=a_0 &lt;\cdots &lt;a_m =b$, we refer to the collection<br>$$<br>{[a_0,a_1], [a_1, a_2],\cdots,  [a_{m-1},a_m]}<br>$$</p></li><li><p>as a <strong>dissection</strong> of $[a,b]$, and denote it by either $\mathbf{a}$ or $[a_0,\cdots,a_m]$. </p></li><li><p>Any one of the intervals $[a_{i-1},a_i] $ is called a <strong>subinterval</strong> of $\mathbf{a}$, while any one of $a_i$s is called a <strong>division point</strong> of $\mathbf{a}$. </p></li><li><p>The maximum value of the lengths of its subintervals is called the <strong>mesh</strong> of $\mathbf{a}$-this value is denoted by $\mbox{mesh}(\mathbf{a})$.</p></li><li><p>The collection of all dissections of $[a, b]$ is denoted as $\mathcal{D}[a, b]$.</p></li></ul><p><strong>Finer than</strong>:</p><ul><li>For any dissections $\mathbf{a} = [a_0,\cdots,a_m]$ and $\mathbf{b} = [b_0,\cdots, b_k]$ of $[a, b]$, we write</li></ul><p>$$<br>\mathbf{a}\Cup\mathbf{b}<br>$$</p><ul><li>for the dissection $[c_0,\cdots, c_l]$ in $\mathcal{D}[a, b]$ where ${c_0,\cdots, c_l} = {a_0,\cdots, a_m}\cup{b_0,\cdots, b_k}$. </li><li>Moreover, we say that $\mathbf{b}$ is <strong>finer than</strong> $\mathbf{a}$ if ${a_0,\cdots, a_m} \subseteq {b_0,\cdots, b_k}$. </li><li>Evidently, $\mathbf{a}\Cup \mathbf{b} = \mathbf{b}$ iff $\mathbf{b}$ is finer than $\mathbf{a}$. </li><li>We also have $\mbox{mesh}(\mathbf{b})\leq \mbox{mesh}(\mathbf{a})$ if $\mathbf{b}$ is finer than $\mathbf{a}$, but not conversely.</li></ul><p><strong>Darboux Sum</strong>: </p><ul><li>Take any bounded real functions $\varphi$ and $F$ on $[a, b]$ with $F$ being increasing. </li><li>For any dissection $\mathbf{a} = [a_0,\cdots, a_m]$ of $[a, b]$, we define</li></ul><p>$$<br>\check{\varphi}<em>{\mathbf{a}}(i):=\sup{\varphi(t):a</em>{i-1}\leq t\leq a_i}<br>$$</p><ul><li><p>and<br>$$<br>\hat{\varphi}<em>{\mathbf{a}}(i):=\inf{\varphi(t):a</em>{i-1}\leq t\leq a_i}<br>$$</p></li><li><p>for each $i\in [m]$. (Since $\varphi$ is bounded, every one of these numbers is real.) </p></li><li><p>By a <strong>Darboux sum</strong> of $\varphi$ with respect to $F$ and $\mathbf{a}$, we mean a number like<br>$$<br>\sum_{i\in[m]}\alpha_i(F(a_i)-F(a_{i-1}))<br>$$</p></li><li><p>where $\hat{\varphi}_{\mathbf{a}} (i)\leq \alpha_i\leq \check{\varphi}_{\mathbf{a}}(i)$ for each $i$.</p></li><li><p>In particular, the <strong>$\mathbf{a}$-upper Darboux sum</strong> of $\varphi$ with respect to $F$ is defined as the number<br>$$<br>\mathbf{S}<em>{F,\mathbf{a}}(\varphi):=\sum</em>{i\in[m]}\check{\varphi}_{\mathbf{a}}(i)(F(a_i)-F(a_{i-1}))<br>$$</p></li><li><p>and the <strong>$\mathbf{a}$-lower Darboux sum</strong> of $\varphi$ with respect to $F$ is defined dually as<br>$$<br>\mathbf{s}<em>{F,\mathbf{a}}(\varphi):=\sum</em>{i\in[m]}\hat{\varphi}_{\mathbf{a}}(i)(F(a_i)-F(a_{i-1}))<br>$$</p></li><li><p>Clearly, $\mathbf{S}<em>{F,\mathbf{a}}(\varphi)$ decreases, and $\mathbf{s}</em>{F,\mathbf{a}}(\varphi)$ increases, as a becomes finer. </p></li><li><p>Evidently, we always have $\mathbf{S}<em>{F,\mathbf{a}}(\varphi)\geq\mathbf{s}</em>{F,\mathbf{a}}(\varphi) $. </p></li><li><p>Furthermore, and this is important, we have<br>$$<br>\inf{\mathbf{S}<em>{F,\mathbf{a}}(\varphi):\mathbf{a}\in\mathcal{D}[a,b]}\geq\sup{\mathbf{s}</em>{F,\mathbf{a}}(\varphi):\mathbf{a}\in\mathcal{D}[a,b]}<br>$$</p></li><li><p>The number on the left-hand side of this inequality is called the <strong>upper Stieltjes integral of $\varphi$ with respect to</strong> $F$, and is denoted by $\mathcal{S}_F (\varphi)$. </p></li><li><p>Similarly, the number on the right-hand side is called the <strong>lower Stieltjes integrals of $\varphi$ with respect to $F$</strong>, and is denoted by $\mathbf{s}_F (\varphi)$. </p></li><li><p>Thus, our inequality reads:<br>$$<br>\mathcal{S}_F (\varphi)\geq\mathbf{s}_F (\varphi)<br>$$</p></li></ul><p><strong>Stiltjes Integrals, Stieltjes Integrable, $F$-integrable</strong>: </p><ul><li>Let $\varphi,F\in \mathbf{B}[a, b]$ and assume that $F$ is increasing. </li><li>Suppose that there exists a real number $\theta $ such that, for any $\varepsilon &gt; 0$, there exists a dissection $\mathbf{a}$ of $[a, b]$ with</li></ul><p>$$<br>|\mathbf{S}<em>{F,\mathbf{a}}(\varphi)-\theta|&lt;\varepsilon\mbox{ and }|\mathbf{s}</em>{F,\mathbf{a}}(\varphi)-\theta|&lt; \varepsilon<br>$$</p><ul><li><p>Then this number $\theta$ is unique, and  it is denoted by<br>$$<br>\int^b_a\varphi dF\mbox{ or }\int^b_a\varphi(t)dF(t)<br>$$</p></li><li><p>When it exists, we refer to this number as the <strong>Stieltjes integral of $\varphi$ with respect to $F$,</strong> and in that case, we say that $\varphi$ is <strong>Stieltjes integrable with respect to $F$</strong>, or simply, <strong>$F$-integrable</strong>. </p></li><li><p>If , $\varphi$ is $F$-integrable, we also define<br>$$<br>\int^a_b\varphi dF=-\int^b_a\varphi dF<br>$$</p></li><li><p>Finally, where $a\leq c\leq  d\leq  b$, we say that $\varphi$ is $F$-integrable on $[c,d]$, if $\varphi|<em>{[c,d]}$ is $F|</em>{[c,d]}$-integrable.</p></li><li><p>$\varphi$ is $F$-integrable iff the upper and lower Stieltjes integrals of $\varphi$ with respect to $F$ are equal, that is,<br>$$<br>\inf{\mathbf{S}<em>{F,\mathbf{a}}(\varphi) :\mathbf{a}\in\mathcal{D}[a, b]} = \sup{\mathbf{s}</em>{F,\mathbf{a}}(\varphi) : \mathbf{a} \in \mathcal{D}[a, b]}<br>$$</p></li></ul><blockquote><p>The following statements are equivalent:</p><ul><li><p>$\varphi$ is $F$-integrable,</p></li><li><p>$\mathbf{S}_F(\varphi) = \mathbf{s}_F(\varphi)$,</p></li><li><p>For every $\varepsilon &gt; 0$, there is a dissection $\mathbf{a}\in \mathcal{D}[a, b]$ such that<br>$$<br>\mathbf{S}<em>{F,\mathbf{a}}(\varphi) -\mathbf{s}</em>{F,\mathbf{a}}(\varphi)&lt;\varepsilon<br>$$</p></li></ul></blockquote><p><strong>Riemman Integrable, Riemann Intergral</strong>: </p><ul><li>The Riemann integral is a special case of the Stieltjes integral. </li><li>Formally, we say that $\varphi\in \mathbb{R}^{[a,b]}$ is <strong>Riemann integrable</strong> if it is $\mbox{id}_{[a,b]}$-integrable, and for any such $\varphi$, we define the <strong>Riemann integral</strong> of $\varphi$, denoted by</li></ul><p>$$<br>\int^b_a\varphi(t)dt<br>$$</p><ul><li>as the Stieltjes integral of $\varphi$ with respect to the identity function on $[a, b]$.</li></ul><p><strong>Continuous almost everywhere</strong>: </p><ul><li>Let us agree to call a real map $\varphi$ on $[a,b]$ continuous almost everywhere on $[a, b]$ if</li></ul><p>$$<br>d(\varphi):={t\in[a,b]:\varphi\mbox{ is not continuous at }t}<br>$$</p><ul><li>is a “small” set in the following sense: For every $\varepsilon &gt; 0$ there is an open subset $O$ of $[a,b]$ such that $ d(\varphi)\subseteq O$ and $\ell(O) &lt;\varepsilon$.</li></ul><blockquote><p>The totality of discontinuity points $\varphi$ fits within an open set of arbitrarily Lebesgue measure.</p></blockquote><p><strong>$F$-integrable, Improper Stieltjes Integral, Riemann Integrable</strong>: </p><ul><li><p>Let $\varphi$ and $F$ be any two self-maps on $\mathbb{R} $, and assume that $F$ is increasing. </p></li><li><p>Given any real numbers $a$ and $b$ with $a \leq b$, we say that $\varphi$ is <strong>$F$-integrable on $[a,b]$</strong>, if $\varphi$ is bounded on $[a,b]$ and $\int^b_a\varphi dF$ exists. (Note. If $\varphi$ is $F$-integrable on $[a,b]$, then $\int^b_a\varphi dF$ is a real number.) </p></li><li><p>In turn, we say that $\varphi$ is <strong>$F$-integrable on $[a,\infty)$</strong> if $\varphi$ is $F$-integrable on $[a,b]$ for each $b\geq a$, and $\lim_{b\to \infty}\int^b_a\varphi dF\in \overline{\mathbb{R} }$</p></li><li><p>In this case we define the <strong>(improper) Stieltjes integral</strong> of $\varphi$ with respect to $F$ on this unbounded interval as<br>$$<br>\int^\infty_a\varphi dF:=\lim_{c\to \infty}\int^c_a\varphi dF.<br>$$</p></li><li><p>The $F $-integrability of $\varphi$ on $(-\infty,a]$ and the extended real number $\int^a_{-\infty}\varphi dF$ is analogously defined. </p></li><li><p>Finally, we say that $\varphi$ is <strong>$F$-integrable</strong>, if there exists a real number a such that $\varphi$ is $F$-integrable on both $(-\infty,a]$ and $[a,\infty)$, and the expression $\int^a_\infty \varphi dF+\int^\infty_a\varphi dF$ is not of the indeterminate form $\infty-\infty$. </p></li><li><p>In this case, we define<br>$$<br>\int^\infty_{-\infty}\varphi dF:=\int^a_{-\infty}\varphi dF+\int^\infty_{a}\varphi dF,<br>$$</p></li><li><p>and note that this definition allows $\int^\infty_{-\infty}\varphi dF$ to equal $-\infty$ or $\infty$. </p></li><li><p>If $\varphi$ is $id_{\mathbb{R} }$-integrable, then we say that it is <strong>Riemann integrable</strong>.</p></li></ul><blockquote><p>The $F $-integrability of $\varphi$ implies that, on every compact interval, $\varphi$ is $F $-integrable. In particular, such a $\varphi$ is bounded on every compact interval. </p><p>A self-map on $\mathbb{R} $ with this property is said to be locally bounded.</p></blockquote><p><strong>Continous Almost Everywhere</strong>: </p><ul><li>The notion of almost everywhere continuity is extended to self-maps on $\mathbb{R} $ in the obvious way. </li><li>Put precisely, we say that $\varphi$ is <strong>continuous almost everywhere</strong> (or almost everywhere continuous) if, for every $\varepsilon &gt; 0$, there is an open subset $O$ of $\mathbb{R} $ such that $\ell(O) &lt; \varepsilon$ and every point of discontinuity of $\varphi$ is contained in $O$. </li></ul><blockquote><p>Obviously, if $\varphi$ is continuous almost everywhere, then it is continuous almost everywhere on any compact interval.</p></blockquote><p><strong>Density, Density Function</strong>: </p><ul><li>We say that a distribution function $F$ has <strong>density</strong>, if there exists an almost everywhere continuous map $f : \mathbb{R}\to \mathbb{R}_+$ such that $f$ is locally bounded and</li></ul><p>$$<br>F(s)=\int^s_{-\infty}f(t)dt<br>$$</p><ul><li>for every real number $s$. </li><li>In this case, we say that $f$ is a <strong>density function</strong> for $F$.</li></ul><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Lemma 1.1</strong>: </p><ul><li>Let $\varphi,\ F\in \mathbf{B}[a,b]$ be such that $F$ is increasing. If $\varphi$ is $F$-integrable, then,</li></ul><p>$$<br>\int^b_a\varphi dF=\int^c_a\varphi dF+\int^b_c\varphi dF,\ \ \ \  a\leq c\leq b<br>$$</p><ul><li>The same conclusion also holds if $\varphi$ is $F $-integrable on both $[a, c]$ and $[c, b]$.</li></ul><blockquote><p>The Stieltjes integral is additive with respect to the interval of integration. </p></blockquote><p><strong>Lemma 1.2</strong>: </p><ul><li>Assume $a &lt; b$, and take any $\varphi, F\in \mathbf{B}[a,b]$ with $F$ being increasing. </li><li>If $F$ is continuous and $\varphi$ is $F $-integrable. Then, </li></ul><p>$$<br>\int^{\frac{b-1}{m}}_a\varphi dF\to\int^b_a\varphi dF\ \ \ and\  \ \ \ \int^b_{\frac{a+1}{m}}\varphi dF\to\int^b_a\varphi dF<br>$$</p><blockquote><p>When the integrator is a continuous function, altering the value of an integrable function at a single point does not alter the value of its integral</p></blockquote><p><strong>Lemma 1.3</strong>: </p><ul><li>Let $\varphi,\ F\in \mathbf{B}[a, b]$ be such that $F$ is increasing and $\varphi$ is $F $-integrable. </li><li>If $\varphi\geq 0$ almost everywhere on $[a, b]$, then</li></ul><p>$$<br>\int^b_a\varphi dF\geq0<br>$$</p><p><strong>Corollary 1.4</strong>: </p><ul><li>Let $\varphi, F \in \mathbf{B}[a, b]$ be such that $F$ is increasing and $\varphi$ is $F $-integrable. If $\varphi$ vanishes almost everywhere on $[a,b]$, then</li></ul><p>$$<br>\int^b_a\varphi  dF=0<br>$$</p><p><strong>Lemma 1.5</strong>: </p><ul><li>Let $\varphi, F \in \mathbf{B}[a, b]$ be such that $F$ is increasing, and both $\varphi$ and $\phi$ are $F$-integrable. Then, for any real number $\alpha$, we have</li></ul><p>$$<br>\int^b_a(\alpha\varphi+\phi)dF=\alpha\int^b_a\varphi dF+\int^b_a\phi dF<br>$$</p><blockquote><p>The Stieltjes integral is linear with respect to its integrand. </p></blockquote><p><strong>Proposition 1.6</strong>: (Stieltjes) </p><ul><li>Take any $\varphi, F\in \mathbf{B}[a, b]$ with $F$ being increasing. </li><li>If $\varphi$ is continuous, then it is $F$-integrable.</li></ul><p><strong>Proposition 1.7</strong>: </p><ul><li>Let $\varphi, F \in \mathbf{B}[a, b]$ and assume that $F$ is increasing and Lipschitz continuous. </li><li>If $\varphi$ is continuous almost everywhere on $[a,b]$, then it is $F $-integrable.</li></ul><p><strong>The Lebesgue Criterion</strong>: </p><ul><li>If $\varphi \in \mathbf{B}[a,b]$ is continuous almost everywhere on $[a,b] $, then it is Riemann integrable.</li></ul><blockquote><p>Riemann integrability = Continuity almost everywhere.</p></blockquote><p><strong>Corollary 1.8</strong>: </p><ul><li>Let $\varphi\in \mathbf{B}[a, b]$ be continuous almost everywhere on $[a, b]$. If $\varphi$ vanishes almost everywhere on $[a, b]$, then</li></ul><p>$$<br>\int^b_a\varphi(t)dt=0<br>$$</p><p><strong>The Fundamental Theorem of Calculus 1</strong>: </p><ul><li>Let $f \in \mathbf{B}[a, b]$ be continuous almost everywhere and $F\in \mathbb{R}^{[a,b]}$ satisfy</li></ul><p>$$<br>F(s)=F(a)+\int^s_af(r)dr,\ \ \ \ a\leq s\leq b<br>$$</p><ul><li>Then, $F$ is Lipschitz continuous on $[a,b]$, and for every $s\in [a,b]$ at which $f$ is continuous, $F’(s)$ exists and equals $f(s)$.</li></ul><p><strong>The Fundamental Theorem of Calculus 2</strong>: </p><ul><li><p>Take any $F\in \mathbf{C}^1[a,b]$, and let $f \in \mathbf{B}[a,b]$ is a Riemann integrable function such that $F’ = f$ almost everywhere on $[a, b]$. </p></li><li><p>Then we have<br>$$<br>F(s)=F(a)+\int^s_af(r)dr,\ \ \ \ a\leq s\leq b<br>$$</p></li></ul><blockquote><p>Roughly speaking, The Fundamental Theorem of Calculus says that we can think of dif ferentiation and Riemann integration as inverse operations.</p></blockquote><p><strong>Theorem 1.10</strong>: </p><ul><li>Let $\varphi\in \mathbf{B}[a, b]$ be continuous almost everywhere on $[a, b]$. </li><li>Then, for any increasing $F\in \mathbf{C}^1[a,b]$, we have</li></ul><p>$$<br>\int^b_a\varphi dF=\int^b_a\varphi(t)F’(t)dt<br>$$</p><p><strong>Corollary 1.11</strong>: </p><ul><li><p>Let $f,\varphi\in \mathbf{B}[a,b]$ be continuous almost everywhere on $[a,b]$ and assume $f \geq 0$. </p></li><li><p>Then, for any $F\in \mathbf{C}^1[a, b]$ such that<br>$$<br>F(s)=F(a)+\int^s_af(r)dr,\ \ \ \ a\leq s\leq b<br>$$</p></li><li><p>holds, we have</p></li></ul><p>$$<br>\int^b_a\varphi dF=\int^b_a\varphi(t)f(t)dt<br>$$</p><blockquote><p>Theorem 1.10 and Corollary 1.11 help to reduce a Stieltjes Integral to a Riemann Integral, gaining computational power.</p></blockquote><p><strong>Proposition 1.12</strong>: </p><ul><li>Let $\varphi$ and $F$ be two self-maps on $\mathbb{R} $, and assume that $F$ is increasing and continuously differentiable. </li><li>If $\varphi$ is nonnegative, locally bounded, and continuous almost everywhere, then it is $F $-integrable.</li></ul><p><strong>Theorem 1.13</strong>: </p><ul><li>Let $F\in \mathbf{C}^1(\mathbb{R} )$ be a distribution function, and $\varphi$ a locally bounded self-map on $\mathbb{R} $ that is continuous almost everywhere. </li><li>If $f$ is a density function for $F $, then</li></ul><p>$$<br>\int^\infty_{-\infty}\varphi dF=\int^\infty_{-\infty}\varphi(t)f(t)dt<br>$$</p><ul><li>in the sense that if one side exists so does the other.</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Definitions&quot;&gt;&lt;a href=&quot;#Definitions&quot; class=&quot;headerlink&quot; title=&quot;Definitions&quot;&gt;&lt;/a&gt;Definitions&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Dissection, Subinterval, D</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
  <entry>
    <title>Spaces of Integrable Random Variables</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/10%20Expectation%20via%20the%20Lebesgue%20Integral/Spaces-of-Integrable-Random-Variables/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/10%20Expectation%20via%20the%20Lebesgue%20Integral/Spaces-of-Integrable-Random-Variables/</id>
    <published>2020-12-15T13:55:59.000Z</published>
    <updated>2021-01-02T05:47:01.908Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>$p$-Integrable</strong></p><ul><li><p>Let $(X,\Sigma, \mathbf{p})$ be a measure space. </p></li><li><p>For any real number $p\geq 1$, we define $\mathcal{L}^p(X,\Sigma,\mu)$ as the set of all random variables $f$ on $(X,\Sigma,\mu)$ such that $|f|^p$ is integrable. </p></li><li><p>In other words,<br>$$<br>\mathcal{L}^p(X,\Sigma,\mu):=\left{f\in\mathcal{L}^0(X,\Sigma):\int_X|f|^pd\mu&lt;\infty\right}<br>$$</p></li><li><p>Any one member of $\mathcal{L}^p(X,\Sigma,\mu)$ is said to be $p$-<strong>integrable</strong>.</p></li></ul><blockquote><p>There is a natural way of making $\mathcal{L}^p(X,\Sigma,\mu)$ a seminormed linear space.</p><p>We define the real map $|\cdot|_p$ on $\mathcal{L}^p(X,\Sigma,\mu)$ by<br>$$<br>|f|_p:=\left(\int_X|f|^pd\mu\right)^\frac{1}{p}<br>$$<br>It is not a normed linear space proper, because $|\cdot|_p$ identifies any two random variables that are distinct from each other only on a negligible set with respect to $\mu$. </p><p>In other words, the map $(f, g) \mapsto |f- g|_p$ is a semimetric, but it is not a metric, for it fails to separate points in $\mathcal{L}^p(X,\Sigma,\mu)$. </p><p>The problem is that $|f|<em>p =0$ does not yield $f =\mathbf{0}$, it implies only that $f =</em>{a.s.} \mathbf{0}$.</p></blockquote><p><strong>$\mathcal{L}^p$-bounded</strong>:</p><ul><li>For any given real number $p\geq1$, we say that a set $\mathcal{X}$ of random variables on a given probability space is <strong>$\mathcal{L}^p$-bounded</strong> if either it is empty or<br>$$<br>\sup{\mathbb{E}(|x|^p):x\in\mathcal{X}}&lt;\infty<br>$$</li></ul><p><strong>$\mathcal{L}^p$-convergence</strong>:</p><ul><li><p>For any given real number $p\geq1$, let $x, x_1,x_2,\dots$ be random variables on $\mathcal{L}^p(X,\Sigma,\mu)$</p></li><li><p>We say that $x_m$ is <strong>$\mathcal{L}^p$-convergent</strong> if<br>$$<br>\lim_{m\to\infty}|x_m-x|<em>p=\lim</em>{m\to\infty}\left(\int_X|x_m-x|^pd\mu\right)^\frac{1}{p}=\lim_{m\to\infty}\mathbb{E}(|x_m-x|^p)=0<br>$$</p></li><li><p>$\mathcal{L}^1$-convergence is <strong>called convergence in the mean</strong>.</p></li></ul><blockquote><p>$\mathcal{L}^p$-convergence is a natural notion of convergence for sequences in this semimetric space.</p></blockquote><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 5.4</strong>: </p><ul><li>Let $X$ be a nonempty set of random variables on a probability space $(X,\Sigma,\mathbf{p})$. </li><li>If $X$ is uniformly integrable, then it is $\mathcal{L}^1$-bounded.</li></ul><p><strong>Proposition 5.5</strong>:</p><ul><li>Let $X$ be a set of random variables on a probability space $(X,\Sigma,\mathbf{p})$.  </li><li>If $X$ is $\mathcal{L}^p$-bounded for some $p &gt; 1$, then it is uniformly integrable.</li></ul><blockquote><p>$$<br>\mathcal{L}^p\mbox{-boundeness}\ \ (p&gt;1)\Longrightarrow\mbox{uniform integrability}\Longrightarrow\mathcal{L}^1\mbox{-boundeness}<br>$$</p></blockquote><p><strong>Proposition 5.6</strong>:</p><ul><li>Let $x, x_1,x_2,\dots$ be integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$ such that $\mathbb{E}(|x_m -x|) \to 0$. </li><li>Then, $(x_m)$ is uniformly integrable.</li></ul><blockquote><p>$$<br>\mathcal{L}^1\mbox{-convergence}\Longrightarrow\mbox{uniform integrability}\Longrightarrow\mathcal{L}^1\mbox{-boundeness}<br>$$</p></blockquote><p><strong>Proposition 5.7</strong>: </p><ul><li>Let $(x_m)$ be a uniformly integrable sequence of random variables on a probability space $(X,\Sigma, \mathbf{p})$ such that $x_m\to_{a.s.} x$ for some $x \in \mathcal{L}^0(X,\Sigma)$</li><li>Then, $x$ is integrable and $\mathbb{E}(|x_m-x |) \to 0$.</li></ul><blockquote><p>Almost sure convergence does imply $\mathcal{L}^1$-convergence for uniformly integrable sequences.</p></blockquote><p><strong>Corollary 5.8</strong>: </p><ul><li>Let $x, x_1, x_2,\dots$ be integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$ such that $x_m\to_{a.s.} x$. </li><li>Then, $\mathbb{E}(|x_m-x|) \to 0$ iff, $(x_m)$ is uniformly integrable.</li></ul><h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example 5.1</strong>: </p><ul><li>Almost sure convergence does not imply $\mathcal{L}^1$-convergence. </li><li>Consider the sequence $(x_m)$ of random variables on the probability space  $([0,1],\mathcal{B}[0,1],\ell)$ where $x_m$ equals $m$ on $[0,\frac{1}{m}]$ and $0$ elsewhere on $[0,1]$. </li><li>Then, $x_m(\omega) \to0$ for each $\omega\in (0,1]$, and hence, $x_m \to_{a.s.} 0$. </li><li>But $|x_m|_1 = 1$ for each $m$.</li></ul><p><strong>Example 5.2</strong>: </p><ul><li>$\mathcal{L}^1$-convergence does not imply almost sure convergence. </li><li>Consider $(x_m) := (\mathbf{1}<em>{[0,1)},\mathbf{1}</em>{[0,\frac{1}{2})}, \mathbf{1}<em>{[\frac{1}{2},1)},\mathbf{1}</em>{[0,\frac{1}{3})}, \mathbf{1}<em>{[\frac{1}{3},\frac{2}{3})},\mathbf{1}</em>{[\frac{2}{3},1)},\dots),$ which is a sequence in $\mathcal{L}^1([0,1),\mathcal{B}[0,1),\ell)$. </li><li>Clearly, we have $\mathbb{E}(x_m)\to 0$, that is, $|x_m|_1\to 0$. </li><li>But $ (x_m(\omega))$ does not converge to a real number for any $\omega$ in $[0, 1)$.</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Definitions&quot;&gt;&lt;a href=&quot;#Definitions&quot; class=&quot;headerlink&quot; title=&quot;Definitions&quot;&gt;&lt;/a&gt;Definitions&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;$p$-Integrable&lt;/strong&gt;&lt;/p</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
  <entry>
    <title>Elementary Probability Inequalities</title>
    <link href="https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/10%20Expectation%20via%20the%20Lebesgue%20Integral/Elementary-Probability-Inequalities/"/>
    <id>https://hqin-2020.github.io/2020/12/15/Mathematics/Analysis/10%20Expectation%20via%20the%20Lebesgue%20Integral/Elementary-Probability-Inequalities/</id>
    <published>2020-12-15T13:54:59.000Z</published>
    <updated>2021-01-02T05:46:51.258Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Jensen’s Inequality</strong>:</p><ul><li><p>Let $x$ be an integrable random variable on a probability space $(X,\Sigma, \mathbf{p})$, $I$ an open interval that contains $x(X)$, and $\varphi : I \to \mathbb{R}$ a concave function. </p></li><li><p>Then, $\varphi \circ x \in \mathcal{L}^0(X ,\Sigma)$ and<br>$$<br>\mathbb{E}(\varphi\circ x) \leq \varphi(\mathbb{E}(x))<br>$$</p></li><li><p>If $\varphi$ is convex, then the inequality goes the other direction.</p></li></ul><blockquote><p>Under fairly general conditions, the expectation of a concave transformation of a random variable $x$ is always less than the same transformation of the expectation of $x$</p></blockquote><p><strong>Lemma 4.1</strong>:</p><ul><li><p>Let $Y$ be a metric space, and $x$ a $Y$-valued random variable on a probability space $(X,\Sigma, \mathbf{p})$. </p></li><li><p>Then, for any continuous $\varphi: Y \to \mathbb{R}_+$ and real number $\lambda &gt; 0$, we have<br>$$<br>\mathbf{p}{\varphi\circ x\geq \lambda}\leq\frac{1}{\lambda}\mathbb{E}(\varphi\circ x).<br>$$</p></li></ul><p><strong>Markov’s Inequality</strong>:</p><ul><li><p>Let $Y$ be a normed metric space, and $x$ a $Y$-valued random variable on a probability space $(X,\Sigma, \mathbf{p})$. </p></li><li><p>Then, for any real number $\lambda &gt; 0$, we have<br>$$<br>\mathbf{p}{| x|_Y\geq \lambda}\leq\frac{1}{\lambda}\mathbb{E}(| x|_Y).<br>$$</p></li><li><p>In particular, for any  random variable $x$ on $(x,\Sigma,\mathbf{p})$<br>$$<br>\mathbf{p}{| x|\geq \lambda}\leq\frac{1}{\lambda}\mathbb{E}(| x|).<br>$$</p></li></ul><p><strong>The Chebyshev-Bienaymé Inequality</strong>:</p><ul><li>For any random variable $x$ defined on a probability space $(X,\Sigma,\mathbf{p})$, and any real number $\lambda&gt; 0$, we have<br>$$<br>\mathbf{p}{| x|\geq \lambda}\leq\frac{1}{\lambda^2}\mathbb{E}(x^2).<br>$$</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li><li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li><li>Walter Rudin, 1987, “Real and Complex Analysis”; </li><li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Propositions-and-Theorems&quot;&gt;&lt;a href=&quot;#Propositions-and-Theorems&quot; class=&quot;headerlink&quot; title=&quot;Propositions and Theorems&quot;&gt;&lt;/a&gt;Proposition</summary>
      
    
    
    
    <category term="1 Mathematics" scheme="https://hqin-2020.github.io/categories/1-Mathematics/"/>
    
    <category term="1.1 Analysis" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/"/>
    
    <category term="1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral" scheme="https://hqin-2020.github.io/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/"/>
    
    
    <category term="Measure Theoretic Probability" scheme="https://hqin-2020.github.io/tags/Measure-Theoretic-Probability/"/>
    
  </entry>
  
</feed>
