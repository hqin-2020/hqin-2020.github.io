<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: 1.1.C Linear Spaces and Convexity - hqin</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="hqin"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="hqin"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="hqin"><meta property="og:url" content="https://hqin-2020.github.io/"><meta property="og:site_name" content="hqin"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://hqin-2020.github.io/img/og_image.png"><meta property="article:author" content="hqin"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hqin-2020.github.io"},"headline":"hqin","image":["https://hqin-2020.github.io/img/og_image.png"],"author":{"@type":"Person","name":"hqin"},"description":""}</script><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="hqin" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">hqin</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/1-Mathematics/">1 Mathematics</a></li><li><a href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a></li><li class="is-active"><a href="#" aria-current="page">1.1.C Linear Spaces and Convexity</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-10-26T10:51:53.000Z" title="10/26/2020, 6:51:53 PM">2020-10-26</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-C-Linear-Spaces-and-Convexity/">1.1.C Linear Spaces and Convexity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/10/26/Mathematics/Analysis/5%20Convexity/Convex-Sets/">Convex Sets</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=531B93>Convex Set</font></strong>: </p>
<ul>
<li>For any $0 &lt; λ &lt; 1$, a subset $S$ of a linear space $X$ is said to be <strong>$λ$-convex</strong> if</li>
</ul>
<p>$$<br>λx+(1−λ)y∈S\ \ \ \mbox{for any}\ \ \ x,y∈S<br>$$</p>
<ul>
<li><p>or equivalently, if<br>$$<br>λS+(1−λ)S=S<br>$$</p>
</li>
<li><p>If $S$ is $λ$-convex for all $0 &lt; λ &lt; 1$, that is, if<br>$$<br>λS+(1−λ)S= S\ \ \ \mbox{for all} \ \ \ \ 0≤λ≤1,<br>$$</p>
</li>
<li><p>then it is said to be a <strong><font color=531B93>convex set</font></strong>.</p>
</li>
</ul>
<blockquote>
<p>For any two distinct vectors $x$ and $y$ in the linear space $X$, we think of the set ${λx+(1−λ)y ∈ X : λ ∈ \mathbb{R} }$ as the line through $x$ and $y$. </p>
<p>The set ${λx+(1−λ)y ∈ X :0 ≤ λ ≤ 1}$ corresponds to the line segment between $x$ and $y$.</p>
<p>A subset $S$ of $X$ is is convex iff it contains the entire line segment between any two of its elements.</p>
</blockquote>
<blockquote>
<p>The affine hull of a convex set can in general be expressed much more easily than that of a nonconvex set. Indeed, for any convex subset $S$ of a linear space, we simply have<br>$$<br>aff (S) = {λx + (1 − λ)y : λ ∈ \mathbb{R} \mbox{ and } x, y ∈ S}.<br>$$</p>
</blockquote>
<p><strong><font color=941751>Convex Hull</font></strong>: </p>
<ul>
<li><p>Let $S$ be any set in a linear space $X$, and let $\mathcal{S}$ be the class of all convex subsets of $X$ that contain $S$.</p>
</li>
<li><p>We have $  \mathcal{S}\neq∅$ — after all, $X∈\mathcal{S}$. </p>
</li>
<li><p>$\bigcap \mathcal {S}$ is a convex set in $X$ which contains $S$. </p>
</li>
<li><p>This set is the smallest (that is, $⊇$-minimum) subset of $X$ that contains $S$ — it is called the <strong><font color=941751>convex hull</font></strong> of $S$, and denoted by $co(S)$. </p>
<ul>
<li>The closed convex hull is the intersection of all closed convex set containing $S$</li>
</ul>
</li>
<li><p>It is worth noting that the definition of $co(S)$ uses vectors that lie outside $S$ (because the members of $S$ may well contain such vectors). For this reason, this definition can be viewed as an external one. </p>
<ul>
<li>We may also characterize $co(S)$ internally, that is, by using only the vectors in $S$. </li>
<li>$co(S)$ is  the set of all convex combinations of finitely many members of $S$. That is,</li>
</ul>
<p>$$<br>co(S)=\left\{\sum^m_{i=1}\lambda_ix^i:m\in\mathbb{N},\ x^1,\dots,x^m\in S\ \mbox{and}\ (\lambda_1,\cdots,\lambda_m)\in[0,1]^m \mbox{ and } \sum^m_{i=1}\lambda_i=1 \right\}<br>$$</p>
</li>
</ul>
<blockquote>
<p>$S = co(S)$ iff $ S$ is convex.</p>
<p>Moreover, for any $x, y ∈ X$, we see that $co({x, y})$ — which is denoted simply as $co{x,y}$ henceforth — is nothing but the line segment between $x$ and $y$, and we have<br>$$<br>co(S) = \bigcup{co(T ) : T ∈ \mathcal{P}(S)},<br>$$<br>where $\mathcal{P}(S)$ is the class of all nonempty finite subsets of $S$.</p>
</blockquote>
<p><strong>Concave Function, Convex Function</strong>: </p>
<ul>
<li><p>Let $X$ be a linear space and $T$ a nonempty convex subset of $X$. </p>
</li>
<li><p>A real map $φ$ on $T$ is called <strong>concave</strong> if</p>
</li>
</ul>
<p>$$<br>φ(λx+(1−λ)y)≥λφ(x)+(1−λ)φ(y)\ \ \ \ \mbox{for all}\ \ \ \ x,y∈T\ \ \ \mbox{and} \ \ \ \ 0≤λ≤1,<br>$$</p>
<ul>
<li>while it is called <strong>convex</strong> if $−φ$ is concave. </li>
</ul>
<blockquote>
<p>If both $φ$ and $−φ$ are concave, then $φ$ is an affine map</p>
</blockquote>
<p><strong><font color=F27200>Cone</font>, <font color=F27200>Convex Cone</font></strong>: </p>
<ul>
<li><p>A nonempty subset $C$ of a linear space $X$ is said to be a **<font color=F27200>cone </font>**if it is closed under nonnegative scalar multiplication, that is, $λC ⊆ C$ for all $λ ≥ 0$, i.e.<br>$$<br>λx∈C\ \ \ \mbox{for any}\ \ \ x∈C\ \ \ \mbox{and}\ \ \ λ≥0.<br>$$</p>
</li>
<li><p>If $C$ is, in addition, closed under addition, that is, $C + C ⊆ C$, i.e.<br>$$<br>x+y∈C\ \ \ \ \mbox{for any}\ \ \ \ \ x,y∈C,<br>$$</p>
</li>
<li><p>then it is called a <strong><font color=F27200>convex cone</font></strong>. </p>
<ul>
<li>We say that a cone $C$ in $X$ is pointed if $C ∩ −C ={\mathbf{0} }$, generating if $\mbox{span}(C) = X$, and nontrivial if $C \neq {\mathbf{0}}$.</li>
</ul>
</li>
</ul>
<blockquote>
<p>Geometrically speaking, a cone is a set that contains all rays that start from the origin and pass through another member of the set. In turn, a convex cone is none other than a cone which is also convex set. </p>
<p>In a manner of speaking, the concept of convex cone lies in between that of a convex set and that of a linear subspace.</p>
<p>Any linear subspace $C$ of a linear space $X$ is a convex cone. </p>
<p>The smallest (i.e. $⊇$-minimum) convex cone in a linear space $X$ is the trivial linear subspace ${\mathbf{0}}$ while the largest (i.e. $⊇$-maximum) convex cone is $X$ itself. Moreover, if $C$ is a convex cone in this space, so is $−C$.</p>
<p>If $X$ and $Y$ are two linear spaces, and $L∈\mathcal{L}(X,Y)$, then ${x∈X:L(x)≥\mathbf{0}}$ is a (pointed) convex cone in $X$. All of these cones are infinite dimensional and generating.</p>
</blockquote>
<p><strong><font color=941751>Conical Hull</font></strong>: </p>
<ul>
<li><p>Let $S$ be any nonempty set in a linear space $X$. </p>
</li>
<li><p>It is easily checked that $S$ is a convex cone in $X$ iff $\sum_{x∈T} λ(x)x ∈ S$ for any nonempty finite subset $T$ of $S$ and $λ ∈ \mathbb{R}^T_+$. </p>
</li>
<li><p>It follows that the smallest convex cone that contains $S$ — the **<font color=941751>conical hull </font>**of $S$ — exists and equals the set of all positive linear combinations of finitely many members of $S$.  </p>
</li>
<li><p>Denoting this convex cone by $cone(S)$, therefore, we may write</p>
</li>
</ul>
<p>$$<br>cone(S)=\left\{\sum_{x\in T}λ(x)x:T ∈\mathcal{P}(S)\ \ \mbox{and}\ \ \ λ∈\mathbb{R}^T_+\right\}<br>$$</p>
<ul>
<li>where, as usual, $\mathcal{P}(S)$ stands for the class of all nonempty finite subsets of $S$. </li>
</ul>
<blockquote>
<p>By convention, we let $cone(∅) = {\mathbf{0} }$</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Examples</strong>:</p>
<ul>
<li>Note that $\mathbb{Q} $ is a midpoint convex subset of $\mathbb{R} $ which is not convex. </li>
<li>Indeed, a nonempty subset of $\mathbb{R}$ is convex iff it is an interval. </li>
<li>In any linear space $X$, all singleton sets, line segments and lines, along with $∅$, are convex sets. </li>
<li>Any linear subspace, affine manifold, hyperplane or half space in $X$ is also convex. </li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li>Both ${f∈\mathbf{C}^1[0,1]:f≥0}$ and $C:={f∈\mathbf{C}^1[0,1]:f’ ≥0}$ are convex cones in $\mathbf{C}^1[0, 1]$. The former one is pointed, but the latter is not — all constant functions belong to $C ∩ −C$.</li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li>For any $n ∈ \mathbb{N} $, $cone(\mathbb{R}^n_{++}) = \mathbb{R}^n_{++}∪ {\mathbf{0}}$ and $cone({\mathbf{e}^1, …, \mathbf{e}^n}) = \mathbb{R}^n_+$, where ${\mathbf{e}^1, …, \mathbf{e}^n}$ is the standard basis for $\mathbb{R}^n$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-10-26T09:41:54.000Z" title="10/26/2020, 5:41:54 PM">2020-10-26</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-C-Linear-Spaces-and-Convexity/">1.1.C Linear Spaces and Convexity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/10/26/Mathematics/Analysis/4%20Linear%20Spaces/Linear-Algebra/">Linear Algebra</a></h1><div class="content"><h1 id="Linear-Algebra"><a href="#Linear-Algebra" class="headerlink" title="Linear Algebra"></a>Linear Algebra</h1><p>A point in $\mathbb{R}^n$ is represented by an $n$-tuple of elements of $\mathbb{R}$, written $\mathbf{p} = (p_1, . . . , p_n)$, with each $p_i ∈ \mathbb{R}$. </p>
<p>Geometrically, these are thought of as points in space, where the $p_i$’s give the coordinates of the point $\mathbf{p}$. At the same time, we may consider $n$-tuples of real numbers as $n$-dimensional vectors giving the data of a direction and a magnitude, without specifying a base point from which this vector emanates. </p>
<p>Thinking this way, we see that such vectors are elements of a vector space, $\mathbb{E}^n$, where elements can be written as $\mathbf{v} = (v_1,…,v_n)$, with each $v_i ∈ \mathbb{R}$. We will consistently distinguish between the “points” of $\mathbb{R}^n$, and “vectors” in $\mathbb{E}^n$, since geometrically they are quite different. </p>
<p>Vectors are free to wander around in space, and points have to stay where they are.</p>
<p>vector + vector = vector, </p>
<p>point + vector = point,</p>
<p>point − point = vector</p>
<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=005493>Algebra</font></strong>:</p>
<ul>
<li>Let $F$ be a field. </li>
<li>An <strong><font color=005493>algebra</font></strong> over $F$ is a set $A$ such that $A$ is a vector space over $F$ and $A$ has an internal law of composition $◦$ satisfying the associative law, and left and right distributivity. </li>
<li>That is, for $a,b,c ∈ A$, we have<ol>
<li>(Associativity) $a ◦ (b ◦ c) = (a ◦ b) ◦ c$, </li>
<li>(Left Distributivity) $a ◦ (b + c) = (a ◦ b) + (a ◦ c)$ and </li>
<li>(Right Distributivity) $(a + b) ◦ c = (a ◦ c) + (b ◦ c)$.</li>
</ol>
</li>
<li>For scalar multiplication we have for $α ∈ F$ , <ol>
<li>$(α · a) ◦ b = α · (a ◦ b) = a ◦ (α · b)$.</li>
</ol>
</li>
<li>An algebra $A$ is an <strong>algebra with identity</strong> if there is an element $\mathbf{1} ∈ A$ so that $a◦\mathbf{1} = \mathbf{1}◦a = a$ for all $a ∈ A$. </li>
<li>The algebra $A$ is a <strong>commutative algebra</strong> if $a ◦ b = b ◦ a$ for all $a, b ∈ A$.</li>
</ul>
<p><strong>Line</strong>:</p>
<ul>
<li>Let $\mathbf{p}_0$ be a point in $\mathbb{R}^n$ and $\mathbf{v}$ be a direction in $\mathbb{E}^n$. </li>
<li>The line $\ell$ through $\mathbf{p}_0$ in the direction $\mathbf{v}$ is given by $\ell=\{\mathbf{p}∈\mathbb{R}^n |\mathbf{p}=\mathbf{p}_0 +t\mathbf{v}, t∈\mathbb{R}\}$.</li>
</ul>
<p><strong>Plane</strong>:</p>
<ul>
<li>Suppose that $\mathbf{p}_0\in\mathbb{R}^n$ and $\mathbf{v}, \mathbf{w}$ are linearly independent vectors in $\mathbb{E}^n$. </li>
<li>The <strong>plane</strong> through $\mathbf{p}_0$ spanned by $\mathbf{v}$ and $\mathbf{w}$ is $\mathcal{P} = {\mathbf{p} ∈ \mathbb{R}^n | \mathbf{p} = \mathbf{p}_0 + t\mathbf{v} + s\mathbf{w}, t, s ∈ \mathbb{R}}$. </li>
</ul>
<p><strong>Affine Subspace, Hyperplane</strong>:</p>
<ul>
<li>If $\mathbf{v}_1, . . . , \mathbf{v}_k$ are linearly independent vectors in $\mathbb{E}^n$, then we define the $k$-dimensional <strong>affine subspace</strong> through $\mathbf{p}_0\in\mathbb{R}^n$ spanned by $\mathbf{v}_1, . . . , \mathbf{v}_k$ as</li>
</ul>
<p>$\mathbf{H}=\{\mathbf{p}\in\mathbb{R}^n |\mathbf{p}=\mathbf{p}_0+t_1\mathbf{v}_1+…+t_k\mathbf{v}_k,\ \mbox{where}\ t_j ∈\mathbb{R},1≤j≤k\}$.</p>
<ul>
<li>The collection of vectors $\{t_1\mathbf{v}_1 + … + t_k\mathbf{v}_k,\ t_j ∈ \mathbb{R}\}$ is actually a subspace of $\mathbb{E}^n$. Thus, a $k$-dimensional affine subspace is constructed by taking a $k$-dimensional subspace of  $\mathbb{E}^n$ and adding it to a point of  $\mathbb{R}^n$. </li>
<li>When $k=n−1$, $\mathbf{H}$ is called a <strong>hyperplane</strong> in $\mathbb{R}^n$.</li>
</ul>
<p><strong>Parallelogram</strong>:</p>
<ul>
<li>If $\mathbf{v}_1, . . . , \mathbf{v}_k$ are linearly independent vectors in $\mathbb{E}^n$, and $\mathbf{p}_0\in\mathbb{R}^n$, we define the $k$-dimensional <strong>parallelepiped</strong> with vertex $\mathbf{p}_0$ spanned by $\mathbf{v}_1, . . . , \mathbf{v}_k$ as $\mathbf{P} = {\mathbf{p} ∈ \mathbb{R}^n | \mathbf{p} = \mathbf{p}_0 + t_1\mathbf{v}_1 + . . . + t_k\mathbf{v}_k,\ \mbox{with}\ 0 ≤ t_j ≤ 1}$.</li>
<li>Note that if $k = n = 2$ then $\mathbf{P}$ is just a standard <strong>parallelogram</strong> in $\mathbb{R}^2$.</li>
</ul>
<p><strong>Bilinear Form</strong>:</p>
<ul>
<li><p>Let $V$ be a vector space over a field $F$ . A <strong>bilinear form</strong> $⟨·, ·⟩$ on $V$ is a map $⟨·, ·⟩ : V × V → F$ which satisfies linearity in both variables.</p>
</li>
<li><p>That is, for all $\mathbf{v},\ \mathbf{v}_1,\ \mathbf{v}_2,\ \mathbf{w},\ \mathbf{w}_1, \mathbf{w}_2 ∈V$, and all $α∈F$,</p>
<ol>
<li>$⟨\mathbf{v}_1 +\mathbf{v}_2,\mathbf{w}⟩=⟨\mathbf{v}_1,\mathbf{w}⟩+⟨\mathbf{v}_2,\mathbf{w}⟩ $</li>
<li>$⟨α\mathbf{v}, \mathbf{w}⟩ = α⟨\mathbf{v}, \mathbf{w}⟩$</li>
<li>$⟨\mathbf{v},\mathbf{w}_1 +\mathbf{w}_2⟩=⟨\mathbf{v},\mathbf{w}_1⟩+⟨\mathbf{v},\mathbf{w}_2⟩ $</li>
<li>$⟨\mathbf{v}, α\mathbf{w}⟩ = α⟨\mathbf{v}, \mathbf{w}⟩$.</li>
</ol>
</li>
<li><p>The form $⟨·, ·⟩$ is said to be symmetric if $⟨\mathbf{v}, \mathbf{w}⟩ = ⟨\mathbf{w}, \mathbf{v}⟩$ for all $\mathbf{v}, \mathbf{w} ∈ V$ .</p>
</li>
</ul>
<p><strong>Positive Definite</strong>:</p>
<ul>
<li>Let $V$ be a vector space over $\mathbb{R}$. </li>
<li>The bilinear form $⟨·, ·⟩$ is said to be <strong>positive definite</strong> if $⟨\mathbf{v}, \mathbf{v}⟩ ≥ 0$ for all $\mathbf{v} ∈ V$ , and $⟨\mathbf{v}, \mathbf{v}⟩ = 0$ iff $\mathbf{v} = 0$.</li>
</ul>
<p><strong>Scalar Product, Dot Product</strong>:</p>
<ul>
<li>Suppose that $\mathbf{v} = (\mathbf{v}_1,…,\mathbf{v}_n)$ and $\mathbf{w} = (\mathbf{w}_1,…,\mathbf{w}_n)$ are vectors in $\mathbb{E}^n$. The <strong>scalar product</strong> of $\mathbf{v}$ and $\mathbf{w}$ is $⟨\mathbf{v}, \mathbf{w}⟩ = \mathbf{v}_1\mathbf{w}_1 + . . . + \mathbf{v}_n\mathbf{w}_n$. </li>
<li>The scalar product is sometimes called the <strong>dot product</strong> and is denoted by $\mathbf{v}· \mathbf{w}$. </li>
</ul>
<p><strong>Length, Norm</strong>:</p>
<ul>
<li>If $\mathbf{v} = (\mathbf{v}_1,…,\mathbf{v}_n) ∈ \mathbb{E}^n$, then the <strong>length</strong> or <strong>norm</strong> of $\mathbf{v}$ is defined by $\|\mathbf{v}\|=\sqrt{⟨\mathbf{v}, \mathbf{v}⟩}=(v_1^2+\dots+v_n^2)^\frac{1}{2}$</li>
</ul>
<p><strong>Orthogonal, Mutually Orthogonal</strong>:</p>
<ul>
<li>Let $\mathbf{v}, \mathbf{w} ∈ \mathbb{E}^n$. Then $\mathbf{v}$ and $\mathbf{w}$ are said to be <strong>orthogonal</strong> if $⟨\mathbf{v}, \mathbf{w}⟩ = 0$. </li>
<li>A set $\{\mathbf{v}_1,…,\mathbf{v}_n\}$ of vectors in $\mathbb{E}^n$ is said to be <strong>mutually orthogonal</strong> or <strong>pairwise orthogonal</strong> if $⟨\mathbf{v}_i, \mathbf{v}_j ⟩ = 0$ for all pairs $i,j$ with $i\neq j$.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Theorem 2.5.12</strong>: (Cauchy-Schwarz Inequality) </p>
<ul>
<li>Let $\mathbf{v}, \mathbf{w} ∈ \mathbb{E}^n$ . Then $|⟨\mathbf{v}, \mathbf{w}⟩| ≤ \|\mathbf{v}\|\|\mathbf{w}\|$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-10-26T09:41:53.000Z" title="10/26/2020, 5:41:53 PM">2020-10-26</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-C-Linear-Spaces-and-Convexity/">1.1.C Linear Spaces and Convexity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/10/26/Mathematics/Analysis/4%20Linear%20Spaces/Linear-Operators-and-Functionals/">Linear Operators and Functionals</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=F27200>Linear Operator, Linear Transformation</font></strong>: </p>
<ul>
<li>Let $X$ and $Y$ be two linear spaces. </li>
<li>A function $L : X → Y$ is called a <strong><font color=F27200>linear operator</font></strong> (or a <strong><font color=F27200>linear transformation</font></strong>) if</li>
</ul>
<p>$$<br>L(α\mathbf{x}+\mathbf{x}’)=αL(\mathbf{x})+L(\mathbf{x}’)\ \ \ \mbox{for all}\ \ \ \ \mathbf{x},\mathbf{x}’ ∈X\ \ \ \mbox{and}\ \ \ \ α∈\mathbb{R}<br>$$</p>
<ul>
<li>or equivalently,<br>$$<br>L\left(\sum^m_{i=1}\alpha_i\mathbf{x}^i\right)=\sum^m_{i=1} α_iL(\mathbf{x}^i)\ \ \ \ \ \mbox{for all}\ \ \ \ m ∈ \mathbb{N}\ \ \ \ \ \mbox{and}\ \ \ \ (\mathbf{x}^i, α_i) ∈ X × \mathbb{R},\ \ \ i = 1, …, m.<br>$$</li>
</ul>
<blockquote>
<p>As is customary, we don’t adopt a notation that makes this explicit</p>
<p>The $+$ operation on the left of the equation in<br>$$<br>L(α\mathbf{x}+\mathbf{x}’)=αL(\mathbf{x})+L(\mathbf{x}’)\ \ \ \mbox{for all}\ \ \ \ \mathbf{x},\mathbf{x}’ ∈X\ \ \ \mbox{and}\ \ \ \ α∈\mathbb{R}<br>$$<br>is not the same as the $+$ operation on the right of this equation. Indeed, the former one is the addition operation on $X$ while the latter is that on $Y$. </p>
<p>The same comment applies to the $\cdot $ operation as well — we use the scalar multiplication operation on $X$ when writing $αx$, and that on $Y$ when writing $αL(x)$.</p>
</blockquote>
<p><strong>Null Space, Kernel</strong>: </p>
<ul>
<li>The set $L^{−1}(0)$ is called the null space (or the kernel) of $L$, and is denoted by $null(L)$, that is,</li>
</ul>
<p>$$<br>null(L):={x∈X :L(x)=0}.<br>$$</p>
<p><strong><font color=941751>Linear Funtional</font></strong>: </p>
<ul>
<li>A real-valued linear operator is referred to as a **<font color=941751>linear functional </font>**on $X$.</li>
</ul>
<blockquote>
<p>The set of all linear operators from a linear space $X$ into a linear space $Y$ is denoted as $\mathcal{L}(X,Y)$. So, $L$ is a linear functional on $X$ iff $L ∈ \mathcal{L}(X,\mathbb{R} )$.</p>
</blockquote>
<p><strong>Linear Correspondence</strong>: </p>
<ul>
<li>Let $X$ and $Y$ be two linear spaces, and $Γ : X \rightrightarrows  Y$ a correspondence. We say that $Γ$ is a linear correspondence if</li>
</ul>
<p>$$<br>αΓ(x) ⊆ Γ(αx)\ \ \ \ \mbox{and}\ \ \ \ Γ(x) + Γ(x’) ⊆ Γ(x + x’)<br>$$</p>
<ul>
<li>for all $x, x’ ∈ X$ and $α ∈ \mathbb{R} $.</li>
</ul>
<p><strong><font color=941751>Linear Function</font></strong>:  </p>
<ul>
<li>Let $S$ be a nonempty subset of a linear space $X$, and denote by $\mathcal{P}(S)$ the class of all nonempty finite subsets of $S$. </li>
<li>A real map $φ ∈ \mathbb{R}^S$ is linear if</li>
</ul>
<p>$$<br>φ\left(\sum_{\mathbf{x}\in A}λ(\mathbf{x})\mathbf{x}\right) = \sum_{\mathbf{x}\in A}λ(\mathbf{x})φ(\mathbf{x})<br>$$</p>
<ul>
<li>for any $A∈\mathcal{P}(S)$ and $λ∈\mathbb{R}^A $ such that $\sum_{\mathbf{x}∈A}λ(\mathbf{x})\mathbf{x}∈S$.</li>
</ul>
<p><strong>Affine Function</strong>: </p>
<ul>
<li>Let $S$ be a nonempty subset of a linear space $X$, and denote by $\mathcal{P}(S)$ the class of all nonempty finite subsets of $S$. </li>
<li>A real map $φ ∈ \mathbb{R}^S$ is called <strong>affine</strong> if</li>
</ul>
<p>$$<br>φ\left(\sum_{x\in A}λ(\mathbf{x})\mathbf{x}\right) = \sum_{\mathbf{x}\in A}λ(\mathbf{x})φ(\mathbf{x})<br>$$</p>
<ul>
<li>for any $A∈\mathcal{P}(S)$ and $λ∈\mathbb{R}^A$ such that $\sum_{\mathbf{x}∈A}λ(\mathbf{x})=1$ and $\sum_{\mathbf{x}∈A}λ(\mathbf{x})\mathbf{x}∈S$.</li>
</ul>
<p><strong>Linear Isomorphism</strong>: </p>
<ul>
<li>Let $X$ and $Y$ be two linear spaces and $L ∈ \mathcal{L}(X,Y)$. </li>
<li>If $L$ is a bijection, then it is called a <strong>linear isomorphism</strong> between $X$ and $Y$, and we say that $X$ and $Y$ are <strong>isomorphic</strong>.</li>
</ul>
<blockquote>
<p>Just like two isometric metric spaces are indistinguishable from each other insofar as their metric properties are concerned, the linear algebraic structures of two isomorphic linear spaces coincide. </p>
<p>Put differently, from the perspective of linear algebra, one can regard two isomorphic linear spaces as differing from each other only in the labelling of their constituent vectors.</p>
<p>It is quite intuitive that a finite dimensional linear space can never be isomorphic to an infinite dimensional linear space. But there is more to the story. Given the interpretation of the notion of isomorphism, you might expect that the dimensions of any two isomorphic linear spaces must in fact be identical. </p>
</blockquote>
<p><strong>Closed Halfspace, Open Halfspace</strong>: </p>
<ul>
<li><p>Given Corollary 4, we may identify a hyperplane $H$ in a linear space $X$ with a real number and a nonzero linear functional $L$ on $X$. </p>
</li>
<li><p>This allows us to give an analytic form to the intuition that a hyperplane “divides” the entire space into two parts. </p>
</li>
<li><p>In keeping with this, we refer to either one of the sets<br>$$<br>{x∈X :L(x)≥α}\ \ \ \mbox{and}\ \ \  {x∈X :L(x)≤α}<br>$$</p>
</li>
<li><p>as a <strong>closed halfspace</strong> induced by $H$, and to either one of the sets<br>$$<br>{x∈X :L(x)&gt;α}\ \ \ \ \mbox{and}\ \ \ \ {x∈X :L(x)&lt;α}<br>$$</p>
</li>
<li><p>as an <strong>open halfspace</strong> induced by $H$.</p>
</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Lemma 1</strong>: </p>
<ul>
<li>Let $T$ be a subset of a linear space with $\mathbf{0} ∈T$. Then $φ∈\mathbb{R}^T $ is affine iff, $φ − φ(\mathbf{0} )$ is a linear real map on $T$ .</li>
</ul>
<blockquote>
<p>It is important to note that this fact remains true even if the domain of the affine map is not a linear space.</p>
</blockquote>
<p><strong>Proposition 4</strong>: </p>
<ul>
<li>Let $n∈\mathbb{N} $ and $∅\neq S⊆\mathbb{R}^n$. Then $φ∈\mathbb{R}^S$ is affine iff, there exist real numbers $α_1, …, α_n, \beta$, such that $φ(x) =\sum^n α_ix_i + β$ for all $x ∈ S$.</li>
</ul>
<p><strong>Proposition 5</strong>: </p>
<ul>
<li>Two finite dimensional linear spaces are isomorphic iff, they have the same dimension.</li>
</ul>
<p><strong>Corollary 3</strong>: </p>
<ul>
<li>Every nontrivial finite dimensional linear space is isomorphic to $\mathbb{R}^n$, for some $n ∈ \mathbb{N} $.</li>
</ul>
<p><strong>Proposition 6</strong>: </p>
<ul>
<li><p>Let $Y$ be a subset of a linear space $X$. </p>
</li>
<li><p>Then, $Y$ is a $⊇$-maximal proper linear subspace of $X$ iff,<br>$$<br>Y = null(L)<br>$$</p>
</li>
<li><p>for some nonzero linear functional $L$ on $X$.</p>
</li>
</ul>
<p><strong>Corollary 4</strong>: </p>
<ul>
<li>A subset $H$ of a linear space $X$ is a hyperplane in $X$ iff,</li>
</ul>
<p>$$<br>H ={x∈X :L(x)=α}<br>$$</p>
<ul>
<li>for some $α ∈ \mathbb{R} $ and nonzero linear functional $L$ on $X$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-10-26T09:41:52.000Z" title="10/26/2020, 5:41:52 PM">2020-10-26</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-C-Linear-Spaces-and-Convexity/">1.1.C Linear Spaces and Convexity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/10/26/Mathematics/Analysis/4%20Linear%20Spaces/Elements-of-Linear-Spaces/">Elements of Linear Spaces</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=005493>Group, Abelian Group</font></strong>: </p>
<ul>
<li><p>Let $X$ be any nonempty set, and $+$ a binary operation on $X$. </p>
</li>
<li><p>The doubleton $(X, +)$ is called a **<font color=005493>group </font>**if the properties 1-4 are satisfied.</p>
<ol>
<li>(Closure) $\mathbf{x}+\mathbf{y}\in X$ for all $\mathbf{x},\mathbf{y}∈X$;</li>
<li>(Associativity) $(\mathbf{x}+\mathbf{y})+\mathbf{z}=\mathbf{x}+(\mathbf{y}+\mathbf{z})$ for all $\mathbf{x},\mathbf{y},\mathbf{z}∈X$;</li>
<li>(Existence of an identity element) There exists an element $\mathbf{0}  ∈ X$ such that $\mathbf{0}  + \mathbf{x} = \mathbf{x} = \mathbf{x} + \mathbf{0} $ for all $\mathbf{x} ∈ X$;</li>
<li>(Existence of inverse elements) For each $\mathbf{x} ∈ X$, there exists an element $−\mathbf{x} ∈ X$ such that $\mathbf{x} + −\mathbf{x} = 0 = −\mathbf{x} + \mathbf{x}$.</li>
<li>(Commutativity) $\mathbf{x}+\mathbf{y}=\mathbf{y}+\mathbf{x}$ for all $\mathbf{x},\mathbf{y}∈X$,</li>
</ol>
</li>
<li><p>$(X, +)$ is said to be an <strong><font color=005493>Abelian (or commutative) group</font></strong> if all the properties are satisfied.</p>
</li>
</ul>
<blockquote>
<p>For any group $(X, +)$, and any nonempty subsets $A$ and $B$ of $X$, we let<br>$$<br>A + B := {x + y : (x, y) ∈ A × B}<br>$$</p>
</blockquote>
<p><strong><font color=941100>Linear Space</font>, Vector Space</strong>: </p>
<ul>
<li>Let $X$ be a nonempty set. </li>
<li>The list $(X,+,·)$ is called a <font color=941100><strong>linear space</strong></font> (or <strong>vector space</strong>) if $(X, +)$ is an Abelian group, and if $·$ is a mapping that assigns to each $(λ,\mathbf{x}) ∈ \mathbb{R} ×X$ an element $λ·\mathbf{x}$ of $X $ such that, for all $α, λ ∈ \mathbb{R} $ and $\mathbf{x}, \mathbf{y} ∈ X$, we have<ol>
<li>(Closure) $λ\mathbf{x}\in X$</li>
<li>(Associativity) $α(λ\mathbf{x}) = (αλ)\mathbf{x}$;</li>
<li>(Distributivity) $(α+λ)\mathbf{x}=α\mathbf{x}+λ\mathbf{x}$ </li>
<li>(Distributivity) $λ(\mathbf{x}+\mathbf{y})=λ\mathbf{x}+λ\mathbf{y}$;</li>
<li>(Existence of an identity element) $1\mathbf{x} = \mathbf{x}$.</li>
</ol>
</li>
</ul>
<blockquote>
<p>Let $(X,+,·)$ be a linear space, and $A,B ⊆ X$ and $λ ∈ \mathbb{R} $. Then,<br>$$<br>A+B:={x+y:(x,y)∈A×B}\ \ \ \mbox{and}\ \ \ \ λA:={λx:x∈A}.<br>$$</p>
<p>For simplicity, we write $A + y$ for $A + {y}$, and similarly, $y + A := {y} + A$.</p>
</blockquote>
<blockquote>
<p>Linear spaces provide an ideal structure for a proper investigation of convex sets. </p>
</blockquote>
<blockquote>
<p>We use the notation $X$ instead of $(X,+,\cdot)$ for a linear space, but we should always keep in mind that what makes a linear space “linear” is the two operations defined on it. </p>
<p>Two different types of addition and scalar multiplication operations on a given set may well endow this set with different linear structures, and hence yield two very different linear spaces.</p>
</blockquote>
<p><strong>Addition, Scalar Multipulation, Origin, Zero, Vector</strong>: </p>
<ul>
<li>In a linear space $(X,+,·)$, the mappings $+$ and $·$ are called <strong>addition</strong> and <strong>scalar multiplication</strong> operations on $X$, respectively. </li>
<li>The identity element $\mathbf{0} $ is called the <strong>origin</strong> (or <strong>zero</strong>), and any member of $X$ is referred to as a <strong>vector</strong>. <ul>
<li>If $\mathbf{x} ∈ X\backslash {0}$, then we say that $x$ is a nonzero vector in $X$.</li>
</ul>
</li>
</ul>
<p><strong>Linear Subspace, Proper Linear Subspace</strong>: </p>
<ul>
<li><p>Let $X$ be a linear space and $∅ \neq Y ⊆ X$. </p>
</li>
<li><p>If $Y$ is a linear space with the same operations of addition and scalar multiplication as with $X$, then it is called a **linear subspace **of $X$. </p>
</li>
<li><p>If, further, $Y \neq X$, then $Y$ is called a <strong>proper linear subspace</strong> of $X$.</p>
</li>
</ul>
<p><strong>Affine Manifold, Hyperplane</strong>: </p>
<ul>
<li><p>A subset $S$ of a linear space $X$ is said to be an <strong>affine manifold</strong> of $X$ if $S=Z+\mathbf{x}^*$ for some linear subspace $Z$ of $X$ and some vector $\mathbf{x}^* ∈X$. </p>
</li>
<li><p>If $Z$ is a $⊇$-maximal proper linear subspace of $X, S$ is called a <strong>hyperplane</strong> in $X$. </p>
</li>
<li><p>Equivalently, a hyperplane is a $⊇$-maximal proper affine manifold.</p>
</li>
</ul>
<blockquote>
<p>A good way of thinking intuitively about the notion of affinity in linear analysis is this: affinity = linearity + translation.</p>
<p>Since we think of $\mathbf{0} $ as the origin of the linear space $X$ — this is a geometric interpretation; don’t forget that the definition of $\mathbf{0} $ is purely algebraic — it makes sense to view a linear subspace of $X$ as untranslated (relative to the origin of the space), for a linear subspace “passes through” $\mathbf{0} $. </p>
<p>The following simple but important observation thus gives support to our informal equation above: An affine manifold $S$ of a linear space $X$ is a linear subspace of $X$ iff $\mathbf{0}  ∈ S$. </p>
<p>An immediate corollary of this is: If $S$ is an affine manifold of X, then $S −x$ is a linear subspace of $X$ for any $x ∈ S$. </p>
<p>Moreover, this subspace is determined independently of $x$, because if $S$ is an affine manifold, then<br>$$<br>S−x=S−y\ \ \  \mbox{for any}\ \ \ x,y∈S.<br>$$</p>
</blockquote>
<p><strong><font color=FF2600>Linear Combination, Affine Combination, Positive Linear Combination, Convex Combination</font></strong>:  </p>
<ul>
<li>For any $m ∈ \mathbb{N} $, by a <strong><font color=FF2600>linear combination</font></strong> of the vectors $\mathbf{x}^1,…,\mathbf{x}^m$ in a linear space $X$, we mean a vector $\sum^m λ_i\mathbf{x}^i ∈ X$, where $λ_1, …, λ_m$ are any real numbers.</li>
<li>If we have $\sum^m\lambda_i=1$, then $\sum^m\lambda_i\mathbf{x}^i$ is referred to as an <strong><font color=FF2600>affine combination</font></strong> of the vectors $\mathbf{x}^1,…,\mathbf{x}^m$. </li>
<li>If $\lambda_i\geq0$ for each $i$, then $\sum^m\lambda_i\mathbf{x}^i$ is called a <strong><font color=FF2600>positive linear combination</font></strong> of $\mathbf{x}^1,…,\mathbf{x}^m$. </li>
<li>If $\lambda_i\geq0$ for each $i$ and $\sum^m\lambda_i=1$, then $\sum^m\lambda_i\mathbf{x}^i$ is called a <strong><font color=FF2600>convex combination</font></strong> of $\mathbf{x}^1,…,\mathbf{x}^m$. <ul>
<li>Equivalently, a linear (affine (convex)) combination of the elements of a nonempty finite subset $T$ of $X$ is $\sum_{x\in T}\lambda(\mathbf{x})\mathbf{x}$, where $\lambda\in \mathbb{R}^T$. (and $\sum_{\mathbf{x}\in T}\lambda(\mathbf{x})=1$ (and $\lambda(T)\subseteq \mathbb{R}_{+}$)).</li>
</ul>
</li>
</ul>
<p><strong><font color=941751>Span / Linear Hull</font></strong>: </p>
<ul>
<li>The set of all linear combinations of finitely many members of a nonempty subset $S$ of a linear space $X$ is called the <strong><font color=941751>span / linear Hull</font></strong> of $S$ in $X$, and is denoted by $span(S)$. </li>
<li>That is, for any $∅ \neq S ⊆ X$,</li>
</ul>
<p>$$<br>span(S):= \left\{λ_i\mathbf{x}^i :m∈\mathbb{N}\ \mbox{and}\ \ (\mathbf{x}^i,λ_i)∈S×\mathbb{R}, i=1,…,m \right\},<br>$$</p>
<ul>
<li><p>or equivalently,<br>$$<br>span(S)=\left\{\sum_{\mathbf{x}\in T}λ(\mathbf{x})\mathbf{x}:T ∈\mathcal{P}(S)\ \ \ \mbox{and}\ \ λ∈\mathbb{R}^T\right\}<br>$$</p>
</li>
<li><p>where $\mathcal{P}(S)$ is the class of all nonempty finite subsets of $S$. </p>
</li>
</ul>
<blockquote>
<p>By convention, we let $span(∅) = \{\mathbf{0} \}$.</p>
<p>$span(S)$ is the smallest (i.e. $ ⊇$-minimum) linear subspace of the mother space that contains $S$. </p>
<p>Especially when $S$ is finite, this linear subspace has a very concrete description in that every vector in it can be expressed as linear combination of all the vectors in $S$.</p>
</blockquote>
<p><strong><font color=941751>Affine Hull</font></strong>: </p>
<ul>
<li><p>The set of all affine combinations of finitely many members of a nonempty subset $S$ of a linear space $X$ is called the <strong><font color=941751>affine hull</font></strong> of $S$ (in $X$), and is denoted by $aff(S)$. </p>
</li>
<li><p>That is, for any $∅ \neq S ⊆ X$,<br>$$<br>aff(S):=\left\{\sum_{\mathbf{x}\in T}λ(\mathbf{x})\mathbf{x}:T ∈\mathcal{P}(S)\  \ \ \mbox{and}\ \ \ λ∈\mathbb{R}^T\ \ \mbox{with} \sum_{\mathbf{x}\in T}λ(\mathbf{x})=1\right\}<br>$$</p>
</li>
<li><p>where $\mathcal{P}(S)$ is the class of all nonempty finite subsets of $S$. </p>
</li>
</ul>
<blockquote>
<p>By convention, we let $aff(∅) = \{\mathbf{0} \}$.</p>
<p>By Proposition 1, $aff(S)$ is an affine manifold of $X$. </p>
<p>Moreover, again by Proposition 1 and the Principle of Mathematical Induction, any affine manifold of $X$ that contains $S$ also contains $aff (S)$. </p>
<p>Therefore, $aff (S)$ is the smallest affine manifold of $X$ that contains $S$. </p>
<p>Equivalently, this manifold equals the intersection of all affine manifolds of $X$ that contain $S$.</p>
<p>These observations help clarify the nature of the tight connection between the notions of span and affine hull of a set. Put precisely, for any nonempty subset $S$ of a linear space $X$, we have<br>$$<br>aff(S)=span(S−x)+x \mbox{ for any }x∈S.<br>$$</p>
</blockquote>
<p><strong>Linearly Dependent, Linearly Independent</strong>: </p>
<ul>
<li><p>Let $X$ be a linear space. </p>
</li>
<li><p>A subset $S$ of $X$ is <strong>linearly dependent</strong> in $X$ if it either equals ${0}$ or at least one of the vectors in $S$ can be expressed as a linear combination of finitely many vectors in $S\backslash\{\mathbf{x}\}$. </p>
</li>
<li><p>For any $m ∈ \mathbb{N} $, any distinct vectors $\mathbf{x}_1, …, \mathbf{x}_m ∈ X$ are <strong>linearly dependent</strong> if $\{\mathbf{x}_1, …, \mathbf{x}_m\}$ is linearly dependent in $X$.</p>
</li>
<li><p>A subset of $X$ is <strong>linearly independent</strong> in $X$ if no finite subset of it is linearly dependent in $X$. </p>
</li>
<li><p>For any $m ∈ \mathbb{N} $, the vectors $\mathbf{x}_1, …, \mathbf{x}_m ∈ X$ are called <strong>linearly independent</strong> if $\{\mathbf{x}_1, …, \mathbf{x}_m\}$ is linearly independent in $X$.</p>
</li>
</ul>
<blockquote>
<p>It follows from these definitions that any set $S$ in a linear space $X$ with $\mathbf{0} ∈ S$ is linearly dependent in $X$.</p>
<p>For any distinct $x, y ∈ X\backslash\{\mathbf{0}\}$, the set $\{x, y\}$ is linearly dependent in $X$ iff $y ∈ span(\{x\})$ iff $span(\{x\}) = span(\{y\})$. Hence, one says that two nonzero vectors $x$ and $y$ are linearly dependent iff they both lie on a line through the origin. More generally, a subset $S$ of $X\backslash\{\mathbf{0}\}$ is linearly dependent in $X$ iff there exists an $x ∈ S$ such that $x ∈ span(S\backslash\{x\})$.</p>
<p>A fundamental principle of linear algebra is that there cannot be more than $m$ linearly independent vectors in a linear space spanned by $m$ vectors. </p>
</blockquote>
<p><strong>Affinely Independent, Affinely Dependent</strong>: </p>
<ul>
<li><p>Let $X$ be a linear space. </p>
</li>
<li><p>A finite subset $T$ of $X$ is <strong>affinely independent</strong> in $X$ if $\{\mathbf{z} − \mathbf{x} : \mathbf{z} ∈ T\backslash\{\mathbf{x}\}\}$ is linearly independent in $X$ for any $\mathbf{x} ∈ T$. </p>
</li>
<li><p>An arbitrary nonempty subset $S$ of $X$ is <strong>affinely independent</strong> in $X$ if every finite subset of $S$ is affinely independent in $X$. </p>
</li>
<li><p>$S$ is <strong>affinely dependent</strong> in $X$ if it is not affinely independent in $X$.</p>
</li>
</ul>
<blockquote>
<p>Since, at most $n$ vectors can be linearly independent in $\mathbb{R}^n, n = 1, 2, …$, it follows that there can be at most $n + 1$ affinely independent vectors in $\mathbb{R}^n$. </p>
</blockquote>
<p><strong>Basis</strong>: </p>
<ul>
<li>A basis for a linear space $X$ is a $⊇$-minimal subset of $X$ that spans $X$. </li>
<li>That is, $S$ is a **basis **for $X$ iff,<ol>
<li>$X = span(S)$; and</li>
<li>If $X = span(T)$, then $T ⊂ S$ is false.</li>
</ol>
</li>
<li>If a linear space $X$ has a finite basis, then it is said to be finite dimensional, and its dimension, $dim(X)$, is defined as the cardinality of any one of its bases. </li>
<li>If $X$ does not have a finite basis, then it is called infinite dimensional, in which case we write $dim(X) = ∞$.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1</strong>: </p>
<ul>
<li>Let $X$ be a linear space and $∅ \neq S ⊆ X$. </li>
<li>Then $S$ is an affine manifold of $X $ if, and only if,</li>
</ul>
<p>$$<br>λx+(1−λ)y∈S\ \ \  \mbox{for any}\ \ \ x,y∈S\ \ \mbox{and} \ \ \ λ∈\mathbb{R}.<br>$$</p>
<p><strong>Proposition 2</strong>: </p>
<ul>
<li>Let $X$ be a linear space, and $A,B ⊆ X$. If $B$ is linearly independent in $X$ and $B ⊆ span(A)$, then $|B| ≤ |A|$.</li>
</ul>
<p><strong>Proposition 3</strong>: </p>
<ul>
<li>A subset $S$ of a linear space $X$ is a basis for $X$ if, and only if, $S$ is linearly independent and $X = span(S)$.</li>
</ul>
<p><strong>Corollary 1</strong>: </p>
<ul>
<li>Any two bases of a finite dimensional linear space have the same number of elements.</li>
</ul>
<p><strong>Remark 1</strong>: </p>
<ul>
<li>The dimension of an affine manifold $S$ in a linear space $X$, denoted by $dim(S)$, is defined as the dimension of the linear subspace of $X$ that is “parallel” to $S$. Put more precisely,</li>
</ul>
<p>$$<br>dim(S) := dim(S − x)<br>$$</p>
<ul>
<li> for any $x ∈ S$. </li>
</ul>
<p><strong>Corollary 2</strong>: </p>
<ul>
<li><p>Let $S$ be a basis for a linear space $X$. </p>
</li>
<li><p>Any nonzero vector $x ∈ X$ can be expressed as a linear combination of finitely many members of $S$ with nonzero coefficients in only one way.</p>
</li>
<li><p>If $X$ is finite dimensional, then every vector in $X$ can be uniquely written as a linear combination of all vectors in $S$.</p>
</li>
</ul>
<p><strong>Theorem 1</strong>: </p>
<ul>
<li>Every linear space has a basis.</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Examples</strong>:</p>
<ul>
<li>$(\mathbb{Z},+), (\mathbb{Q} ,+), (\mathbb{R} ,+), (\mathbb{R}^n,+)$ and $(\mathbb{R} \backslash\{0\},\cdot)$ are Abelian groups where $+$ and $ \cdot$ are the usual addition and multiplication operations. <ul>
<li>In the $(\mathbb{R} \backslash\{0\},\cdot)$ number $1$ plays the role of the identity element.</li>
</ul>
</li>
<li> $(\mathbb{R} , \cdot)$ is not a group because it does not have inverse elements. Similarly, $(\mathbb{Z} , \cdot)$ and $(\mathbb{Q} , \cdot)$ are not groups.</li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li><p>$\mathbb{R}^n_{++}$ is not a linear space since it does not contain the origin. </p>
<ul>
<li>This is of course not the only problem. </li>
<li>After all, $\mathbb{R}^n_+$ is not a linear space either (under the usual operations), for it does not contain the inverse of any nonzero vector.</li>
</ul>
</li>
<li><p>The real sequence space $\mathbb{R}^∞$ (which is none other than $\mathbb{R}^\mathbb{N} $) and the function spaces $\mathbb{R}^T$ and $\mathbf{B}(T)$ (for any nonempty set $T$) are linear spaces under usual operations. </p>
</li>
<li><p>$\ell^p$ (for any $1 ≤ p ≤ ∞$), along with the function spaces $\mathbf{CB}(T )$ and $\mathbf{C}(T )$ (for any metric space $T$), are linear spaces under usual operations.</p>
<ul>
<li>The same goes as well for other function spaces, such as $\mathbf{P}(T )$ or the space of all polynomials on $T$ of degree $m ∈ \mathbb{Z}_+$ (for any nonempty subset $T$ of $\mathbb{R} $).</li>
</ul>
</li>
<li><p>Since the negative of an increasing function is decreasing, the set of all increasing real functions on $\mathbb{R} $ (or on any compact interval $[a, b]$ with $a &lt; b$) is not a linear space under the usual operations. </p>
<ul>
<li>Less trivially, the set of all monotonic self-maps on $\mathbb{R} $ is not a linear space either. </li>
</ul>
</li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li><p>$[0,1]$ is not a linear subspace of $\mathbb{R} $ whereas $\{x ∈ \mathbb{R}^2 : x_1 +x_2 = 0\}$ is a proper linear subspace of $\mathbb{R}^2$.</p>
</li>
<li><p>For any $n ∈ \mathbb{N} $, $\mathbb{R}^{n×n}$ is a linear space under the usual operations. The set of all symmetric $n × n$ matrices, that is, $\{[a_{ij}]<em>{n×n} : a</em>{ij} = a_{ji}\mbox{ for each }i,j\}$ is a linear subspace of this space.</p>
</li>
<li><p>For any $n∈\mathbb{N} $ and linear $f:\mathbb{R}^n →\mathbb{R} $, $\{x∈\mathbb{R}^n :f(x)=0\}$ is a linear subspace of $\mathbb{R}^n$. </p>
</li>
<li><p>For any $m ∈ \mathbb{N} $, the set of constant functions on $[0, 1]$ is a proper linear subspace of the set of all polynomials on $[0, 1]$ of degree at most $m$. </p>
<ul>
<li>The latter set is a proper linear subspace of $\mathbf{P}[0, 1]$ which is a proper linear subspace of $\mathbf{C}[0, 1]$ which is itself a proper linear subspace of $\mathbf{B}[0, 1]$. </li>
<li>$\mathbf{B}[0, 1]$ is a proper linear subspace of $\mathbb{R}^{[0,1]}$.</li>
</ul>
</li>
<li><p>$\{x ∈ \mathbb{R}^2 : x_1 + x_2 = 1\}$ is not a linear subspace of $\mathbb{R}^2$, since this set does not contain the origin of $\mathbb{R}^2$. </p>
<ul>
<li>On the other hand, geometrically speaking, this set is very “similar” to the linear subspace $\{x ∈ \mathbb{R}^2 : x_1 + x_2 = 0\}$. </li>
</ul>
</li>
</ul>
<ul>
<li>Indeed, the latter is nothing but a parallel shift (translation) of the former set.</li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li>There is no hyperplane in the trivial space $\{\mathbf{0}\}$. <ul>
<li>Since the only proper linear subspace of $\mathbb{R} $ is $\{0\}$, any one-point set in $\mathbb{R} $ and $\mathbb{R} $ itself are the only affine manifolds in $\mathbb{R} $. </li>
<li>So, all hyperplanes in $\mathbb{R} $ are singleton sets. </li>
</ul>
</li>
<li>In $\mathbb{R}^2$, any one-point set, any line (with no endpoints) and the entire $ \mathbb{R}^2$ are the only affine manifolds. <ul>
<li>A hyperplane in this space is necessarily of the form $\{x ∈ \mathbb{R}^2 : a_1x_1 + a_2x_2 = b\}$ for some real numbers $a_1,a_2$ with at least one of them being nonzero, and some real number $b$. </li>
</ul>
</li>
<li>All hyperplanes are of the form of (infinitely extending) planes in $\mathbb{R}^3 $.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="hqin"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">hqin</p><p class="is-size-6 is-block">A Student in Economics</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Zhengzhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">48</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">5</p></a></div></div></nav></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:17:12.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/The-Fundamental-Problem-of-the-Calculus-of-Variations/">The Fundamental Problem of the Calculus of Variations</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-02T14:17:12.000Z">2021-01-02</time></p><p class="title"><a href="/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/">The Nature of Dynamic Optimization</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-A-Preliminaries-of-Dynamic-Optimization/">1.4.A Preliminaries of Dynamic Optimization</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-24T14:52:59.000Z">2020-12-24</time></p><p class="title"><a href="/2020/12/24/Mathematics/Algebra/2%20Linear%20Algebra/Geometric-Linear-Algebra/">Geometric Linear Algebra</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-2-Algebra/">1.2 Algebra</a> / <a href="/categories/1-Mathematics/1-2-Algebra/1-2-B-Linear-Algebra/">1.2.B Linear Algebra</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-24T14:52:00.000Z">2020-12-24</time></p><p class="title"><a href="/2020/12/24/Mathematics/Algebra/1%20Abstract%20Algebra/Elements-of-Abstract-Algebra/">Elements of Abstract Algebra</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-2-Algebra/">1.2 Algebra</a> / <a href="/categories/1-Mathematics/1-2-Algebra/1-2-A-Abstract-Algebra/">1.2.A Abstract Algebra</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-15T14:50:59.000Z">2020-12-15</time></p><p class="title"><a href="/2020/12/15/Mathematics/Analysis/15%20Stochastic%20Dependence/Propertities-of-Conditional-Expectation/">Propertities of Conditional Expectation</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a> / <a href="/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/">1.1.H Stochastic Independence and Dependence</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li></ul></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://ygnmax.github.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Guangnan Yang</span></span><span class="level-right"><span class="level-item tag">ygnmax.github.io</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Abstract-Algebra/"><span class="tag">Abstract Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Control-Theory/"><span class="tag">Control Theory</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Measure-Theoretic-Probability/"><span class="tag">Measure Theoretic Probability</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Real-Analysis/"><span class="tag">Real Analysis</span><span class="tag">24</span></a></div></div></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/1-Mathematics/"><span class="level-start"><span class="level-item">1 Mathematics</span></span><span class="level-end"><span class="level-item tag">48</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/"><span class="level-start"><span class="level-item">1.1 Analysis</span></span><span class="level-end"><span class="level-item tag">44</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-A-Preliminaries-of-Real-Analysis/"><span class="level-start"><span class="level-item">1.1.A Preliminaries of Real Analysis</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/"><span class="level-start"><span class="level-item">1.1.B Metric Spaces and Continuity</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-C-Linear-Spaces-and-Convexity/"><span class="level-start"><span class="level-item">1.1.C Linear Spaces and Convexity</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-D-Metric-Linear-Spaces-and-Normed-Linear-Spaces/"><span class="level-start"><span class="level-item">1.1.D Metric Linear Spaces and Normed Linear Spaces</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-E-Probability-via-Measure-Theory/"><span class="level-start"><span class="level-item">1.1.E Probability via Measure Theory</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/"><span class="level-start"><span class="level-item">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-G-Weak-Convergence-and-Probability-Limit/"><span class="level-start"><span class="level-item">1.1.G Weak Convergence and Probability Limit</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/"><span class="level-start"><span class="level-item">1.1.H Stochastic Independence and Dependence</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-2-Algebra/"><span class="level-start"><span class="level-item">1.2 Algebra</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-2-Algebra/1-2-A-Abstract-Algebra/"><span class="level-start"><span class="level-item">1.2.A Abstract Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-2-Algebra/1-2-B-Linear-Algebra/"><span class="level-start"><span class="level-item">1.2.B Linear Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/"><span class="level-start"><span class="level-item">1.4 Dynamic Optimization</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-A-Preliminaries-of-Dynamic-Optimization/"><span class="level-start"><span class="level-item">1.4.A Preliminaries of Dynamic Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/"><span class="level-start"><span class="level-item">1.4.B The Calculus of Variations</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">hqin</a><p class="is-size-7"><span>&copy; 2020 - 2021 hqin</span>  </p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'folded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>