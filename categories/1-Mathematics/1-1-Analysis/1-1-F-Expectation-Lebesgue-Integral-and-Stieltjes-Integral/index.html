<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: 1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral - hqin</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="hqin"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="hqin"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="hqin"><meta property="og:url" content="https://hqin-2020.github.io/"><meta property="og:site_name" content="hqin"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://hqin-2020.github.io/img/og_image.png"><meta property="article:author" content="hqin"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hqin-2020.github.io"},"headline":"hqin","image":["https://hqin-2020.github.io/img/og_image.png"],"author":{"@type":"Person","name":"hqin"},"description":""}</script><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="hqin" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">hqin</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/1-Mathematics/">1 Mathematics</a></li><li><a href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a></li><li class="is-active"><a href="#" aria-current="page">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:43:59.000Z" title="12/15/2020, 10:43:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/11%20Expectation%20via%20the%20Stieltjes%20Integral/Integration-By-Parts/">Integration By Parts</a></h1><div class="content"><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Integration by Parts Formula</strong>: (Stieltjes) </p>
<ul>
<li>Let $F$ and $\varphi$ be two increasing real functions on $[a,b]$. If $\varphi$ is $F$-integrable, then $F$ is $\varphi$-integrable, and</li>
</ul>
<p>$$<br>\int^b_a\varphi dF=\varphi(b)F(b)-\varphi(a)F(a)-\int^b_aFd\varphi<br>$$</p>
<p><strong>Proposition 3.1</strong>: </p>
<ul>
<li>Let $x$ be a nonnegative random variable on a probability space $(X,\Sigma, \mathbf{p})$. If $\varphi$ is an increasing and $F_x$-integrable self-map on $\mathbb{R} $, then</li>
</ul>
<p>$$<br>\mathbb{E}(\varphi\circ x)=\varphi(0)+\int^\infty_0(1-F_x)d\varphi<br>$$</p>
<ul>
<li>In particular,<br>$$<br>\mathbb{E}(x)=\int^\infty_0(1-F_x(t))dt<br>$$</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:42:59.000Z" title="12/15/2020, 10:42:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/11%20Expectation%20via%20the%20Stieltjes%20Integral/Expectation-as-a-Stieltjes-Integral/">Expectation as a Stieltjes Integral</a></h1><div class="content"><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Theorem 2.1</strong>: </p>
<ul>
<li>Let $x$ be a random variable and $\varphi$ an $F_x$-integrable self-map on $\mathbb{R} $. </li>
<li>If $\mathbb{E}(\varphi\circ x)$ exists, then</li>
</ul>
<p>$$<br>\mathbb{E}(\varphi\circ x)=\int_X(\varphi\circ x)d\mathbf{p}=\int_\mathbb{R}\varphi d\mathbf{p}<em>x=\int^\infty</em>{-\infty}\varphi dF_x<br>$$</p>
<p><strong>Corollary 2.2</strong>: </p>
<ul>
<li>Let $x$ be a nonnegative random variable and $\varphi$ an $F_x$-integrable self-map on $\mathbb{R} $. If $\mathbb{E}(\varphi\circ x)$ exists, then</li>
</ul>
<p>$$<br>\mathbb{\varphi\circ x}=\varphi(0)F_x(0)+\int^\infty_0\varphi dF_x<br>$$</p>
<p><strong>Corollary 2.3</strong>: </p>
<ul>
<li>Let $x$ be a random variable with a continuously differentiable distribution function $F_x$, and $f$ a density function for $F_x$. </li>
<li>Let $\varphi$ be an almost everywhere continuous and locally bounded self-map on $\mathbb{R} $. </li>
<li>If $\varphi$ is $F_x$-integrable and $\mathbb{E}(\varphi\circ x)$ exists, then</li>
</ul>
<p>$$<br>\mathbb{E}(\varphi\circ x)=\int^\infty_{-\infty}\varphi dF_x=\int^\infty_{-\infty}\varphi(t)f(t)dt<br>$$</p>
<ul>
<li><p>In particular, above equation holds for every continuous $\varphi : \mathbb{R}\to \mathbb{R}_+$. </p>
</li>
<li><p>Moreover, if $\mathbb{E}(x)$ exists,<br>$$<br>\mathbb{E}(x)=\int_Xxd\mathbf{p}=\int^\infty_{-\infty}tdF_x(t)=\int^\infty_{-\infty}tf(t)dt<br>$$</p>
</li>
</ul>
<p><strong>Proposition 2.4</strong>: </p>
<ul>
<li>Let $f : \mathbb{R}\to \mathbb{R}_+$ be a Borel measurable function. </li>
<li>If $f$ is Riemann integrable, then</li>
</ul>
<p>$$<br>\int_{\mathbb{R}}fd\ell=\int^\infty_{-\infty}f(t)dt<br>$$</p>
<ul>
<li>In particular, above equation holds when $f$ is bounded and continuous almost everywhere.</li>
</ul>
<p><strong>Corollary 2.5</strong>: </p>
<ul>
<li>Let $a$ and $b$ be real numbers with $a &lt; b$. </li>
<li>If $f : \mathbb{R}\to \mathbb{R}_+$ is Borel measurable and Riemann integrable (or bounded and continuous almost everywhere), then</li>
</ul>
<p>$$<br>\int_{[a,b]}fd\ell=\int^b_af(t)dt<br>$$</p>
<ul>
<li><p>Similarly, we have<br>$$<br>\int_{(-\infty,a]}fd\ell=\int^a_{-\infty}f(t)dt, \ \ \ and \ \ \ \int_{[b,\infty)}fd\ell=\int^\infty_bf(t)dt<br>$$</p>
</li>
<li><p>The Riemann and Lebesgue integrals of a Riemann integrable and nonnegative Borel measurable real function (defined on an interval) are equal.</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:41:59.000Z" title="12/15/2020, 10:41:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/11%20Expectation%20via%20the%20Stieltjes%20Integral/The-Stieltjes-Integral/">The Stieltjes Integral</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Dissection, Subinterval, Division Point, Mesh</strong>: </p>
<ul>
<li><p>For two arbitrarily fixed real numbers $a$ and $b$ with $a &lt; b$, for any positive integer $m$ and real numbers $a_0,\cdots,a_m$ with $a=a_0 &lt;\cdots &lt;a_m =b$, we refer to the collection<br>$$<br>{[a_0,a_1], [a_1, a_2],\cdots,  [a_{m-1},a_m]}<br>$$</p>
</li>
<li><p>as a <strong>dissection</strong> of $[a,b]$, and denote it by either $\mathbf{a}$ or $[a_0,\cdots,a_m]$. </p>
</li>
<li><p>Any one of the intervals $[a_{i-1},a_i] $ is called a <strong>subinterval</strong> of $\mathbf{a}$, while any one of $a_i$s is called a <strong>division point</strong> of $\mathbf{a}$. </p>
</li>
<li><p>The maximum value of the lengths of its subintervals is called the <strong>mesh</strong> of $\mathbf{a}$-this value is denoted by $\mbox{mesh}(\mathbf{a})$.</p>
</li>
<li><p>The collection of all dissections of $[a, b]$ is denoted as $\mathcal{D}[a, b]$.</p>
</li>
</ul>
<p><strong>Finer than</strong>:</p>
<ul>
<li>For any dissections $\mathbf{a} = [a_0,\cdots,a_m]$ and $\mathbf{b} = [b_0,\cdots, b_k]$ of $[a, b]$, we write</li>
</ul>
<p>$$<br>\mathbf{a}\Cup\mathbf{b}<br>$$</p>
<ul>
<li>for the dissection $[c_0,\cdots, c_l]$ in $\mathcal{D}[a, b]$ where ${c_0,\cdots, c_l} = {a_0,\cdots, a_m}\cup{b_0,\cdots, b_k}$. </li>
<li>Moreover, we say that $\mathbf{b}$ is <strong>finer than</strong> $\mathbf{a}$ if ${a_0,\cdots, a_m} \subseteq {b_0,\cdots, b_k}$. </li>
<li>Evidently, $\mathbf{a}\Cup \mathbf{b} = \mathbf{b}$ iff $\mathbf{b}$ is finer than $\mathbf{a}$. </li>
<li>We also have $\mbox{mesh}(\mathbf{b})\leq \mbox{mesh}(\mathbf{a})$ if $\mathbf{b}$ is finer than $\mathbf{a}$, but not conversely.</li>
</ul>
<p><strong>Darboux Sum</strong>: </p>
<ul>
<li>Take any bounded real functions $\varphi$ and $F$ on $[a, b]$ with $F$ being increasing. </li>
<li>For any dissection $\mathbf{a} = [a_0,\cdots, a_m]$ of $[a, b]$, we define</li>
</ul>
<p>$$<br>\check{\varphi}<em>{\mathbf{a}}(i):=\sup{\varphi(t):a</em>{i-1}\leq t\leq a_i}<br>$$</p>
<ul>
<li><p>and<br>$$<br>\hat{\varphi}<em>{\mathbf{a}}(i):=\inf{\varphi(t):a</em>{i-1}\leq t\leq a_i}<br>$$</p>
</li>
<li><p>for each $i\in [m]$. (Since $\varphi$ is bounded, every one of these numbers is real.) </p>
</li>
<li><p>By a <strong>Darboux sum</strong> of $\varphi$ with respect to $F$ and $\mathbf{a}$, we mean a number like<br>$$<br>\sum_{i\in[m]}\alpha_i(F(a_i)-F(a_{i-1}))<br>$$</p>
</li>
<li><p>where $\hat{\varphi}_{\mathbf{a}} (i)\leq \alpha_i\leq \check{\varphi}_{\mathbf{a}}(i)$ for each $i$.</p>
</li>
<li><p>In particular, the <strong>$\mathbf{a}$-upper Darboux sum</strong> of $\varphi$ with respect to $F$ is defined as the number<br>$$<br>\mathbf{S}<em>{F,\mathbf{a}}(\varphi):=\sum</em>{i\in[m]}\check{\varphi}_{\mathbf{a}}(i)(F(a_i)-F(a_{i-1}))<br>$$</p>
</li>
<li><p>and the <strong>$\mathbf{a}$-lower Darboux sum</strong> of $\varphi$ with respect to $F$ is defined dually as<br>$$<br>\mathbf{s}<em>{F,\mathbf{a}}(\varphi):=\sum</em>{i\in[m]}\hat{\varphi}_{\mathbf{a}}(i)(F(a_i)-F(a_{i-1}))<br>$$</p>
</li>
<li><p>Clearly, $\mathbf{S}<em>{F,\mathbf{a}}(\varphi)$ decreases, and $\mathbf{s}</em>{F,\mathbf{a}}(\varphi)$ increases, as a becomes finer. </p>
</li>
<li><p>Evidently, we always have $\mathbf{S}<em>{F,\mathbf{a}}(\varphi)\geq\mathbf{s}</em>{F,\mathbf{a}}(\varphi) $. </p>
</li>
<li><p>Furthermore, and this is important, we have<br>$$<br>\inf{\mathbf{S}<em>{F,\mathbf{a}}(\varphi):\mathbf{a}\in\mathcal{D}[a,b]}\geq\sup{\mathbf{s}</em>{F,\mathbf{a}}(\varphi):\mathbf{a}\in\mathcal{D}[a,b]}<br>$$</p>
</li>
<li><p>The number on the left-hand side of this inequality is called the <strong>upper Stieltjes integral of $\varphi$ with respect to</strong> $F$, and is denoted by $\mathcal{S}_F (\varphi)$. </p>
</li>
<li><p>Similarly, the number on the right-hand side is called the <strong>lower Stieltjes integrals of $\varphi$ with respect to $F$</strong>, and is denoted by $\mathbf{s}_F (\varphi)$. </p>
</li>
<li><p>Thus, our inequality reads:<br>$$<br>\mathcal{S}_F (\varphi)\geq\mathbf{s}_F (\varphi)<br>$$</p>
</li>
</ul>
<p><strong>Stiltjes Integrals, Stieltjes Integrable, $F$-integrable</strong>: </p>
<ul>
<li>Let $\varphi,F\in \mathbf{B}[a, b]$ and assume that $F$ is increasing. </li>
<li>Suppose that there exists a real number $\theta $ such that, for any $\varepsilon &gt; 0$, there exists a dissection $\mathbf{a}$ of $[a, b]$ with</li>
</ul>
<p>$$<br>|\mathbf{S}<em>{F,\mathbf{a}}(\varphi)-\theta|&lt;\varepsilon\mbox{ and }|\mathbf{s}</em>{F,\mathbf{a}}(\varphi)-\theta|&lt; \varepsilon<br>$$</p>
<ul>
<li><p>Then this number $\theta$ is unique, and  it is denoted by<br>$$<br>\int^b_a\varphi dF\mbox{ or }\int^b_a\varphi(t)dF(t)<br>$$</p>
</li>
<li><p>When it exists, we refer to this number as the <strong>Stieltjes integral of $\varphi$ with respect to $F$,</strong> and in that case, we say that $\varphi$ is <strong>Stieltjes integrable with respect to $F$</strong>, or simply, <strong>$F$-integrable</strong>. </p>
</li>
<li><p>If , $\varphi$ is $F$-integrable, we also define<br>$$<br>\int^a_b\varphi dF=-\int^b_a\varphi dF<br>$$</p>
</li>
<li><p>Finally, where $a\leq c\leq  d\leq  b$, we say that $\varphi$ is $F$-integrable on $[c,d]$, if $\varphi|<em>{[c,d]}$ is $F|</em>{[c,d]}$-integrable.</p>
</li>
<li><p>$\varphi$ is $F$-integrable iff the upper and lower Stieltjes integrals of $\varphi$ with respect to $F$ are equal, that is,<br>$$<br>\inf{\mathbf{S}<em>{F,\mathbf{a}}(\varphi) :\mathbf{a}\in\mathcal{D}[a, b]} = \sup{\mathbf{s}</em>{F,\mathbf{a}}(\varphi) : \mathbf{a} \in \mathcal{D}[a, b]}<br>$$</p>
</li>
</ul>
<blockquote>
<p>The following statements are equivalent:</p>
<ul>
<li><p>$\varphi$ is $F$-integrable,</p>
</li>
<li><p>$\mathbf{S}_F(\varphi) = \mathbf{s}_F(\varphi)$,</p>
</li>
<li><p>For every $\varepsilon &gt; 0$, there is a dissection $\mathbf{a}\in \mathcal{D}[a, b]$ such that<br>$$<br>\mathbf{S}<em>{F,\mathbf{a}}(\varphi) -\mathbf{s}</em>{F,\mathbf{a}}(\varphi)&lt;\varepsilon<br>$$</p>
</li>
</ul>
</blockquote>
<p><strong>Riemman Integrable, Riemann Intergral</strong>: </p>
<ul>
<li>The Riemann integral is a special case of the Stieltjes integral. </li>
<li>Formally, we say that $\varphi\in \mathbb{R}^{[a,b]}$ is <strong>Riemann integrable</strong> if it is $\mbox{id}_{[a,b]}$-integrable, and for any such $\varphi$, we define the <strong>Riemann integral</strong> of $\varphi$, denoted by</li>
</ul>
<p>$$<br>\int^b_a\varphi(t)dt<br>$$</p>
<ul>
<li>as the Stieltjes integral of $\varphi$ with respect to the identity function on $[a, b]$.</li>
</ul>
<p><strong>Continuous almost everywhere</strong>: </p>
<ul>
<li>Let us agree to call a real map $\varphi$ on $[a,b]$ continuous almost everywhere on $[a, b]$ if</li>
</ul>
<p>$$<br>d(\varphi):={t\in[a,b]:\varphi\mbox{ is not continuous at }t}<br>$$</p>
<ul>
<li>is a “small” set in the following sense: For every $\varepsilon &gt; 0$ there is an open subset $O$ of $[a,b]$ such that $ d(\varphi)\subseteq O$ and $\ell(O) &lt;\varepsilon$.</li>
</ul>
<blockquote>
<p>The totality of discontinuity points $\varphi$ fits within an open set of arbitrarily Lebesgue measure.</p>
</blockquote>
<p><strong>$F$-integrable, Improper Stieltjes Integral, Riemann Integrable</strong>: </p>
<ul>
<li><p>Let $\varphi$ and $F$ be any two self-maps on $\mathbb{R} $, and assume that $F$ is increasing. </p>
</li>
<li><p>Given any real numbers $a$ and $b$ with $a \leq b$, we say that $\varphi$ is <strong>$F$-integrable on $[a,b]$</strong>, if $\varphi$ is bounded on $[a,b]$ and $\int^b_a\varphi dF$ exists. (Note. If $\varphi$ is $F$-integrable on $[a,b]$, then $\int^b_a\varphi dF$ is a real number.) </p>
</li>
<li><p>In turn, we say that $\varphi$ is <strong>$F$-integrable on $[a,\infty)$</strong> if $\varphi$ is $F$-integrable on $[a,b]$ for each $b\geq a$, and $\lim_{b\to \infty}\int^b_a\varphi dF\in \overline{\mathbb{R} }$</p>
</li>
<li><p>In this case we define the <strong>(improper) Stieltjes integral</strong> of $\varphi$ with respect to $F$ on this unbounded interval as<br>$$<br>\int^\infty_a\varphi dF:=\lim_{c\to \infty}\int^c_a\varphi dF.<br>$$</p>
</li>
<li><p>The $F $-integrability of $\varphi$ on $(-\infty,a]$ and the extended real number $\int^a_{-\infty}\varphi dF$ is analogously defined. </p>
</li>
<li><p>Finally, we say that $\varphi$ is <strong>$F$-integrable</strong>, if there exists a real number a such that $\varphi$ is $F$-integrable on both $(-\infty,a]$ and $[a,\infty)$, and the expression $\int^a_\infty \varphi dF+\int^\infty_a\varphi dF$ is not of the indeterminate form $\infty-\infty$. </p>
</li>
<li><p>In this case, we define<br>$$<br>\int^\infty_{-\infty}\varphi dF:=\int^a_{-\infty}\varphi dF+\int^\infty_{a}\varphi dF,<br>$$</p>
</li>
<li><p>and note that this definition allows $\int^\infty_{-\infty}\varphi dF$ to equal $-\infty$ or $\infty$. </p>
</li>
<li><p>If $\varphi$ is $id_{\mathbb{R} }$-integrable, then we say that it is <strong>Riemann integrable</strong>.</p>
</li>
</ul>
<blockquote>
<p>The $F $-integrability of $\varphi$ implies that, on every compact interval, $\varphi$ is $F $-integrable. In particular, such a $\varphi$ is bounded on every compact interval. </p>
<p>A self-map on $\mathbb{R} $ with this property is said to be locally bounded.</p>
</blockquote>
<p><strong>Continous Almost Everywhere</strong>: </p>
<ul>
<li>The notion of almost everywhere continuity is extended to self-maps on $\mathbb{R} $ in the obvious way. </li>
<li>Put precisely, we say that $\varphi$ is <strong>continuous almost everywhere</strong> (or almost everywhere continuous) if, for every $\varepsilon &gt; 0$, there is an open subset $O$ of $\mathbb{R} $ such that $\ell(O) &lt; \varepsilon$ and every point of discontinuity of $\varphi$ is contained in $O$. </li>
</ul>
<blockquote>
<p>Obviously, if $\varphi$ is continuous almost everywhere, then it is continuous almost everywhere on any compact interval.</p>
</blockquote>
<p><strong>Density, Density Function</strong>: </p>
<ul>
<li>We say that a distribution function $F$ has <strong>density</strong>, if there exists an almost everywhere continuous map $f : \mathbb{R}\to \mathbb{R}_+$ such that $f$ is locally bounded and</li>
</ul>
<p>$$<br>F(s)=\int^s_{-\infty}f(t)dt<br>$$</p>
<ul>
<li>for every real number $s$. </li>
<li>In this case, we say that $f$ is a <strong>density function</strong> for $F$.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Lemma 1.1</strong>: </p>
<ul>
<li>Let $\varphi,\ F\in \mathbf{B}[a,b]$ be such that $F$ is increasing. If $\varphi$ is $F$-integrable, then,</li>
</ul>
<p>$$<br>\int^b_a\varphi dF=\int^c_a\varphi dF+\int^b_c\varphi dF,\ \ \ \  a\leq c\leq b<br>$$</p>
<ul>
<li>The same conclusion also holds if $\varphi$ is $F $-integrable on both $[a, c]$ and $[c, b]$.</li>
</ul>
<blockquote>
<p>The Stieltjes integral is additive with respect to the interval of integration. </p>
</blockquote>
<p><strong>Lemma 1.2</strong>: </p>
<ul>
<li>Assume $a &lt; b$, and take any $\varphi, F\in \mathbf{B}[a,b]$ with $F$ being increasing. </li>
<li>If $F$ is continuous and $\varphi$ is $F $-integrable. Then, </li>
</ul>
<p>$$<br>\int^{\frac{b-1}{m}}_a\varphi dF\to\int^b_a\varphi dF\ \ \ and\  \ \ \ \int^b_{\frac{a+1}{m}}\varphi dF\to\int^b_a\varphi dF<br>$$</p>
<blockquote>
<p>When the integrator is a continuous function, altering the value of an integrable function at a single point does not alter the value of its integral</p>
</blockquote>
<p><strong>Lemma 1.3</strong>: </p>
<ul>
<li>Let $\varphi,\ F\in \mathbf{B}[a, b]$ be such that $F$ is increasing and $\varphi$ is $F $-integrable. </li>
<li>If $\varphi\geq 0$ almost everywhere on $[a, b]$, then</li>
</ul>
<p>$$<br>\int^b_a\varphi dF\geq0<br>$$</p>
<p><strong>Corollary 1.4</strong>: </p>
<ul>
<li>Let $\varphi, F \in \mathbf{B}[a, b]$ be such that $F$ is increasing and $\varphi$ is $F $-integrable. If $\varphi$ vanishes almost everywhere on $[a,b]$, then</li>
</ul>
<p>$$<br>\int^b_a\varphi  dF=0<br>$$</p>
<p><strong>Lemma 1.5</strong>: </p>
<ul>
<li>Let $\varphi, F \in \mathbf{B}[a, b]$ be such that $F$ is increasing, and both $\varphi$ and $\phi$ are $F$-integrable. Then, for any real number $\alpha$, we have</li>
</ul>
<p>$$<br>\int^b_a(\alpha\varphi+\phi)dF=\alpha\int^b_a\varphi dF+\int^b_a\phi dF<br>$$</p>
<blockquote>
<p>The Stieltjes integral is linear with respect to its integrand. </p>
</blockquote>
<p><strong>Proposition 1.6</strong>: (Stieltjes) </p>
<ul>
<li>Take any $\varphi, F\in \mathbf{B}[a, b]$ with $F$ being increasing. </li>
<li>If $\varphi$ is continuous, then it is $F$-integrable.</li>
</ul>
<p><strong>Proposition 1.7</strong>: </p>
<ul>
<li>Let $\varphi, F \in \mathbf{B}[a, b]$ and assume that $F$ is increasing and Lipschitz continuous. </li>
<li>If $\varphi$ is continuous almost everywhere on $[a,b]$, then it is $F $-integrable.</li>
</ul>
<p><strong>The Lebesgue Criterion</strong>: </p>
<ul>
<li>If $\varphi \in \mathbf{B}[a,b]$ is continuous almost everywhere on $[a,b] $, then it is Riemann integrable.</li>
</ul>
<blockquote>
<p>Riemann integrability = Continuity almost everywhere.</p>
</blockquote>
<p><strong>Corollary 1.8</strong>: </p>
<ul>
<li>Let $\varphi\in \mathbf{B}[a, b]$ be continuous almost everywhere on $[a, b]$. If $\varphi$ vanishes almost everywhere on $[a, b]$, then</li>
</ul>
<p>$$<br>\int^b_a\varphi(t)dt=0<br>$$</p>
<p><strong>The Fundamental Theorem of Calculus 1</strong>: </p>
<ul>
<li>Let $f \in \mathbf{B}[a, b]$ be continuous almost everywhere and $F\in \mathbb{R}^{[a,b]}$ satisfy</li>
</ul>
<p>$$<br>F(s)=F(a)+\int^s_af(r)dr,\ \ \ \ a\leq s\leq b<br>$$</p>
<ul>
<li>Then, $F$ is Lipschitz continuous on $[a,b]$, and for every $s\in [a,b]$ at which $f$ is continuous, $F’(s)$ exists and equals $f(s)$.</li>
</ul>
<p><strong>The Fundamental Theorem of Calculus 2</strong>: </p>
<ul>
<li><p>Take any $F\in \mathbf{C}^1[a,b]$, and let $f \in \mathbf{B}[a,b]$ is a Riemann integrable function such that $F’ = f$ almost everywhere on $[a, b]$. </p>
</li>
<li><p>Then we have<br>$$<br>F(s)=F(a)+\int^s_af(r)dr,\ \ \ \ a\leq s\leq b<br>$$</p>
</li>
</ul>
<blockquote>
<p>Roughly speaking, The Fundamental Theorem of Calculus says that we can think of dif ferentiation and Riemann integration as inverse operations.</p>
</blockquote>
<p><strong>Theorem 1.10</strong>: </p>
<ul>
<li>Let $\varphi\in \mathbf{B}[a, b]$ be continuous almost everywhere on $[a, b]$. </li>
<li>Then, for any increasing $F\in \mathbf{C}^1[a,b]$, we have</li>
</ul>
<p>$$<br>\int^b_a\varphi dF=\int^b_a\varphi(t)F’(t)dt<br>$$</p>
<p><strong>Corollary 1.11</strong>: </p>
<ul>
<li><p>Let $f,\varphi\in \mathbf{B}[a,b]$ be continuous almost everywhere on $[a,b]$ and assume $f \geq 0$. </p>
</li>
<li><p>Then, for any $F\in \mathbf{C}^1[a, b]$ such that<br>$$<br>F(s)=F(a)+\int^s_af(r)dr,\ \ \ \ a\leq s\leq b<br>$$</p>
</li>
<li><p>holds, we have</p>
</li>
</ul>
<p>$$<br>\int^b_a\varphi dF=\int^b_a\varphi(t)f(t)dt<br>$$</p>
<blockquote>
<p>Theorem 1.10 and Corollary 1.11 help to reduce a Stieltjes Integral to a Riemann Integral, gaining computational power.</p>
</blockquote>
<p><strong>Proposition 1.12</strong>: </p>
<ul>
<li>Let $\varphi$ and $F$ be two self-maps on $\mathbb{R} $, and assume that $F$ is increasing and continuously differentiable. </li>
<li>If $\varphi$ is nonnegative, locally bounded, and continuous almost everywhere, then it is $F $-integrable.</li>
</ul>
<p><strong>Theorem 1.13</strong>: </p>
<ul>
<li>Let $F\in \mathbf{C}^1(\mathbb{R} )$ be a distribution function, and $\varphi$ a locally bounded self-map on $\mathbb{R} $ that is continuous almost everywhere. </li>
<li>If $f$ is a density function for $F $, then</li>
</ul>
<p>$$<br>\int^\infty_{-\infty}\varphi dF=\int^\infty_{-\infty}\varphi(t)f(t)dt<br>$$</p>
<ul>
<li>in the sense that if one side exists so does the other.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T13:55:59.000Z" title="12/15/2020, 9:55:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/10%20Expectation%20via%20the%20Lebesgue%20Integral/Spaces-of-Integrable-Random-Variables/">Spaces of Integrable Random Variables</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>$p$-Integrable</strong></p>
<ul>
<li><p>Let $(X,\Sigma, \mathbf{p})$ be a measure space. </p>
</li>
<li><p>For any real number $p\geq 1$, we define $\mathcal{L}^p(X,\Sigma,\mu)$ as the set of all random variables $f$ on $(X,\Sigma,\mu)$ such that $|f|^p$ is integrable. </p>
</li>
<li><p>In other words,<br>$$<br>\mathcal{L}^p(X,\Sigma,\mu):=\left{f\in\mathcal{L}^0(X,\Sigma):\int_X|f|^pd\mu&lt;\infty\right}<br>$$</p>
</li>
<li><p>Any one member of $\mathcal{L}^p(X,\Sigma,\mu)$ is said to be $p$-<strong>integrable</strong>.</p>
</li>
</ul>
<blockquote>
<p>There is a natural way of making $\mathcal{L}^p(X,\Sigma,\mu)$ a seminormed linear space.</p>
<p>We define the real map $|\cdot|_p$ on $\mathcal{L}^p(X,\Sigma,\mu)$ by<br>$$<br>|f|_p:=\left(\int_X|f|^pd\mu\right)^\frac{1}{p}<br>$$<br>It is not a normed linear space proper, because $|\cdot|_p$ identifies any two random variables that are distinct from each other only on a negligible set with respect to $\mu$. </p>
<p>In other words, the map $(f, g) \mapsto |f- g|_p$ is a semimetric, but it is not a metric, for it fails to separate points in $\mathcal{L}^p(X,\Sigma,\mu)$. </p>
<p>The problem is that $|f|<em>p =0$ does not yield $f =\mathbf{0}$, it implies only that $f =</em>{a.s.} \mathbf{0}$.</p>
</blockquote>
<p><strong>$\mathcal{L}^p$-bounded</strong>:</p>
<ul>
<li>For any given real number $p\geq1$, we say that a set $\mathcal{X}$ of random variables on a given probability space is <strong>$\mathcal{L}^p$-bounded</strong> if either it is empty or<br>$$<br>\sup{\mathbb{E}(|x|^p):x\in\mathcal{X}}&lt;\infty<br>$$</li>
</ul>
<p><strong>$\mathcal{L}^p$-convergence</strong>:</p>
<ul>
<li><p>For any given real number $p\geq1$, let $x, x_1,x_2,\dots$ be random variables on $\mathcal{L}^p(X,\Sigma,\mu)$</p>
</li>
<li><p>We say that $x_m$ is <strong>$\mathcal{L}^p$-convergent</strong> if<br>$$<br>\lim_{m\to\infty}|x_m-x|<em>p=\lim</em>{m\to\infty}\left(\int_X|x_m-x|^pd\mu\right)^\frac{1}{p}=\lim_{m\to\infty}\mathbb{E}(|x_m-x|^p)=0<br>$$</p>
</li>
<li><p>$\mathcal{L}^1$-convergence is <strong>called convergence in the mean</strong>.</p>
</li>
</ul>
<blockquote>
<p>$\mathcal{L}^p$-convergence is a natural notion of convergence for sequences in this semimetric space.</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 5.4</strong>: </p>
<ul>
<li>Let $X$ be a nonempty set of random variables on a probability space $(X,\Sigma,\mathbf{p})$. </li>
<li>If $X$ is uniformly integrable, then it is $\mathcal{L}^1$-bounded.</li>
</ul>
<p><strong>Proposition 5.5</strong>:</p>
<ul>
<li>Let $X$ be a set of random variables on a probability space $(X,\Sigma,\mathbf{p})$.  </li>
<li>If $X$ is $\mathcal{L}^p$-bounded for some $p &gt; 1$, then it is uniformly integrable.</li>
</ul>
<blockquote>
<p>$$<br>\mathcal{L}^p\mbox{-boundeness}\ \ (p&gt;1)\Longrightarrow\mbox{uniform integrability}\Longrightarrow\mathcal{L}^1\mbox{-boundeness}<br>$$</p>
</blockquote>
<p><strong>Proposition 5.6</strong>:</p>
<ul>
<li>Let $x, x_1,x_2,\dots$ be integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$ such that $\mathbb{E}(|x_m -x|) \to 0$. </li>
<li>Then, $(x_m)$ is uniformly integrable.</li>
</ul>
<blockquote>
<p>$$<br>\mathcal{L}^1\mbox{-convergence}\Longrightarrow\mbox{uniform integrability}\Longrightarrow\mathcal{L}^1\mbox{-boundeness}<br>$$</p>
</blockquote>
<p><strong>Proposition 5.7</strong>: </p>
<ul>
<li>Let $(x_m)$ be a uniformly integrable sequence of random variables on a probability space $(X,\Sigma, \mathbf{p})$ such that $x_m\to_{a.s.} x$ for some $x \in \mathcal{L}^0(X,\Sigma)$</li>
<li>Then, $x$ is integrable and $\mathbb{E}(|x_m-x |) \to 0$.</li>
</ul>
<blockquote>
<p>Almost sure convergence does imply $\mathcal{L}^1$-convergence for uniformly integrable sequences.</p>
</blockquote>
<p><strong>Corollary 5.8</strong>: </p>
<ul>
<li>Let $x, x_1, x_2,\dots$ be integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$ such that $x_m\to_{a.s.} x$. </li>
<li>Then, $\mathbb{E}(|x_m-x|) \to 0$ iff, $(x_m)$ is uniformly integrable.</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example 5.1</strong>: </p>
<ul>
<li>Almost sure convergence does not imply $\mathcal{L}^1$-convergence. </li>
<li>Consider the sequence $(x_m)$ of random variables on the probability space  $([0,1],\mathcal{B}[0,1],\ell)$ where $x_m$ equals $m$ on $[0,\frac{1}{m}]$ and $0$ elsewhere on $[0,1]$. </li>
<li>Then, $x_m(\omega) \to0$ for each $\omega\in (0,1]$, and hence, $x_m \to_{a.s.} 0$. </li>
<li>But $|x_m|_1 = 1$ for each $m$.</li>
</ul>
<p><strong>Example 5.2</strong>: </p>
<ul>
<li>$\mathcal{L}^1$-convergence does not imply almost sure convergence. </li>
<li>Consider $(x_m) := (\mathbf{1}<em>{[0,1)},\mathbf{1}</em>{[0,\frac{1}{2})}, \mathbf{1}<em>{[\frac{1}{2},1)},\mathbf{1}</em>{[0,\frac{1}{3})}, \mathbf{1}<em>{[\frac{1}{3},\frac{2}{3})},\mathbf{1}</em>{[\frac{2}{3},1)},\dots),$ which is a sequence in $\mathcal{L}^1([0,1),\mathcal{B}[0,1),\ell)$. </li>
<li>Clearly, we have $\mathbb{E}(x_m)\to 0$, that is, $|x_m|_1\to 0$. </li>
<li>But $ (x_m(\omega))$ does not converge to a real number for any $\omega$ in $[0, 1)$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T13:54:59.000Z" title="12/15/2020, 9:54:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/10%20Expectation%20via%20the%20Lebesgue%20Integral/Elementary-Probability-Inequalities/">Elementary Probability Inequalities</a></h1><div class="content"><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Jensen’s Inequality</strong>:</p>
<ul>
<li><p>Let $x$ be an integrable random variable on a probability space $(X,\Sigma, \mathbf{p})$, $I$ an open interval that contains $x(X)$, and $\varphi : I \to \mathbb{R}$ a concave function. </p>
</li>
<li><p>Then, $\varphi \circ x \in \mathcal{L}^0(X ,\Sigma)$ and<br>$$<br>\mathbb{E}(\varphi\circ x) \leq \varphi(\mathbb{E}(x))<br>$$</p>
</li>
<li><p>If $\varphi$ is convex, then the inequality goes the other direction.</p>
</li>
</ul>
<blockquote>
<p>Under fairly general conditions, the expectation of a concave transformation of a random variable $x$ is always less than the same transformation of the expectation of $x$</p>
</blockquote>
<p><strong>Lemma 4.1</strong>:</p>
<ul>
<li><p>Let $Y$ be a metric space, and $x$ a $Y$-valued random variable on a probability space $(X,\Sigma, \mathbf{p})$. </p>
</li>
<li><p>Then, for any continuous $\varphi: Y \to \mathbb{R}_+$ and real number $\lambda &gt; 0$, we have<br>$$<br>\mathbf{p}{\varphi\circ x\geq \lambda}\leq\frac{1}{\lambda}\mathbb{E}(\varphi\circ x).<br>$$</p>
</li>
</ul>
<p><strong>Markov’s Inequality</strong>:</p>
<ul>
<li><p>Let $Y$ be a normed metric space, and $x$ a $Y$-valued random variable on a probability space $(X,\Sigma, \mathbf{p})$. </p>
</li>
<li><p>Then, for any real number $\lambda &gt; 0$, we have<br>$$<br>\mathbf{p}{| x|_Y\geq \lambda}\leq\frac{1}{\lambda}\mathbb{E}(| x|_Y).<br>$$</p>
</li>
<li><p>In particular, for any  random variable $x$ on $(x,\Sigma,\mathbf{p})$<br>$$<br>\mathbf{p}{| x|\geq \lambda}\leq\frac{1}{\lambda}\mathbb{E}(| x|).<br>$$</p>
</li>
</ul>
<p><strong>The Chebyshev-Bienaymé Inequality</strong>:</p>
<ul>
<li>For any random variable $x$ defined on a probability space $(X,\Sigma,\mathbf{p})$, and any real number $\lambda&gt; 0$, we have<br>$$<br>\mathbf{p}{| x|\geq \lambda}\leq\frac{1}{\lambda^2}\mathbb{E}(x^2).<br>$$</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T13:53:59.000Z" title="12/15/2020, 9:53:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/10%20Expectation%20via%20the%20Lebesgue%20Integral/The-Expectation-Functional-of-Arbitrary-Random-Variables/">The Expectation Functional of Arbitrary Random Variables</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Expectation of Arbitrary Random Variables</strong>:</p>
<ul>
<li>Let $x$ be an $\overline{\mathbb{R}}$-valued random variable on a probability space $(X,\Sigma, \mathbf{p})$. </li>
<li>Note that $x^+ := \max{x, 0}$ and $x^- := \max{-x,0}$ are $[0, \infty]$-valued random variables on $(X,\Sigma,\mathbf{p})$ with $x = x^++  x^-$ </li>
<li>Define the <strong>expectation</strong> of $x$ as the (extended real) number $\mathbb{E}(x) := \mathbb{E}(x^+) -\mathbb{E}(x^-)$, provided that $\mathbb{E}(x^+)$ and $\mathbb{E}(x^-)$ are not both infinite.<ul>
<li>If $\mathbb{E} (x^+) = \infty = \mathbb{E}(x^-)$, we say that the expectation of $x$ does not exist.</li>
</ul>
</li>
</ul>
<blockquote>
<p>This definition generalizes the one we gave for the expectation of $[0,\infty]$-valued random variables.</p>
</blockquote>
<blockquote>
<p>In real analysis, $\mathbb{E}(x)$ is called the Lebesgue integral of $x$ (with respect to $\mathbf{p}$), and is denoted by $\int_Xxd\mathbf{p}$.</p>
</blockquote>
<p><strong>Variance of Arbitrary Random Variables</strong>:</p>
<ul>
<li><p>In turn, the <strong>variance</strong> of an arbitrary random variable $x$ on $(X,\Sigma,\mathbf{p})$ is the number<br>$$<br>\mathbb{V}(x) := \mathbb{E}((x - \mathbb{E}(x))^2)<br>$$</p>
</li>
<li><p>where $(x-\mathbb{E}(x))^2$ is the arbitrary random variable $\omega\mapsto (x(\omega) -\mathbb{E}(x))^2$ on $(X,\Sigma, \mathbf{p})$.</p>
</li>
</ul>
<blockquote>
<p>However, it is often more convenient to use the alternate formula<br>$$<br>\mathbb{V}(x) := \mathbb{E}(x^2) - \mathbb{E}(x)^2<br>$$</p>
</blockquote>
<p><strong>Integrable</strong>:</p>
<ul>
<li>$x$ is <strong>integrable</strong> (with respect to $\mathbf{p}$) if $\mathbb{E}(|x|)=\mathbb{E}(x^+)+\mathbb{E}(x^-)&lt;\infty$ .</li>
</ul>
<blockquote>
<p>$\mathbb{E}(x)$ exists iff $\min{\mathbb{E}(x^+), \mathbb{E}(x^-)} &lt; \infty$, iff either $\mathbb{E}(x^+)$ or $\mathbb{E}(x^-)$ is finite</p>
<p>$x$ is integrable iff $\max{\mathbb{E}(x^+), \mathbb{E}(x^-)} &lt; \infty$, iff both $\mathbb{E}(x^+)$ and $\mathbb{E}(x^-)$ is finite</p>
<p>$x$ is integrable iff $\mathbb{E}(x)$ exists and it is finite.</p>
</blockquote>
<blockquote>
<p>Integrability of a random variable means simply that the expectation of this random variable is a real number.</p>
</blockquote>
<p><strong>Lebesgue Integration of Aribitrary Maps</strong>:</p>
<ul>
<li><p>Let $(X,\Sigma,\mu)$ be a measure space. </p>
</li>
<li><p>$f : X\to \overline{\mathbb{R}}$ a -measurable map. The <strong>Lebesgue integral of $f$ (with respect to $\mu$</strong>) is defined as the (extended real) number<br>$$<br>\int_Xfdu:=\int_Xf^+d\mu-\int_Xf^-d\mu<br>$$</p>
</li>
<li><p>provided that the right-hand side of this expression is not of the $\infty-\infty$ form. </p>
<ul>
<li>If the latter condition is not met, we say that the Lebesgue integral of $f$ (with respect to $\mu$) does not exist.</li>
</ul>
</li>
<li><p>For any $S\in\Sigma$, we also define<br>$$<br>\int_Sfd\mu:=\int_X\mathbf{1}_Sd\mu,<br>$$</p>
</li>
<li><p>provided that the right-hand side of this expression exists. </p>
</li>
</ul>
<p><strong>Integrable</strong>:</p>
<ul>
<li>$f$ is <strong>integrable</strong> (with respect to $\mu$) if $\int_X |f|d\mu &lt; \infty$.</li>
</ul>
<p><strong>Uniform Integrable</strong>:</p>
<ul>
<li><p>A collection $\mathcal{X}$ of random variables on a probability space $(X,\Sigma,\mathbf{p})$ is said to be <strong>uniformly integrable</strong> (with respect to $\mathbf{p}$) if<br>$$<br>\lim_{ a\to\infty}\sup\left{ \int_{ { \vert x\vert&gt;a} }\vert x\vert d\mathbf{p}:x\in\mathcal{X} \right }=0<br>$$</p>
</li>
<li><p>that is, for every $\varepsilon &gt; 0$, there is a real number $a &gt; 0$ such that<br>$$<br>\int_{ {|x|&gt;a } }|x|d\mathbf{p}&lt;\varepsilon \ \ \ \ \ \mbox{ for each }x\in\mathcal{X}<br>$$</p>
</li>
<li><p>In turn, a sequence $(x_m)$ of random variables on $(X,\Sigma,\mathbf{p})$ is called uniformly integrable if ${x_1, x_2,\dots}$ is <strong>uniformly integrable</strong></p>
</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 2.1</strong>: </p>
<p>For any $\mathbb{R}$-valued random variable $x$ on a probability space $(X,\Sigma,\mathbf{p})$ such that $\mathbb{E}(x)$ exists, we have $|\mathbb{E}(x)| \leq \mathbb{E}(|x|)$</p>
<p><strong>Proposition 2.2</strong>:</p>
<ul>
<li>Let $x$ and $y$ be $\overline{\mathbb{R}}$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$.</li>
<li>For any real number $a$, we have $\mathbb{E}(ax) = a\mathbb{E}(x)$, provided that $\mathbb{E}(x)$ exists.</li>
<li>Furthermore, $\mathbb{E}(x + y) = \mathbb{E}(x) + \mathbb{E}(y)$, provided that $x + y\in \overline{\mathbb{R}}^X$, $\mathbb{E}(x)$ exists, and $y$ is integrable.</li>
</ul>
<blockquote>
<p>The set of all integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$ is denoted by $\mathcal{L}^1(X,\Sigma,\mathbf{p})$, that is,<br>$$<br>\mathcal{L}^1(X,\Sigma,\mathbf{p}):=\left{x\in\mathcal{L}^0(X,\Sigma,\mathbf{p}):\int_X|x|d\mathbf{p}\right}<br>$$<br>$\mathcal{L}^1(X,\Sigma,\mathbf{p})$ is a linear subspace of $\mathcal{L}^0(X,\Sigma)$.</p>
<p>The map $\mathbb{E}$ acts as a linear functional on this linear space.</p>
</blockquote>
<p><strong>Proposition 2.3</strong>:</p>
<ul>
<li><p>Let $x$ and $y$ be $\overline{\mathbb{R}}$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$.</p>
</li>
<li><p>If both $\mathbb{E}(x)$ and $\mathbb{E}(y)$ exist, then<br>$$<br>x\geq_{a.s.}y\ \ \ \ \mbox{ implies }\ \ \ \ \mathbb{E}(x) \geq \mathbb{E}(y)<br>$$</p>
</li>
</ul>
<p><strong>Corollary 2.4</strong>:</p>
<ul>
<li><p>Let $x$ and $y$ be $\overline{\mathbb{R}}$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$.</p>
</li>
<li><p>If both $\mathbb{E}(x)$ and $\mathbb{E}(y)$ exist, then<br>$$<br>x=_{a.s.}y\ \ \ \ \mbox{ implies }\ \ \ \ \mathbb{E}(x) = \mathbb{E}(y)<br>$$</p>
</li>
</ul>
<p><strong>The Monotone Convergence Theorem 2</strong>:</p>
<ul>
<li>Let $x, x_1,x_2,\dots$ be $\overline{\mathbb{R}}$-valued random variables on a probability space $(X,\Sigma, \mathbf{p})$. Then,<br>$$<br>\mathbb{E}(x_1)&gt;-\infty,\ \ \ x_m \uparrow_{a.s.} x\ \ \ \ \ \mbox{ implies }\ \ \ \ \ \ \mathbb{E}(x_m)\uparrow \mathbb{E}(x)<br>$$</li>
</ul>
<p><strong>The Change of Variables Formula</strong>:</p>
<ul>
<li><p>Let $Y$ be a metric space, $x$ a $Y$-valued random variable on a probability space $(X,\Sigma,\mathbf{p})$, and $\varphi$ an $\overline{\mathbb{R}}$-valued random variable on $(Y,\mathcal{B}(Y ),\mathbf{p}_x)$. </p>
</li>
<li><p>If either $\mathbb{E}_{\mathbf{p}_x} (\varphi)$ or $\mathbb{E}_\mathbf{p}(\varphi\circ x)$ exists, then<br>$$<br>\int_Y\varphi d\mathbf{p}_x=\int_X(\varphi\circ x)d\mathbf{p}<br>$$</p>
</li>
</ul>
<blockquote>
<p>The Change of Variables Formula remains valid in the context of any measure space. </p>
</blockquote>
<blockquote>
<p>Note that when $Y = \mathbb{R} $, the distribution function induced by $\mathbf {p}_x $is the distribution function of $x$, and denoted by $F_x$.</p>
<p>And the distribution induced by $F_x$ is the Lebesgue-Stieltjes probability measure actually, that is $\mathbf{p}_x=\ell_x$</p>
</blockquote>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example 2.5</strong>:</p>
<ul>
<li><p>Let $x$ be a nonnegative random variable on a probability space $\mathbb(X,\Sigma,\mathbf{p})$ such that $x(X)$ is countable.</p>
</li>
<li><p>The Change of Variables Formula says that the expectation of $x$ equals the Lebesgue integral of the identity function on $x(X)$ with respect to the probability measure $\mathbf{p}_x$.</p>
</li>
<li><p>Therefore,<br>$$<br>\mathbb{E}(x)=\sum_{a\in x(X)}a\mathbf{p}{x=a}<br>$$</p>
</li>
</ul>
<p><strong>Example 2.6, Geometric distribution</strong>:</p>
<ul>
<li><p>Take any $p\in (0,1)$ and let $x$ be an $\mathbb{N}$-valued random variable on a probability space $(X,\Sigma, \mathbf{p})$ such that $\mathbf{p}{x = i} = p(1-p)^{i-1}$ for each $i\in\mathbb{N}$. </p>
</li>
<li><p>Such a random variable is said to have a <strong>geometric distribution</strong> with parameter $p$.<br>$$<br>\mathbb{E}(x)=p+2p(1-p)+3p(1-p)^2 +\cdots=\frac{p}{1-p}\sum^\infty_{i=1}i(1-p)^i=\frac{p}{1-p}\frac{1-p}{1-(1-p)^2}=\frac{1}p<br>$$</p>
</li>
</ul>
<p><strong>Example 2.7, Poisson distribution</strong>:</p>
<ul>
<li><p>Given a positive real number  $\lambda&gt; 0$, a $\mathbb{Z}_+$-valued random variable $x$ on a probability space $(X,\Sigma, \mathbf{p})$ such that<br>$$<br>\mathbf{p}{x=i}=\frac{e^{-\lambda}\lambda^i}{i!},\ \ \ i=0,1,\dots<br>$$</p>
</li>
<li><p>is said to have a Poisson distribution with parameter $\lambda$.</p>
</li>
<li><p>Since $e^\lambda=1+\lambda+\frac{\lambda^2}{2}+\frac{\lambda^3}{3!}+\cdots$</p>
</li>
<li><p>We have $\mathbb{E}(x)=\lambda$, $\mathbb{E}(x^2)=\lambda+\lambda^2$ and $\mathbb{V}(x)=\lambda$.</p>
</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li><p>Consider the experiment of throwing a single die, that is, the probability space $(X, 2^X,\mathbf{p})$ where $X := [6]$ and $\mathbf{p}(S) := \frac{|S|}{6} $ for all $S\in 2^X$.</p>
</li>
<li><p>Consider the random variable $x\in[2]^X$ which is defined as<br>$$<br>x(\omega):=\left{\begin{matrix}1,&amp;\mbox{ if }\omega\mbox{ is even}\2, &amp;\mbox{ if }\omega\mbox{ is odd}\end{matrix}\right.<br>$$</p>
</li>
<li><p>In addition, define $\varphi : [2] \to \mathbb{R}$ by $\varphi(t) := t^2$, and observe that $\varphi$ is a ${1,4}$-valued random variable on $([2], 2^{[2]},\mathbf{p}_x)$.</p>
</li>
<li><p>Clearly, we have $\mathbb{E}_{\mathbf{p}_x} (\varphi) = (1) \frac{1}{2} + (4) \frac{1}{2} = \frac{5}{2}$.</p>
</li>
<li><p>On the other hand, $\varphi\circ x$ is a ${1,4}$-valued random variable on $(X,2^X,\mathbf{p})$ that takes value $1$ if the outcome of the experiment is even and $4$ if the outcome is odd. </p>
</li>
<li><p>Thus, we have $\mathbb{E}_{\mathbf{p}} (\varphi\circ x) = (1) \frac{1}{2} + (4) \frac{1}{2} = \frac{5}{2}$.</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T13:52:59.000Z" title="12/15/2020, 9:52:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/10%20Expectation%20via%20the%20Lebesgue%20Integral/The-Expectation-Functional-of-Simple-Random-Variables/">The Expectation Functional of Simple Random Variables</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Expectation of Simple Random Variables</strong>:</p>
<ul>
<li><p>Let $(X,\Sigma, \mathbf{p})$ be a probability space. </p>
</li>
<li><p>The set of all simple random variables on $(X,\Sigma, \mathbf{p})$ is a linear space relative to the pointwise defined addition and scalar multiplication operations.</p>
</li>
<li><p>By definition, for any simple random variable $x$ on $(X,\Sigma, \mathbf{p})$, we have $|x(X)| &lt; \infty$ and<br>$$<br>x=\sum_{a\in x(X)}a\mathbf{1}_{x=a}<br>$$</p>
</li>
<li><p>where $\mathbf{1}_{ {x=a} }$ stands for the indicator function of the event ${x = a}\in\Sigma$ on $X$.</p>
</li>
<li><p>We define the <strong>expectation</strong> of any such $x$ as the real number<br>$$<br>\mathbb{E}(x):=\sum_{a\in  x(X)}a\mathbf{p}{x=a}<br>$$</p>
</li>
<li><p>Thus, the expected value of $x$ is the weighted average of its values, where the weight of $a$ is $\mathbf{p}<em>x{a}$ for each $a\in x(X)$. That is, $\mathbf{E}(x) =\sum</em>{a\in x(X)}\mathbf{p}_x{a}$.</p>
</li>
</ul>
<blockquote>
<p>Linearity of $\mathbb{E}$:<br>$$<br>\mathbb{E}(x+y)=\mathbb{E}(x)+\mathbb{E}(y)<br>$$<br>Monotonicity of $\mathbb{E}$:<br>$$<br>\mathbb{E}(x) \geq \mathbb{E}(y)\mbox{ whenever }x\geq_{a.s.}y<br>$$</p>
<p>$$<br>\mathbb{E}(x)= \mathbb{E}(y)\mbox{ whenever }x=_{a.s.}y<br>$$</p>
</blockquote>
<p><strong>Variance of Simple Random Variables</strong>:</p>
<ul>
<li><p>The <strong>variance</strong> of a simple random variable $x$ on $(X,\Sigma,\mathbf{p})$ is the number<br>$$<br>\mathbb{V}(x) := \mathbb{E}((x - \mathbb{E}(x))^2)<br>$$</p>
</li>
<li><p>where $(x-\mathbb{E}(x))^2$ is the simple random variable $\omega\mapsto (x(\omega) -\mathbb{E}(x))^2$ on $(X,\Sigma, \mathbf{p})$.</p>
</li>
</ul>
<blockquote>
<p>However, it is often more convenient to use the alternate formula<br>$$<br>\mathbb{V}(x) := \mathbb{E}(x^2) - \mathbb{E}(x)^2<br>$$</p>
</blockquote>
<p><strong>Expectation of Nonnegative Random Variables</strong></p>
<ul>
<li><p>Let $x$ be a $[0, \infty]$-valued random variable on a probability space $(X,\Sigma,\mathbf{p})$. </p>
</li>
<li><p>We define the <strong>expectation</strong> of $x$ as the (extended real) number<br>$$<br>\mathbb{E}(x) := \sup{\mathbb{E}(z) : z \in \mathfrak{L}(x)}<br>$$</p>
</li>
<li><p>where $\mathfrak{L}(x)$ stands for the set of all simple random variables $z$ such that $z\leq x$.</p>
</li>
<li><p>In turn, the <strong>variance</strong> of $x$ is defined as the (extended real) number<br>$$<br>\mathbb{V}(x) := \mathbb{E}((x - \mathbb{E}(x))^2),<br>$$</p>
</li>
<li><p>provided that $\mathbb{E}(x) &lt; \infty$. </p>
</li>
<li><p>where $(x-\mathbb{E}(x))^2$ is the simple random variable $\omega\mapsto (x(\omega) -\mathbb{E}(x))^2$ on $(X,\Sigma, \mathbf{p})$.</p>
</li>
</ul>
<blockquote>
<p>These definitions agree with those in the case of simple random variables on $(X,\Sigma, \mathbf{p})$. Moreover, they extend those definitions to the case of $[0,\infty]$-valued simple random variables.</p>
</blockquote>
<blockquote>
<p>In words, the (extended real) number $\mathbb{E}(x)$ is defined as the supremum of the weighted averages of all those simple random variables that are “smaller than” $x$ everywhere. </p>
<p>So, in this sense, the idea behind the definition of $\mathbb{E}(x)$ is reminiscent of that of the computation of the area under a given curve in $\mathbb{R}\times \mathbb{R}_+$ by approximating this area with the sum of the areas of the rectangles that lie under the curve and above the horizontal axis.</p>
<p>Put this way, you should see that $\mathbb{E}(x)$ can be thought of as some sort of an integral of $x$ – it is called the <strong>Lebesgue integral</strong> of $x$ with respect to $\mathbf{p}$ –where the sets in the domain of $x$ (analogous to the bases of the rectangles under the curve) are “measured” according to the underlying probability measure.</p>
</blockquote>
<blockquote>
<p>The commonly used notation for the Lebesgue integral of $x$ on $X$ with respect to $\mathbf{p}$ is $\int_Xxdp$, that is<br>$$<br>\int_X xd\mathbf{p}\ \mbox{ and }\ \mathbb{E}(x)<br>$$<br>denote the same (extended real) number. </p>
</blockquote>
<blockquote>
<p>Adopting the widely used conventions of integration theory, we also set<br>$$<br>\int_Sxd\mathbf{p}:=\mathbb{E}(x\mathbf{1}_S)\mbox{ for any } S\in \Sigma,<br>$$<br>where $\mathbf{1}_S$ is the indicator function of $S$ on $X$. Therefore, recalling the definition of the expectation of a simple random variable, we see that $\int_S d\mathbf{p}$, that is, the Lebesgue integral of the constant function $1$ with respect to $\mathbf{p}$ on $S$, and $\mathbb{E}(\mathbf{1}_S )$ denote the same number, namely, $\mathbf{p}(S)$, for any $S\in\Sigma$.</p>
</blockquote>
<blockquote>
<p>Many authors write<br>$$<br>\int_X x(\omega)\mathbf{p}(dw)<br>$$<br>instead of $\int_Xxd\mathbf{p}$. The Lebesgue integral of the map $\omega\mapsto\omega^2$ on the probability space $([0,1],\mathcal{B}[0,1],\ell)$ is, for instance, written as $\int_{[0,1]}\omega^2\ell(d\omega)$. </p>
</blockquote>
<p><strong>Lebesgue Integration of Simple Maps</strong>:</p>
<ul>
<li><p>Let $(X,\Sigma,\mu )$ be a measure space. </p>
</li>
<li><p>By <strong>a nonnegative simple $\Sigma$-measurable map</strong> on $X$, we mean a real function $f\in \mathcal{L}^0(X,\Sigma)$ such that $f\geq0$ and $f(X)$ is finite. </p>
</li>
<li><p>The <strong>Lebesgue integral</strong> of any such map with respect to $\mu$ is defined as the number<br>$$<br>\int_Xfd\mu:=\sum_{a\in f(X)}a\mu{f=a},<br>$$</p>
</li>
<li><p>where we adopt the convention that $0\cdot\infty$ equals $0$.</p>
</li>
</ul>
<p><strong>Lebesgue Integration of Nonnegative Maps</strong>:</p>
<ul>
<li><p>Let $(X,\Sigma,\mu)$ be a measure space. </p>
</li>
<li><p>The <strong>Lebesgue integral of any $\Sigma$-measurable</strong> $f : X \to [0, \infty]$ with respect to $\mu$ is defined as the (extended) real number<br>$$<br>\int_X fd\mu:=\sup{\int_Xhd\mu:h\in\mathfrak{M}(f)}<br>$$</p>
</li>
<li><p>where $\mathfrak{M}(f)$ stands for the set of all nonnegative simple $\Sigma$-measurable maps $h$ on $X$ with $h\leq f$. </p>
</li>
<li><p>In turn, we define<br>$$<br>\int_S fd\mu:=\int_X f\mathbf{1}_S d\mu\ \ \ \ \mbox{ for any }S\in\Sigma<br>$$</p>
</li>
<li><p>where $\mathbf{1}_S$ is the indicator function of $S$ on $X$.</p>
</li>
</ul>
<p><strong>$\sigma$-finite Measure</strong>:</p>
<ul>
<li>Let $(X,\Sigma,\mu)$ be a measure space. </li>
<li>We say that this space (or $\mu$ itself) is <strong>$\sigma$-finite</strong> if there is a countable partition $\mathcal{S}$ of $X$ such that $\mathcal{S}\subseteq \Sigma$ and $\mu(S) &lt; \infty$ for each $S \in\mathcal{S}$</li>
</ul>
<p><strong>Absolutely Continuous, Density Function, Density</strong>:</p>
<ul>
<li><p>Let $F$ be a distribution function. </p>
</li>
<li><p>We say that $F$ is <strong>absolutely continuous</strong> (with respect to the Lebesgue measure) if there is a Borel measurable map $f : \mathbb{R} \to \mathbb{R}<em>+$ such that<br>$$<br>F(t)=\int</em>{(-\infty,t]}fd\ell<br>$$</p>
</li>
<li><p>for every real number $t$.</p>
</li>
<li><p>When this is the case, we say that $F$ is induced by the <strong>density function</strong> $f$, and refer to $f$ as a <strong>density</strong> for $F$.</p>
</li>
</ul>
<blockquote>
<p>When $f$ is a density for $F$, $F(t)-F(s)=\int_{(s,t]}fd\ell$ for any real numbers $s$ and $t$ with $t &gt; s$.</p>
</blockquote>
<blockquote>
<p>Some distribution functions do not have closed form decriptions, but they are rather defined through integrating a density function.</p>
<p>The most important example of such a function is the so-called <strong>normal distribution</strong> function (with parameters $\mu $ and $\sigma$). For any given real numbers $\mu$ and $\sigma&gt;0$, this function $F$ is defined by<br>$$<br>f(t)=\frac{1}{\sqrt{2\pi}}e^{\frac{(t-\mu)^2}{2\sigma^2}}<br>$$<br>for any real number $t$, it is thus trivially absolutely continuous.</p>
<p>If  $\mu= 0$ and $\mu = 1$ here, this function is called the <strong>standard normal distribution</strong> <strong>function</strong>.</p>
</blockquote>
<blockquote>
<p>Not all distribution function arises from a density funtion.</p>
</blockquote>
<p><strong>Absolutely Continuous Measures</strong>:</p>
<ul>
<li>Let $(X,\Sigma)$ be a measurable space, and $\mu$ and $\nu$  two measures on $\Sigma$. </li>
<li>We say that $\mu$ is <strong>absolutely continuous with respect to $\nu$</strong>, this is denoted by writing    $\mu\ll\nu$ </li>
<li>if $\mu(S) = 0$ for every $S\in\Sigma$  with $\nu(S) = 0$.</li>
</ul>
<blockquote>
<p>The zero measure on $\Sigma$ is absolutely continuous with respect to any measure on $\Sigma$</p>
<p>While the counting measure on $\Sigma$ is not absolutely continuous with respect to any measure $\nu$ on $\Sigma$  such that $\nu(S) = 0$ for some nonempty $S\in\Sigma$ .</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1.1</strong>: </p>
<ul>
<li><p>For any $[0,\infty]$-valued random variables $x$ and $y$ (on a given probability space),<br>$$<br>x\geq_{a.s.}y\ \ \ \ \mbox{ implies }\ \ \ \ \mathbb{E}(x) \geq \mathbb{E}(y)<br>$$</p>
<p>$$<br>x=_{a.s.}y\ \ \ \ \mbox{ implies }\ \ \ \ \mathbb{E}(x) = \mathbb{E}(y)<br>$$</p>
</li>
</ul>
<p><strong>The Monotone Convergence Theorem 1</strong>:</p>
<ul>
<li>Let $x, x_1,x_2,\dots$ be $[0,\infty]$-valued random variables on a probability space $(X,\Sigma, \mathbf{p})$. Then,<br>$$<br>x_m \uparrow_{a.s.} x\ \ \ \ \ \mbox{ implies }\ \ \ \ \ \ \mathbb{E}(x_m)\uparrow \mathbb{E}(x)<br>$$</li>
</ul>
<p><strong>Proposition 1.2</strong>: </p>
<ul>
<li>Let $x$ and $y$ be two $[0,\infty]$-valued random variables on a probability space $(X,\Sigma, \mathbf{p})$. Then, for any $a\geq 0$,<br>$$<br>\mathbb{E}(ax+y)=a\mathbb{E}(x)+\mathbb{E}(y)<br>$$</li>
</ul>
<p><strong>Proposition 1.3</strong>: </p>
<ul>
<li><p>Let $(X,\Sigma,\mu )$ be a finite measure space. Then, there is a real number  $\lambda\geq 0$ and a probability measure $\mathbf{p}$ on $\Sigma$ such that<br>$$<br>\int_Xfd\mu=\lambda\int_Xfd\mathbf{p}<br>$$</p>
</li>
<li><p>for every $\Sigma$-measurable $f : X \to [0,\infty]$.</p>
</li>
</ul>
<p><strong>Proposition 1.5</strong>:</p>
<ul>
<li><p>Given any measure space $(X,\Sigma,\mu )$,<br>$$<br>\int_x(af+g)d\mu=a\int_Xfd\mu+\int_Xgd\mu<br>$$</p>
</li>
<li><p>for every $a\geq0$ and $\Sigma$-measurable $f : X \to [0,\infty]$.</p>
</li>
</ul>
<p><strong>Proposition 1.6</strong>: </p>
<ul>
<li>Every absolutely continuous distribution function is uniformly continuous.</li>
</ul>
<blockquote>
<p>If a distribution function is not continuous even at a single point, then it cannot possibly possess a density.</p>
<p>The converse of Proposition 1.6 is false. Indeed, even a continuous distribution function need not possess a density.</p>
</blockquote>
<p><strong>Proposition 1.7</strong>:</p>
<ul>
<li><p>Let $(X,\Sigma,\nu )$ be a measure space and take any $f\in \mathcal{L}^0_+(X,\Sigma )$. </p>
</li>
<li><p>Then, the map $\mu:\Sigma \to\mathbb{R}$ defined by<br>$$<br>\mu(S):=\int_S fd\nu,<br>$$</p>
</li>
<li><p>is a measure on $\Sigma$ with  $\mu\ll\nu$ . </p>
</li>
<li><p>If $\nu$ is $\sigma$-finite, so is $\mu$.</p>
</li>
</ul>
<p><strong>The Radon-Nikodym Theorem</strong>:</p>
<ul>
<li>Let $(X,\Sigma)$ be a measurable space, and $\mu$ and $\nu$ two $\sigma$-finite measures on $\Sigma$ with $\mu\ll\nu$. Then, there is an $f\in \mathcal{L}^0_+ ( X ,\Sigma)$ such that<br>$$<br>\mu(S)=\int_Sfd\nu\mbox{ for every }S\in\Sigma<br>$$</li>
</ul>
<blockquote>
<p>Any map $f\in\mathcal{L}^0_+(X,\Sigma)$ such that above equation holds is commonly referred to as either the <strong>density of $\mu$ with respect to $\nu$</strong> or as the <strong>Radon-Nikodym derivative of $\mu$ with respect to $\nu$</strong>. </p>
<p>It is quite standard to denote any such map as $\frac{d\mu}{d\nu}$.</p>
</blockquote>
<p><strong>Proposition 1.8</strong>:</p>
<ul>
<li>Let $F$ be a distribution function and $\mathbf{p}_F$ the Lebesgue-Stieltjes probability measure on $\mathbb{R}$ induced by $F$. </li>
<li>Then, $F$ is absolutely continuous iff, $\mathbf{p}_F$ is absolutely continuous with respect to $\ell$.</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example 1.1, Binomial Distribution</strong>: </p>
<ul>
<li><p>Let $n$ be a positive integer and $p$ a number in $[0, 1]$.</p>
</li>
<li><p>A ${0,\dots, n}$-valued random variable $x$ on a probability space $(X,\Sigma,\mathbf{p})$ such that<br>$$<br>\mathbf{p}{x=i}=\left(\begin{matrix}n\ i\end{matrix}\right)p^i(1-p)^{n-i},\ \ \ \ i=0,\dots,n<br>$$</p>
</li>
<li><p>is said to have a binomial distribution with parameters $n$ and $p$</p>
</li>
<li><p>If $n = 1$ here, we say that $x$ has a Bernoulli distribution with parameter $p$</p>
</li>
<li><p>When $n = 1$, we have $\mathbb{E}(x) = p(1) + (1-p)(0) = p$, that is, the expected value of any random variable that has a Bernoulli distribution with parameter $p$ is $p$.</p>
</li>
<li><p>In this case, we also find that $\mathbb{E}(x^2) = p$, so $\mathbb{V}(x) = p -p^2$</p>
</li>
<li><p>More generally, for any random variable that has a binomial distribution with parameters $n$ and $p$, we have<br>$$<br>\mathbb{E}(x)=\sum^n_{i=0}\left(\begin{matrix}n\ i \end{matrix}\right)p^i(1-p)^{n-i}i=np\sum^{n-1}_{i=0}\left(\begin{matrix}n-1\ i \end{matrix}\right)p^i(1-p)^{(n-1)-i}<br>$$</p>
</li>
<li><p>so, by the Binomial Theorem, $\mathbb{E}(x) = np(p + (1 -p))^{n-1} = np$. By using a similar</p>
<p>method, we can also show that $\mathbb{E}(x^2) = n^2 p^2 -np^2 + np$, so $\mathbb{V}(x) = np(1-p)$.</p>
</li>
</ul>
<p><strong>Example 1.3</strong>:</p>
<ul>
<li><p>Let $(X,2^X,\mathbf{p})$ be a probability space with $X$ being a countable set. </p>
</li>
<li><p>We wish to find an expression for the expectation of an arbitrary nonnegative random variable $x$ on $(X,2^X,\mathbf{p})$.</p>
</li>
<li><p>Assume that $X$ is countably infinite, and enumerate it as $X := {\omega_1,\omega_2,\dots}$. </p>
</li>
<li><p>For every positive integer $m$, let us define the simple random variable<br>$$<br>x_m(\omega)=\left{\begin{matrix}x(\omega),&amp;\mbox{ if }\omega\in{\omega_1,\dots,\omega_m}\0,&amp;\mbox{otherwise}\end{matrix}\right.<br>$$</p>
</li>
<li><p>By definition of $\mathbb{E}$ for simple random variables, we have $\mathbb{E}(x_m) =\sum_{i\in[m]} x(\omega_i)\mathbf{p}{\omega_i}$ for each $m$.</p>
</li>
<li><p>But, $x_m \uparrow x$, so we have $\mathbb{E}(x_m)\uparrow \mathbb{E}(x)$ by the Monotone Convergence Theorem 1. Consequently,<br>$$<br>\mathbb{E}(x)=\lim\mathbb{E}(x_m)=\lim\sum_{i\in[m]}x(\omega_i)\mathbf{p}{\omega_i}=\sum^\infty_{i=1}x(\omega_i)\mathbf{p}{\omega_i}=\sum_{x\in X}x(\omega)\mathbf{p}{\omega}<br>$$</p>
</li>
<li><p>for any nonnegative random variable $x$ on $(X,2^X,\mathbf{p})$.</p>
</li>
</ul>
<p><strong>Example 1.5</strong>: </p>
<ul>
<li><p>For any real numbers $a$ and $b$ with $a &lt; b$, the uniform distribution $F$ on $[a,b]$ is absolutely continuous.</p>
</li>
<li><p>The map $f:\mathbb{R}\to\mathbb{R}+$ where<br>$$<br>f(t)=\left{\begin{matrix}\frac{1}{a-b},&amp;0\leq t\leq b\0,&amp;\mbox{otherwise}\end{matrix}\right.<br>$$</p>
</li>
<li><p>is a density for that distribution function. </p>
</li>
<li><p>Similarly, for any $\lambda &gt; 0$, the exponential distribution with parameter $\lambda$ ,is absolutely continuous.</p>
</li>
<li><p>The map $f:\mathbb{R}\to\mathbb{R}+$ where<br>$$<br>f(t)=\left{\begin{matrix}\lambda e^{-\lambda t},&amp; t\geq 0\0,&amp;\mbox{otherwise}\end{matrix}\right.<br>$$</p>
</li>
<li><p>is a density for that distribution function. </p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="hqin"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">hqin</p><p class="is-size-6 is-block">A Student in Economics</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Zhengzhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">52</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">5</p></a></div></div></nav></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:23:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Constrained-Problems/">Constrained Problems</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:22:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Infinite-Planning-Horizon/">Infinite Planning Horizon</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:19:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Second-Order-Conditions/">Second-Order Conditions</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:17:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Transversality-Conditions-for-Variable-Endpoint-Problems/">Transversality Conditions for Variable-Endpoint Problems</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:17:21.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/The-Fundamental-Problem-of-the-Calculus-of-Variations/">The Fundamental Problem of the Calculus of Variations</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li></ul></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://ygnmax.github.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Guangnan Yang</span></span><span class="level-right"><span class="level-item tag">ygnmax.github.io</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Abstract-Algebra/"><span class="tag">Abstract Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Control-Theory/"><span class="tag">Control Theory</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Measure-Theoretic-Probability/"><span class="tag">Measure Theoretic Probability</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Real-Analysis/"><span class="tag">Real Analysis</span><span class="tag">24</span></a></div></div></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/1-Mathematics/"><span class="level-start"><span class="level-item">1 Mathematics</span></span><span class="level-end"><span class="level-item tag">52</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/"><span class="level-start"><span class="level-item">1.1 Analysis</span></span><span class="level-end"><span class="level-item tag">44</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-A-Preliminaries-of-Real-Analysis/"><span class="level-start"><span class="level-item">1.1.A Preliminaries of Real Analysis</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/"><span class="level-start"><span class="level-item">1.1.B Metric Spaces and Continuity</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-C-Linear-Spaces-and-Convexity/"><span class="level-start"><span class="level-item">1.1.C Linear Spaces and Convexity</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-D-Metric-Linear-Spaces-and-Normed-Linear-Spaces/"><span class="level-start"><span class="level-item">1.1.D Metric Linear Spaces and Normed Linear Spaces</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-E-Probability-via-Measure-Theory/"><span class="level-start"><span class="level-item">1.1.E Probability via Measure Theory</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/"><span class="level-start"><span class="level-item">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-G-Weak-Convergence-and-Probability-Limit/"><span class="level-start"><span class="level-item">1.1.G Weak Convergence and Probability Limit</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/"><span class="level-start"><span class="level-item">1.1.H Stochastic Independence and Dependence</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-2-Algebra/"><span class="level-start"><span class="level-item">1.2 Algebra</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-2-Algebra/1-2-A-Abstract-Algebra/"><span class="level-start"><span class="level-item">1.2.A Abstract Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-2-Algebra/1-2-B-Linear-Algebra/"><span class="level-start"><span class="level-item">1.2.B Linear Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/"><span class="level-start"><span class="level-item">1.4 Dynamic Optimization</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-A-Preliminaries-of-Dynamic-Optimization/"><span class="level-start"><span class="level-item">1.4.A Preliminaries of Dynamic Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/"><span class="level-start"><span class="level-item">1.4.B The Calculus of Variations</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">hqin</a><p class="is-size-7"><span>&copy; 2020 - 2021 hqin</span>  </p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'folded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>