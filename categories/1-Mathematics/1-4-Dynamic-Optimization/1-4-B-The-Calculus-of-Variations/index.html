<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: 1.4.B The Calculus of Variations - hqin</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="hqin"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="hqin"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="hqin"><meta property="og:url" content="https://hqin-2020.github.io/"><meta property="og:site_name" content="hqin"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://hqin-2020.github.io/img/og_image.png"><meta property="article:author" content="hqin"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hqin-2020.github.io"},"headline":"hqin","image":["https://hqin-2020.github.io/img/og_image.png"],"author":{"@type":"Person","name":"hqin"},"description":""}</script><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="hqin" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">hqin</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/1-Mathematics/">1 Mathematics</a></li><li><a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a></li><li class="is-active"><a href="#" aria-current="page">1.4.B The Calculus of Variations</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-03T14:22:22.000Z" title="1/3/2021, 10:22:22 PM">2021-01-03</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Infinite-Planning-Horizon/">Infinite Planning Horizon</a></h1><div class="content"><h2 id="Methodological-Issues-of-Inifinite-Horizon"><a href="#Methodological-Issues-of-Inifinite-Horizon" class="headerlink" title="Methodological Issues of Inifinite Horizon"></a>Methodological Issues of Inifinite Horizon</h2><p>​    </p>
<ul>
<li>The convergence problem arises because the objective functional, now in the form of $\int^\infty_0F(t, y, y’)dt$, is an improper integral which may or may not have a finite value. </li>
<li>In the case where the integral diverges, there may exist more than one $y(t)$ path that yields an infinite value for the objective functional and it would be difficult to determine which among these paths is optimal. </li>
<li>There are several certain conditions that are sufficient for convergence.</li>
</ul>
<h3 id="Condition-I"><a href="#Condition-I" class="headerlink" title="Condition I"></a>Condition I</h3><p>Given the improper integral $\int^\infty_0F(t, y, y’) dt$, if the integrand $F$ is finite throughout the interval of integration, and if $F$ attains a zero value at some finite point of time, say, $t_0$, and remains at zero for all $t &gt; t_0$, then the integral will converge.</p>
<ul>
<li>Although the integral nominally has an infinite horizon, the effective upper limit of integration is a finite value, $t_0$. Thus, the given improper integral reduces in effect to a proper one, with assurance that it will integrate to a finite value.</li>
</ul>
<h3 id="Condition-III"><a href="#Condition-III" class="headerlink" title="Condition III"></a>Condition III</h3><p>In the integral $\int^\infty_0F(t,y,y’)dt$, if the integrand takes the form of $G(t,y,y’)e^{-\rho t}$, where $\rho$ is a positive rate of discount, and the $G$ function is bounded, then the integral will converge.</p>
<ul>
<li><p>A distinguishing feature of this integral is the presence of the discount factor $e^{\rho t}$ which, ceteris paribus, provides a dynamic force to drive the integrand down toward zero over time at a good speed. </p>
</li>
<li><p>When the $G(t,y,y’)$ component of the integrand is positive (as in most economic applications) and has an upper hound, say, $\hat{G}$, then the downward force of $e^\rho t$ is sufficient to make the integral converge.</p>
</li>
<li><p>More formally, since the value of the $G$ function can never exceed the value of the constant $\hat{G}$, we can write<br>$$</p>
<pre><code>    \int^\infty_0G(t,y,y&#39;)e^&#123;-\rho t&#125;dt\leq\int^\infty_0\hat&#123;G&#125;e^&#123;-\rho t&#125;dt=\frac&#123;\hat&#123;G&#125;&#125;&#123;\rho&#125;\tag&#123;5.3&#125;</code></pre>
<p>$$</p>
</li>
<li><p>It follows that the first integral must also be convergent.</p>
</li>
</ul>
<h3 id="The-Matter-of-Transversality-Conditions"><a href="#The-Matter-of-Transversality-Conditions" class="headerlink" title="The Matter of Transversality Conditions"></a>The Matter of Transversality Conditions</h3><p>When the planning horizon is infinite, there is no longer a specific terminal $T$ value for us to adhere to. And the terminal state may also be left open. Thus transversality conditions are needed.</p>
<ul>
<li><p>Recalling<br>$$<br>[F-y’F_{y’}]<em>{t=T}\Delta T+[F</em>{y’}]<em>{t=T}\Delta y</em>{T}=0\ \ \ \ \ \ \mbox{  [General Transversality Condition]}\tag{3.9}<br>$$</p>
</li>
<li><p>In the present context, $(3.9)$ must be modified to<br>$$</p>
<pre><code>    [F-y&#39;F_&#123;y&#39;&#125;]_&#123;t\to \infty&#125;\Delta T+[F_&#123;y&#39;&#125;]_&#123;t\to \infty&#125;\Delta y_&#123;T&#125;=0\tag&#123;5.4&#125;</code></pre>
<p>$$</p>
<ul>
<li>where each of the two terms must individually vanish.</li>
</ul>
</li>
</ul>
<p>Since there is no fixed $T$ in the present context, $\Delta T$ is perforce nonzero, and this necessitates the condition<br>$$<br>\lim_{t\to \infty}(F-y’F_{y’})=0\ \ \ \ \ \ [\mbox{Transversalilty Condition for the Infinite Horizon}]\tag{5.5}<br>$$</p>
<p>As to the second term in $(5.4)$, if an asymptotic terminal state is specified in the problem:<br>$$<br>\lim_{t\to \infty}y(t)=y_{\infty}=\mbox{a given constant}\tag{5.6}<br>$$</p>
<ul>
<li><p>then the second term in $(5.4)$ will vanish on its own $(\Delta y_T = 0)$ and no transversality condition is needed. </p>
</li>
<li><p>But if the terminal state is free, then we should impose the additional condition<br>$$<br>\lim_{t\to\infty} F_{y’}=0\ \ \ \ \ \ [\mbox{Transversalilty Condition for Free Terminal State}]\tag{5.7}<br>$$</p>
</li>
</ul>
<h2 id="The-Optimal-Social-Saving-Behavior"><a href="#The-Optimal-Social-Saving-Behavior" class="headerlink" title="The Optimal Social Saving Behavior"></a>The Optimal Social Saving Behavior</h2><h3 id="The-Ramsey-Model"><a href="#The-Ramsey-Model" class="headerlink" title="The Ramsey Model"></a>The Ramsey Model</h3><p>The central question addressed by Ramsey is that of intertemporal resource allocation: </p>
<ol>
<li>How much of the national output at any point of time should be for current consumption to yield current utility, and </li>
<li>how much should be saved (and invested) so as to enhance future production and consumption, and hence yield future utility?</li>
</ol>
<p>$$<br>\mbox{Maximize}\ \int^\infty_0[U(C)-D(L)]dt\tag{5.26}<br>$$</p>
<ul>
<li><p>where<br>$$</p>
<pre><code>C=Q(K,L)-K&#39;\tag&#123;5.25&#125;</code></pre>
<p>$$</p>
</li>
<li><p>since we assuming away depreciation.<br>$$<br>Q(K,L)=C+S=C+K’<br>$$</p>
<ul>
<li>This implies the output can be consumed or saved, but what is saved always results in investment and capital accumulation. </li>
<li>Utility function $U(C)$ has nonincreasing marginal utility, $U’’(C)\leq 0$</li>
</ul>
</li>
<li><p>In order to produce its consumption goods, society incurs disutility of labor $D(L)$, with nondecreasing marginal utility, $D’’(L)\geq 0$</p>
</li>
</ul>
<h3 id="The-Question-of-Convergence"><a href="#The-Question-of-Convergence" class="headerlink" title="The Question of Convergence"></a>The Question of Convergence</h3><p>The absence of discount factor unfortunately forfeits the opportunity to take advantage of Condition III to establish convergence, even if the integrand has an upper bound. </p>
<ul>
<li>Thus replace (5.26)​ with the following substitute problem:</li>
</ul>
<p>$$<br>\begin{array}{lll}<br>        \mbox{Minimize} &amp;\int^\infty_0[B-U(C)+D(L)]dt\<br>        \<br>        \mbox{subject to} &amp;K(0)=K_0&amp;(K_0)\ \mbox{given}<br>    \end{array}\tag{5.26’}<br>$$</p>
<ul>
<li>where $B$ (for Bliss) is a postulated maximum attainable level of net utility.</li>
<li>Condition I guarantees the convergence.</li>
</ul>
<h3 id="The-Solution-of-the-Model"><a href="#The-Solution-of-the-Model" class="headerlink" title="The Solution of the Model"></a>The Solution of the Model</h3><p>From (5.26’)​ we get<br>$$<br>F=B-U(C)+D(L)\ \ \ \ \ \ \ \mbox{where}\ C=Q(K,L)-K’<br>$$</p>
<ul>
<li><p>The derivative with respect to $L$ and $L’$:<br>$$<br>\begin{aligned}</p>
<pre><code>    &amp;F_L=-U&#39;(C)\frac&#123;\partial C&#125;&#123;\partial L&#125;+D&#39;(L)\equiv-\mu Q_L+D&#39;(L)\\
    &amp;F_&#123;L&#39;&#125;=0
\end&#123;aligned&#125;</code></pre>
<p>$$</p>
</li>
<li><p>The derivative with respect to $K$ and $K’$:<br>$$</p>
<pre><code>\begin&#123;aligned&#125;
&amp;F_K=-U&#39;(C)\frac&#123;\partial C&#125;&#123;\partial K&#125;\equiv-\mu Q_K\\
&amp;F_&#123;K&#39;&#125;=-U&#39;(C)\frac&#123;\partial C&#125;&#123;\partial K&#125;=-U&#39;(C)(-1)=\mu
\end&#123;aligned&#125;</code></pre>
<p>$$</p>
</li>
</ul>
<p>According to the Euler Equation<br>$$<br>F_{y_j}-\frac{d}{dt}F_{y’_j}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.27}<br>$$</p>
<ul>
<li><p>we have<br>$$</p>
<pre><code>    \begin&#123;aligned&#125;
        &amp;F_L-\frac&#123;dF_&#123;L&#39;&#125;&#125;&#123;dt&#125;=0\\
        &amp;F_K-\frac&#123;dF_&#123;K&#39;&#125;&#125;&#123;dt&#125;=0
    \end&#123;aligned&#125;</code></pre>
<p>$$</p>
</li>
<li><p>which means<br>$$</p>
<pre><code>    D&#39;(L)=\mu Q_L\ \ \ \ \mbox&#123;for all &#125; t\geq0\tag&#123;5.27&#125;</code></pre>
<p>$$</p>
<p>$$<br>\frac{d\mu/dt}{\mu}=-Q_K\ \ \ \ \mbox{for all } t\geq0\tag{5.28}<br>$$</p>
</li>
<li><p>The marginal disutility of labor must, at each point of time, be equated to the product of the marginal utility of consumption and the marginal product of labor.</p>
</li>
<li><p>The marginal utility of consumption, must at every point of time have a growth rate equal to the negative of the marginal product of capital. </p>
</li>
</ul>
<h3 id="A-Look-at-the-Transversality-Conditions"><a href="#A-Look-at-the-Transversality-Conditions" class="headerlink" title="A Look at the Transversality Conditions"></a>A Look at the Transversality Conditions</h3><p>$$<br>\lim_{t\to \infty}(F-y’F_{y’})=0\ \ \ \ \ \ [\mbox{Transversalilty Condition for the Infinite Horizon}]\tag{5.5}<br>$$</p>
<ul>
<li>When it is applied to two state variables, then we have </li>
</ul>
<p>$$<br>\lim_{t\to \infty}(F-L’F_{L’})=0\ \ \ \ \ \ \ \ \ and\ \ \ \ \ \ \ \         \lim_{t\to \infty}(F-K’F_{K’})=0<br>$$</p>
<ul>
<li><p>In view of the fact that $F_{L’} = 0$, the first of these conditions reduces to the condition that $F\to 0$ as $t\to\infty$. </p>
<ul>
<li>This would mean the net utility $U(C) - D(L)$ must tend to Bliss. </li>
<li>Note that, by itself, this condition still leaves the constant in (5.29)​ uncertain. </li>
<li>However, the other condition will fix the constant at zero because $F - K’F_{K’}$ is nothing but the left-hand-side expression in (5.29)​.</li>
</ul>
</li>
<li><p>The present problem implicitly specifies the terminal state at Bliss. Consequently, the transversality condition (5.7) is not needed.</p>
</li>
</ul>
<h3 id="The-Optimal-Investment-and-Capital-Paths"><a href="#The-Optimal-Investment-and-Capital-Paths" class="headerlink" title="The Optimal Investment and Capital Paths"></a>The Optimal Investment and Capital Paths</h3><p>Of greater interest to us is the optimal $K$ path and the related investment (and savings) path $K’$.</p>
<ul>
<li>Instead of deducing these from the previous results, let us find this information by taking advantage of the fact that the present problem also falls under the Special Case II: $F=F(y,y’)$, which means</li>
</ul>
<p>$$<br>F-K’F_{K’}=constant<br>$$</p>
<ul>
<li><p>or<br>$$<br>B-U(C)+D(L)-K’\mu=constant\ \ \ \ \ \ \ \ \mbox{for all } t\geq0\tag{5.29}<br>$$</p>
</li>
<li><p>This equation can be solved for $K’$ as soon as the (arbitrary) constant on the right-hand side can be assigned a specific value. </p>
<ul>
<li>To find this value, we note that this constant is to hold for all $t$, including $t\to\infty$.</li>
<li>We can thus make use of the fact that as $t\to\infty$, the economic objective of the model is to have $U(C) - D(L)$ tend to Bliss. </li>
</ul>
</li>
</ul>
<p>As $t\to\infty$, $B-U(C)+D(L)\to0$, $\mu\to0$,  then we have the arbitrary constant has to be $0$. </p>
<ul>
<li>Thus</li>
</ul>
<p>$$<br>    {K^*}’=\frac{B-U(C)+D(L)}{\mu}\tag{5.30}<br>$$</p>
<ul>
<li><p>With the time argument explicitly written out,<br>$$<br>{K^*}’(t)=\frac{B-U[C(t)]+D[L(t)]}{\mu(t)}\tag{5.30’}<br>$$</p>
</li>
<li><p>This result is known as the Ramsey rule. </p>
<ul>
<li>It stipulates that, optimally, the rate of capital accumulation must at any point of time be equal to the ratio of the shortfall of net utility from Bliss to the marginal utility of consumption. </li>
</ul>
</li>
</ul>
<p>From the Ramsey rule, it is possible to go one step further to find the $K^*(t)$ path by integrating (5.30’)​. </p>
<ul>
<li><p>For that, however, we need specific forms of $U(C)$ and $D(L)$ functions. </p>
</li>
<li><p>The general solution of (5.30’)​ will contain one arbitrary constant, which can be definitized by the initial condition $K(0) = K_0$. </p>
<ul>
<li>And that would complete the solution of the model. </li>
</ul>
</li>
</ul>
<h2 id="The-Concavity-Convexity-Sufficient-Condition-Again"><a href="#The-Concavity-Convexity-Sufficient-Condition-Again" class="headerlink" title="The Concavity / Convexity Sufficient Condition Again"></a>The Concavity / Convexity Sufficient Condition Again</h2><h3 id="Sufficient-Conditions"><a href="#Sufficient-Conditions" class="headerlink" title="Sufficient Conditions"></a>Sufficient Conditions</h3><p>​    </p>
<p>Recall that if the integrand function $F(t,y,y’)$ in a fixed-endpoint problem is concave / convex in the variables $(y, y’)$, then the Euler equation is sufficient for an absolute maximum / minimum of $V[y]$. </p>
<ul>
<li><p>Moreover, this sufficiency condition remains applicable when the terminal time is fixed but the terminal state is variable, provided that the supplementary condition<br>$$</p>
<pre><code>[F_&#123;y&#39;&#125;(y-y^*)]_&#123;t=&#123;T&#125;&#125;\leq 0</code></pre>
<p>$$</p>
<ul>
<li>is satisfied. </li>
</ul>
</li>
<li><p>For the infinite-horizon case, this supplementary becomes<br>$$</p>
<pre><code>\lim_&#123;t\to \infty&#125;[F_&#123;y&#39;&#125;(y-y^*)]\leq 0\tag&#123;5.44&#125;</code></pre>
<p>$$</p>
</li>
<li><p>In this condition, $F_{y’}$ is to be evaluated along the optimal path, and $(y-y^*)$ represents the deviation of any admissible neighboring path $y(t)$ from the optimal path $y^*(t)$.</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-03T14:22:22.000Z" title="1/3/2021, 10:22:22 PM">2021-01-03</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Constrained-Problems/">Constrained Problems</a></h1><div class="content"><h2 id="Four-Basic-Types-of-Constraints"><a href="#Four-Basic-Types-of-Constraints" class="headerlink" title="Four Basic Types of Constraints"></a>Four Basic Types of Constraints</h2><h3 id="Equality-Constraints"><a href="#Equality-Constraints" class="headerlink" title="Equality Constraints"></a>Equality Constraints</h3><p>Let the problem be that of maximizing<br>$$<br>V=\int^T_0 F(t,y_1, \dots, y_n, y_1’,\dots, y_n’)dt\tag{6.1}<br>$$</p>
<ul>
<li><p>subject to a set of $m$ independent but consistent constraints $(m&lt;n)$<br>$$</p>
<pre><code>    \begin&#123;array&#125;&#123;ll&#125;
    &#123;g&#125;^1(t, y_1,\cdots, y_n)=c_1\\
    \vdots\\
    g^m(t, y_1,\cdots, y_n)=c_m
    \end&#123;array&#125;\ \ \ \ \ \ \ \ \ (c_1,\cdots, c_m\mbox&#123; are constants&#125;)\tag&#123;6.2&#125;</code></pre>
<p>$$</p>
</li>
<li><p>and appropriate boundary conditions.</p>
</li>
</ul>
<p>Note that, in this problem, the number of constraints, $m$ , is required to be strictly less than the number of state variables, $n$. </p>
<ul>
<li>Otherwise, with $m=n$, the equation system (6.2)​ would already uniquely determine the $y_j(t)$ paths, and there would remain no degree of freedom for any optimization choice. </li>
<li>In view of this, this type of constrained dynamic optimization problem ought to contain at least two state variables, before a single constraint can meaningfully be accommodated.</li>
</ul>
<p>Form a Largrangian integrand function, $\mathscr{F}$, by augmenting the original integrand $F$ in (6.1)​<br>$$<br>\begin{aligned}<br>        \mathscr{F}&amp;=F+\lambda_1(t)(c_1-g^1)+\cdots+\lambda_m(t)(c_m-g^m)\<br>        &amp;=F+\sum^m_{i=1}\lambda_i(t)(c_i-g^i)<br>        \end{aligned}\tag{6.4}<br>$$</p>
<ul>
<li><p>Replace F by $\mathscr{F}$ in the objective functional<br>$$</p>
<pre><code>\mathscr&#123;V&#125;=\int^T_0\mathscr&#123;F&#125;dt\tag&#123;6.5&#125;</code></pre>
<p>$$</p>
</li>
<li><p>which we can maximize as if it is an unconstrained problem. </p>
</li>
<li><p>As long as all of the constraints in (6.2)​ are satisfied, so that $c_i-g^i=0$ for all $i$, then the value of $\mathscr{V}$ will be identical with that of $F$, and the free extremum of the functional $\mathscr{V}$ will be identical with the constrained extremum of the original functional $V$.</p>
</li>
</ul>
<p>Treat the Lagrange multipliers as additional state variables, each subject to an Euler equation, or the Euler-Lagrange equation.<br>$$<br>F_{y_j}-\frac{d}{dt}F_{y’_j}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \ (j=1,\dots,n)\ \ \ \ \ \ [cf.(2.27)]\tag{6.6}<br>$$</p>
<ul>
<li><p>Applied it to the Lagrange multipliers<br>$$<br>\mathscr{F}_{\lambda_i}-\frac{d}{dt}\mathscr{F}_{\lambda’_i}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \ (i=1,\dots,m)\tag{6.7}<br>$$</p>
</li>
<li><p>However, since $\mathscr{F}$ is independent of any $\lambda_i’$, we have $\mathscr{F}_{\lambda_i’}=0$ for every $i$<br>$$</p>
<pre><code>    \mathscr&#123;F&#125;_&#123;\lambda_i&#125;=0\ \ \ \ \ or\ \ \ \ \ \ c_i-g^i=0\ \ \ \ \mbox&#123;for all&#125;\ t\in[0,T]\tag&#123;6.7&#39;&#125;</code></pre>
<p>$$</p>
</li>
<li><p>So we have a total of $n$ Euler-Lagrange equations for the state variables, plus the $m$ constraints to determine the $n + m$ paths, $y_j(t)$ and $\lambda_i(t)$, with the arbitrary constants to be definitized by the boundary conditions.</p>
</li>
</ul>
<h3 id="Differential-Equation-Constraints"><a href="#Differential-Equation-Constraints" class="headerlink" title="Differential-Equation Constraints"></a>Differential-Equation Constraints</h3><p>Now suppose that the problem is to maximize (6.1)​, subject to a consistent set of $m$ independent constraints $(m &lt; n)$ that are differential equations:<br>$$<br>\begin{array}{ll}<br>        {g}^1(t, y_1,\cdots, y_n,y_1,\cdots, y_n’)=c_1\<br>        \vdots\<br>        {g}^m(t, y_1,\cdots,y_n,y_1’,\cdots, y_n’)=c_m<br>        \end{array}\ \ \ \ \ \ \ \ \ (c_1,\cdots, c_m\mbox{ are constants})\tag{6.8}<br>$$</p>
<ul>
<li><p>and appropriate boundary conditions.</p>
</li>
<li><p>The Lagrangian integrand function is still<br>$$</p>
<pre><code>        \mathscr&#123;F&#125;=F+\lambda_1(t)(c_1-g^1)+\cdots+\lambda_m(t)(c_m-g^m)</code></pre>
<p>$$</p>
</li>
<li><p>and Euler-Lagrange Equations with respect to the state  variables $y_j$ are still in the form of<br>$$</p>
<pre><code>F_&#123;y_j&#125;-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;_j&#125;=0\ \ \ \ \ \  \mbox&#123;for all&#125;\ t\in[0,T]\ \ \ \ \ \ \ (j=1,\dots,n)</code></pre>
<p>$$</p>
</li>
<li><p>Moreover, similar equations with respect to the Lagrange multipliers $\lambda_i$ are again a restatement of the given constraints. </p>
<ul>
<li>So we still have a total of $n$ Euler-Lagrange equations for the state variables, plus the $m$ constraints to determine the $n + m$ paths, $y_j(t)$ and $\lambda_i(t)$, with the arbitrary constants to be definitized by the boundary conditions.</li>
</ul>
</li>
</ul>
<h3 id="Inequality-Constraints"><a href="#Inequality-Constraints" class="headerlink" title="Inequality Constraints"></a>Inequality Constraints</h3><p>Generalized problem<br>$$<br>\begin{array}{lll}<br>            \mbox{Maximize or Minimize}&amp;\int^T_0 F(t,y_1,\dots,y_n,y_1’,\dots,y_n’(t))dt\<br>            \<br>            \mbox{subject to}&amp;<br>            {g}^1(t, y_1,\cdots, y_n,y_1,\cdots, y_n’)\leq c_1\<br>            &amp;\vdots\<br>            &amp;{g}^m(t, y_1,\cdots,y_n,y_1’,\cdots, y_n’)\leq c_m\<br>            \<br>            \mbox{and} &amp;\mbox{appropriate boundary conditions}<br>            \end{array}\tag{6.9}<br>$$</p>
<ul>
<li>Since inequality constraints are much less stringent then equality constraints, there is no need to stipulate that $m &lt; n$ .</li>
<li>Even if the number of constraints exceeds the number of state variables, the inequality constraints as a group will not uniquely determine the $Y_j$ paths, and hence will not eliminate all degrees of freedom from our choice problem. <ul>
<li>However, the inequality constraints do have to be consistent with one another, as well as with the other aspects of the problem.</li>
</ul>
</li>
</ul>
<p>The Lagrangian integrand function is still<br>$$<br>\mathscr{F}=F+\lambda_1(t)(c_1-g^1)+\cdots+\lambda_m(t)(c_m-g^m)<br>$$</p>
<ul>
<li>and Euler-Lagrange Equations with respect to the state  variables $y_j$ are still in the form of<br>$$<pre><code>    F_&#123;y_j&#125;-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;_j&#125;=0\ \ \ \ \ \  \mbox&#123;for all&#125;\ t\in[0,T]\ \ \ \ \ \ \ (j=1,\dots,n)</code></pre>
$$</li>
</ul>
<p>To ensure that all the $\lambda_i(t)(c_i - g^i)$ terms vanish in the solution (so that the optimized values of $\mathscr{F}$ and $F$ are equal), we need a complementary-slackness relationship between the $i$th multiplier and the $i$th constraint, for every $i$ (a set of $m$ equations):<br>$$<br>\lambda_i(t)(c_i - g^i)=0\ \ \ \ \ \ \ \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \ (j=1,\dots,m)\tag{6.10}<br>$$</p>
<ul>
<li>This complementary-slackness relationship guarantees that <ol>
<li>whenever the $i$th Lagrange multiplier is nonzero, the $i$th constraint will be satisfied as a strict equality, and </li>
<li>whenever the $i$th constraint is a strict inequality, the $i$th Lagrange multiplier will be zero.     <ul>
<li>It is this relationship that serves to maintain the identity between the optimal value of the original integrand $F$ and that of the modified integrand $\mathscr{F}$ in (6.4)​.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="Isoperimetric-Constraints"><a href="#Isoperimetric-Constraints" class="headerlink" title="Isoperimetric Constraints"></a>Isoperimetric Constraints</h3><p>Generalized problem<br>$$<br>\begin{array}{lll}<br>            \mbox{Maximize or Minimize}&amp;\int^T_0 F(t,y_1,\dots,y_n,y_1’,\dots,y_n’(t))dt\<br>            \<br>            \mbox{subject to}&amp;<br>            {g}^1(t, y_1,\cdots, y_n,y_1,\cdots, y_n’)=k_1\<br>            &amp;\vdots\<br>            &amp;{g}^m(t, y_1,\cdots,y_n,y_1’,\cdots, y_n’)=k_m\<br>            \<br>            \mbox{and} &amp;\mbox{appropriate boundary conditions}<br>            \end{array}\tag{6.12}<br>$$</p>
<ul>
<li>In the isoperimetric problem, there is again no need to require $m &lt; n$ , because even with $m\geq n$, freedom of optimizing choice is not ruled out.</li>
</ul>
<p>Let $m=n=1$, and define<br>$$<br>    \Gamma(t)=\int^t_0G(t,y,y’)dt\tag{6.13}<br>$$</p>
<ul>
<li><p>which is a function of $t$.</p>
</li>
<li><p>At $t=0$, $t=T$, we have</p>
</li>
</ul>
<p>$$<br>\Gamma(0)=\int^0_0Gdt\ \ \ \ \ \ and\ \  \ \ \ \ \Gamma(T)=\int^T_0Gdt=k\tag{6.14}<br>$$</p>
<ul>
<li><p>Take the derivative on (6.13)​ and get<br>$$</p>
<pre><code>G(t,y,y&#39;)-\Gamma&#39;(t)=0\tag&#123;6.15&#125;</code></pre>
<p>$$</p>
<ul>
<li>which conforms to the general structure of the differential constraint $g(t,y,y’)=c$ with $g=G-\Gamma’$ and $c=0$</li>
</ul>
</li>
</ul>
<p>Take the Lagrangian integrand<br>$$<br>\begin{aligned}<br>        \acute{F}&amp;=F(t,y,y’)+\lambda(t)[0-G(t,y,y’)+\Gamma’(t)]\<br>        &amp;=F(t,y,y’)-\lambda(t)G(t,y,y’)+\lambda(t)\Gamma’(t)<br>    \end{aligned}\tag{6.16}<br>$$</p>
<ul>
<li><p>And there are two state variables $y$ and $\Gamma$, so we have two parallel conditions<br>$$</p>
<pre><code>\acute&#123;F&#125;_&#123;y&#125;-\frac&#123;d&#125;&#123;dt&#125;\acute&#123;F&#125;_&#123;y&#39;&#125;=0\ \ \ \ \ \  \mbox&#123;for all&#125;\ t\in[0,T]\tag&#123;6.17&#125;</code></pre>
<p>$$</p>
<p>$$</p>
<pre><code>\acute&#123;F&#125;_&#123;\Gamma&#125;-\frac&#123;d&#125;&#123;dt&#125;\acute&#123;F&#125;_&#123;\Gamma&#39;&#125;=0\ \ \ \ \ \  \mbox&#123;for all&#125;\ t\in[0,T]\tag&#123;6.18&#125;</code></pre>
<p>$$</p>
<ul>
<li><p>However, inasmuch as $\acute{F}$ is independent of $\Gamma$, and since $\acute{F}_{\Gamma’} = \lambda(t)$, we see that (6.18)​ reduces to the condition<br>$$<br>-\frac{d}{dt}\lambda(t)=0\ \ \ \ \ \ \Longrightarrow\ \ \ \ \ \ \ \lambda(t)=\mbox{constant}\tag{6.18’}<br>$$</p>
</li>
<li><p>Then write the Lagrange multiplier of the isoperimetric problem simply as $\lambda$.</p>
</li>
</ul>
</li>
<li><p>Turning next to (6.17)​ and using ​(6.16)​, obtain a more specific version of Euler-Lagrange equation:<br>$$</p>
<pre><code>(F_y-\lambda G_y)-\frac&#123;d&#125;&#123;dt&#125;(F_&#123;y&#39;&#125;-\lambda G_&#123;y&#39;&#125;)=0\tag&#123;6.19&#125;</code></pre>
<p>$$</p>
</li>
<li><p>However, the same condition could have been obtained without $\Gamma’(t)$ from a modified (abridged) version of $\acute{F}$<br>$$<br>\mathscr{F}=F(t,y,y’)-\lambda G(t,y,y’)\ \ \ \ \ (\lambda=\mbox{constant})\tag{6.20}<br>$$</p>
</li>
<li><p>Thus, in the present one-state-variable problem with a single integral constraint, we can as a practical procedure use the modified Lagrange integrand $\mathscr{F}$ in (6.20) innstead of $\acute{F}$ , and apply the Euler-Lagrange equation to $y$ alone, knowing that the Euler-Lagrange equation for $\lambda$ will merely tell us that $\lambda$ is a constant, the value of which can be determined from the isoperimetric constraint.</p>
</li>
<li><p>Generalize it to the $n$-state-variable, $m$-integral-constraint case<br>$$<br>\mathscr{F}=F-(\lambda_1G^1+\cdots+\lambda_mG^m)\ \ \ \ \  \ \ \ \ (\lambda_i\ \mbox{ are all constants} )\tag{6.21}<br>$$</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-03T14:19:22.000Z" title="1/3/2021, 10:19:22 PM">2021-01-03</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Second-Order-Conditions/">Second-Order Conditions</a></h1><div class="content"><h2 id="Second-Order-Conditions"><a href="#Second-Order-Conditions" class="headerlink" title="Second-Order Conditions"></a>Second-Order Conditions</h2><h3 id="Second-Order-Necessary-Conditions"><a href="#Second-Order-Necessary-Conditions" class="headerlink" title="Second-Order Necessary Conditions"></a>Second-Order Necessary Conditions</h3><p>$$<br>\frac{d^2 V}{d\epsilon ^2}\leq 0\ \ \ \ \ \ [\mbox{for maximization of  } V]<br>$$</p>
<p>$$<br>\frac{d^2 V}{d\epsilon ^2}\geq 0\ \ \ \ \ \ [\mbox{for minimization of } V]<br>$$</p>
<h3 id="Second-Order-Sufficient-Conditions"><a href="#Second-Order-Sufficient-Conditions" class="headerlink" title="Second-Order Sufficient Conditions"></a>Second-Order Sufficient Conditions</h3><p>$$<br>\frac{d^2 V}{d\epsilon ^2}&lt; 0\ \ \ \ \ \ [\mbox{for maximization of } V]<br>$$</p>
<p>$$<br>\frac{d^2 V}{d\epsilon ^2}&gt; 0\ \ \ \ \ \ [\mbox{for minimization of } V]<br>$$</p>
<h3 id="The-Second-Derivative-of-V"><a href="#The-Second-Derivative-of-V" class="headerlink" title="The Second Derivative of $V$"></a>The Second Derivative of $V$</h3><p>Recalling the first derivative of $V$<br>$$<br>\begin{aligned}<br>        \frac{dV}{d\epsilon}&amp;=\int^T_0\frac{\partial F}{\partial \epsilon}dt=\int^T_0\left(\frac{\partial F}{\partial y}\frac{dy}{d\epsilon}+\frac{\partial F}{\partial y’}\frac{dy’}{d\epsilon}\right)dt\<br>        &amp;=\int^T_0[F_yp(t)+F_{y’}p’(t)]dt<br>        \end{aligned}\tag{2.13}<br>$$</p>
<ul>
<li><p>Since all the partial derivatives of $F(t, y, y’)$ are, like $F$ itself, functions of $t$, $y$, and $y’$</p>
</li>
<li><p>And recalling<br>$$<br>\begin{matrix}</p>
<pre><code>    y(t)=y^&#123;*&#125;(t)+\epsilon p(t)
    &amp; \ &amp;
    y&#39;(t)=&#123;y^*&#125;&#39;(t)+\epsilon p&#39;(t)
    \end&#123;matrix&#125;\tag&#123;2.3&#125;</code></pre>
<p>$$</p>
<ul>
<li>So $y$ and $y’$ are, in turn, both functions of $\epsilon$, with derivatives<br>$$<pre><code>    \frac&#123;dy&#125;&#123;d\epsilon&#125;=p(t)\ \ \ \ and\ \ \ \ \ \ \frac&#123;dy&#39;&#125;&#123;d\epsilon&#125;=p&#39;(t)\tag&#123;4.1&#125;</code></pre>
$$</li>
</ul>
</li>
<li><p>Thus we have<br>$$</p>
<pre><code>\begin&#123;aligned&#125;
    \frac&#123;d^2V&#125;&#123;d\epsilon^2&#125;&amp;=\frac&#123;d&#125;&#123;d\epsilon&#125;\left(\frac&#123;dV&#125;&#123;d\epsilon&#125;\right)=\frac&#123;d&#125;&#123;d\epsilon&#125;\int^T_0[F_yp(t)]+F_&#123;y&#39;&#125;p&#39;(t)]dt
    \\&amp;=\int^T_0\left[p(t)\frac&#123;d&#125;&#123;d\epsilon&#125;F_y+p&#39;(t)\frac&#123;d&#125;&#123;d\epsilon&#125;F_&#123;y&#39;&#125;\right]dt
\end&#123;aligned&#125;\tag&#123;4.2&#125;</code></pre>
<p>$$</p>
<ul>
<li>In view of fact that</li>
</ul>
<p>$$<br>\frac{d}{d\epsilon}F_y=F_{yy}\frac{dy}{d\epsilon}+F_{yy’}\frac{dy’}{d\epsilon}=F_{yy}p(t)+F_{y’y}p’(t)<br>$$</p>
<ul>
<li><p>and similarly<br>$$</p>
<pre><code>\frac&#123;d&#125;&#123;d\epsilon&#125;F_&#123;y&#39;&#125;=F_&#123;yy&#39;&#125;p(t)+F_&#123;y&#39;y&#39;&#125;p&#39;(t)</code></pre>
<p>$$</p>
<ul>
<li>The second derivative (4.2)​ can be simplified as<br>$$<pre><code>    \frac&#123;d^2V&#125;&#123;d\epsilon^2&#125;=\int^T_0\left[F_&#123;yy&#125;p^2(t)+2F_&#123;yy&#39;&#125;p(t)p&#39;(t)+F_&#123;y&#39;y&#39;&#125;&#123;p&#39;&#125;^2(t)\right]dt\tag&#123;4.2&#39;&#125;</code></pre>
$$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="The-Quadratic-Form-Test"><a href="#The-Quadratic-Form-Test" class="headerlink" title="The Quadratic-Form Test"></a>The Quadratic-Form Test</h3><p>The second derivative in (4.2’)​ is a definite integral with a quadratic form as its integrand.</p>
<ul>
<li>Since $t$ spans the interval $[0, T ]$, we have, of course, not one, but an infinite number of quadratic forms in the integral.</li>
<li>Nevertheless, if it can be established that the quadratic form-with $F_{yy’}$, $F_{yy’}$, and $F_{y’y’}$ evaluated on the extremal-is negative definite for every $t$, then $\frac{d^2V}{d\epsilon^2} &lt; 0$, and the extremal maximizes $V$. </li>
<li>Similarly, positive definiteness of the quadratic form for every $t$ is sufficient for minimization of $V$.</li>
<li>Even if we can only establish sign semidefiniteness, we can at least have the second-order necessary conditions checked.</li>
</ul>
<p>For some reason, however, the quadratic-form test was totally ignored in the historical development of the classical calculus of variations.</p>
<ul>
<li>In a more recent development, however, the concavity / convexity of the integrand function $F$ is used in a sufficient condition.</li>
<li>While concavity / convexity does not per se require differentiability, it is true that if the $F$ function does possess continuous second derivatives, then its concavity / convexity can be checked by means of the sign semidefiniteness of the second-order total differential of $F$. </li>
<li>So the quadratic-form test definitely has a role to play in the calculus of variations.</li>
</ul>
<h2 id="The-Concavity-Convexity-Sufficient-Condition"><a href="#The-Concavity-Convexity-Sufficient-Condition" class="headerlink" title="The Concavity / Convexity Sufficient Condition"></a>The Concavity / Convexity Sufficient Condition</h2><h3 id="A-Sufficiency-Theorem-for-Fixed-Endpoint-Problems"><a href="#A-Sufficiency-Theorem-for-Fixed-Endpoint-Problems" class="headerlink" title="A Sufficiency Theorem for Fixed-Endpoint Problems"></a>A Sufficiency Theorem for Fixed-Endpoint Problems</h3><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[y]=\int^T_0 F[t,y(t),y’(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=Z\ &amp;(T,Z\ \mbox{given})<br>\end{array}\tag{2.1}<br>$$</p>
<p>Just as a concave / convex objective function in a static optimization prob­ lem is sufficient to identify an extremum as an absolute maximum / mini­mum, a similar sufficiency theorem holds in the calculus of variations:<br>$$<br>\begin{aligned}<br>            &amp;\mbox{For the fixed-endpoint problem } (2.1), \mbox{if the integrand function}\ F(t,y,y’)\ \mbox{is concave} \&amp;\mbox{in the variables}\ (y,y’), \mbox{then the Euler equation is sufficient for an absolute}\&amp;\mbox{maximum of}\ V[y].\mbox{ Similarly, if}\ F(t, y, y’)\ \mbox{is convex in}\ (y, y’), \mbox{then the Euler}\&amp; \mbox{equation is sufficient for an absolute minimum of}\ V[y].<br>        \end{aligned}\tag{4.3}<br>$$</p>
<ul>
<li>It should be pointed out that concavity / convexity in $(y,y’)$ means concavity / convexity in the two variables $y$ and $y’$ jointly, not in each variable separately.</li>
</ul>
<p>The proof of this theorem for the concave case.<br>$$<br>\begin{aligned}<br>            F(t,y,y’)-F(t,y^*,{y^*}’)&amp;\leq F_y(t,y^*,{y^*}’) (y-y^*)+F_{y’}(t,y^*,{y^*}’)(y’-{y^*}’)\<br>            &amp;= F_y(t,y^*,{y^*}’)\epsilon p(t)+F_{y’}(t,y^*,{y^*})’\epsilon p’(t)<br>        \end{aligned}\tag{4.4}<br>$$</p>
<ul>
<li><p>Recalling that<br>$$</p>
<pre><code>\begin&#123;aligned&#125;
\int^T_0F_&#123;y&#39;&#125;p&#39;(t)dt&amp;=[F_&#123;y&#39;&#125;p(t)]^T_0-\int^T_0 p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt\\
&amp;=-\int^T_0p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt
\end&#123;aligned&#125;\tag&#123;2.16&#125;</code></pre>
<p>$$</p>
</li>
<li><p>And the Euler Equation<br>$$</p>
<pre><code>    F_y-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;=0\ \ \ \ \ \  \mbox&#123;for all&#125;\ t\in[0,T]</code></pre>
<p>$$</p>
</li>
<li><p>Integrate both sides of (4.4) with respect to $t$ over the interval $[0,T]$<br>$$</p>
<pre><code>    \begin&#123;aligned&#125;
    V[y]-V[y^*]&amp;\leq \epsilon\int^T_0\left[F_y(t,y^*,&#123;y^*&#125;&#39;)p(t)+F_&#123;y&#39;&#125;(t,y^*,&#123;y^*&#125;&#39;)p&#39;(t)\right]dt\\
    &amp;=\epsilon \int^T_0 p(t)\left[F_y(t,y^*,&#123;y^*&#125;&#39;)-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;(t,y^*,&#123;y^*&#125;&#39;)\right]dt\\
    &amp;=0 
    \end&#123;aligned&#125;\tag&#123;4.5&#125;</code></pre>
<p>$$</p>
<ul>
<li>The first equation follows from (2.16)​, the last equation follows from the fact that $y^*(t)$ satisfies the Euler Equation ​(2.18)​</li>
<li>Note that if the $F$ function is strictly concave in $(y, y’)$, then the weak inequality $\leq$ in (4.4)​ and (4.5)​ will become the strict inequality $&lt;$. <ul>
<li>The result, $V[y] &lt; V[y^*]$, will then establish $V[y^*]$ to be a unique absolute maximum of $V$. </li>
</ul>
</li>
</ul>
</li>
<li><p>By the same token, a strictly convex $F$ will make $V[y^*]$ a unique absolute minimum.    </p>
</li>
</ul>
<h3 id="Generalization-to-Variable-Terminal-Point"><a href="#Generalization-to-Variable-Terminal-Point" class="headerlink" title="Generalization to Variable Terminal Point"></a>Generalization to Variable Terminal Point</h3><p>Recalling<br>$$<br>\begin{aligned}<br>        \int^{T}<em>0F</em>{y’}p’(t)dt&amp;=[F_{y’}p(t)]^{T}<em>0-\int^{T}<em>0 p(t)\frac{d}{dt}F</em>{y’}dt\<br>        &amp;=[F</em>{y’}]_{t={T}} p(T)-\int^{T}<em>0p(t)\frac{d}{dt}F</em>{y’}dt<br>        \end{aligned}\tag{2.16’}<br>$$</p>
<ul>
<li><p>Thus the Equation (4.5)​ becomes<br>$$<br>\begin{aligned}</p>
<pre><code>    V[y]-V[y^*]&amp;\leq \epsilon\int^T_0\left[F_y(t,y^*,&#123;y^*&#125;&#39;)p(t)+F_&#123;y&#39;&#125;(t,y^*,&#123;y^*&#125;&#39;)p&#39;(t)\right]dt\\
    &amp;=\epsilon \int^T_0 p(t)\left[F_y(t,y^*,&#123;y^*&#125;&#39;)-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;(t,y^*,&#123;y^*&#125;&#39;)\right]dt+\epsilon[F_&#123;y&#39;&#125;p(t)]_&#123;t=&#123;T&#125;&#125;\\
    &amp;=\epsilon[F_&#123;y&#39;&#125;p(t)]_&#123;t=&#123;T&#125;&#125;
    \\
    &amp;=[F_&#123;y&#39;&#125;(y-y^*)]_&#123;t=&#123;T&#125;&#125;
\end&#123;aligned&#125;\tag&#123;4.5&#39;&#125;</code></pre>
<p>$$</p>
</li>
<li><p>To ensure the maximum, $F(t, y, y’)$ in (4.3)​ only needs to be supplemented in the present case by a nonpositivity condition on the expression $[F_{y’}(y-y^*)]_{t={T}}$.</p>
<ul>
<li>But this supplementary condition is automatically met when the transversality condition is satisfied for the vertical-terminal-line problem, namely, $[F_{y’}]=0$. </li>
<li>As for the truncated case, the transversality condition calls for either  $[F_{y’}]=0$ (when the minimum acceptable terminal value is nobinding), or $y^*=y_{\min}$ (when that terminal value is binding), thereby in effect turning the problem into one with a fixed terminal point).</li>
<li>Either way, the supplementary condition is met. </li>
<li>Thus, if the integrand function $F$ is concave / convex in the variables $(y,y’)$ in a problem with a vertical terminal lime or truncated vertical terminal line, then the Euler equation plus the transversality condition are sufficient for an absolute maximum / minimum of $V[y]$.    </li>
</ul>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-03T14:17:22.000Z" title="1/3/2021, 10:17:22 PM">2021-01-03</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Transversality-Conditions-for-Variable-Endpoint-Problems/">Transversality Conditions for Variable-Endpoint Problems</a></h1><div class="content"><h2 id="The-General-Transversality-Condition"><a href="#The-General-Transversality-Condition" class="headerlink" title="The General Transversality Condition"></a>The General Transversality Condition</h2><h3 id="The-Variable-Terminal-Point-Problem"><a href="#The-Variable-Terminal-Point-Problem" class="headerlink" title="The Variable-Terminal-Point Problem"></a>The Variable-Terminal-Point Problem</h3><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[y]=\int^T_0 F[t,y(t),y’(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=y_T\ &amp;(T,y_T\ \mbox{free})<br>\end{array}\tag{3.1}<br>$$</p>
<ul>
<li><p>Using the variable $\epsilon$ to express any value of $T$ in the neighborhood of $T^*$.<br>$$</p>
<pre><code>T=T^*+\epsilon\Delta T\tag&#123;3.2&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Consider $T$ as a function of $\epsilon$, with derivative:<br>$$</p>
<pre><code>\frac&#123;dT&#125;&#123;d\epsilon&#125;=\Delta T\tag&#123;3.3&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Generating neighboring paths of the extremal $y^*(t)$ with the same $\epsilon$<br>$$<br>\begin{matrix}</p>
<pre><code>    y(t)=y^&#123;*&#125;(t)+\epsilon p(t)
    &amp; \mbox&#123; [implying&#125;&amp;
    y&#39;(t)=&#123;y^*&#125;&#39;(t)+\epsilon p&#39;(t)]
    \end&#123;matrix&#125;\tag&#123;3.4&#125;</code></pre>
<p>$$</p>
<ul>
<li>However, although the $p(t)$ curve must still satisfy the condition $p(0) = 0$, to force the neighboring paths to pass through the fixed·initial point, the other condition-$p(T)= 0$-should now be dropped, because $Y_T$ is free.</li>
</ul>
</li>
<li><p>Consider $V$ as a function of the variable $\epsilon$, the upper limit of integration in the $V$ function will also vary with $\epsilon$<br>$$<br>V(\epsilon)=\int^{T(\epsilon)}_0 F[y, y^{<em>}(t)+\epsilon p(t), {y^</em>}’(t)+\epsilon p’(t)]dt\tag{3.5}<br>$$</p>
</li>
</ul>
<h3 id="Deriving-the-General-Transversality-Condition"><a href="#Deriving-the-General-Transversality-Condition" class="headerlink" title="Deriving the General Transversality Condition"></a>Deriving the General Transversality Condition</h3><p>The <u>necessary condition</u>:<br>$$<br>\frac{dV}{d\epsilon}=\int^{T(\epsilon)}_0\frac{\partial F}{\partial \epsilon}dt+F[T,y(T),y’(T)]\frac{dT}{d\epsilon}=0\tag{3.6}<br>$$</p>
<ul>
<li><p>Recalling previous equations<br>$$</p>
<pre><code>\begin&#123;aligned&#125;
\int^&#123;T&#125;_0\frac&#123;\partial F&#125;&#123;\partial \epsilon&#125;dt&amp;=\int^&#123;T&#125;_0\left(\frac&#123;\partial F&#125;&#123;\partial y&#125;\frac&#123;dy&#125;&#123;d\epsilon&#125;+\frac&#123;\partial F&#125;&#123;\partial y&#39;&#125;\frac&#123;dy&#39;&#125;&#123;d\epsilon&#125;\right)dt\\
&amp;=\int^&#123;T&#125;_0[F_yp(t)+F_&#123;y&#39;&#125;p&#39;(t)]dt
\end&#123;aligned&#125;\tag&#123;2.13&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Breaking the integral into two parts<br>$$<br>\int^{T}<em>0 F_yp(t)dt+\int^{T}_0F</em>{y’}p’(t)dt=0\tag{2.14}<br>$$</p>
</li>
<li><p>Integration by parts<br>$$</p>
<pre><code>    \begin&#123;aligned&#125;
        \int^&#123;T&#125;_0F_&#123;y&#39;&#125;p&#39;(t)dt&amp;=[F_&#123;y&#39;&#125;p(t)]^&#123;T&#125;_0-\int^&#123;T&#125;_0 p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt\\
            &amp;=[F_&#123;y&#39;&#125;]_&#123;t=&#123;T&#125;&#125;p(T)-\int^&#123;T&#125;_0p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt
    \end&#123;aligned&#125;\tag&#123;2.16&#39;&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Thus we have<br>$$<br>\mbox{First term in } (3.6)=\int^{T}<em>0p(t)\left[F_y-\frac{d}{dt}F_{y’}\right]dt+[F</em>{y’}]_{t={T}}p(T)<br>$$</p>
</li>
<li><p>According to (3.3) we also have<br>$$<br>\mbox{Second term in } (3.6)=[F]_{t=T}\Delta T<br>$$</p>
</li>
<li><p>Then we can transform (3.6)​ into<br>$$</p>
<pre><code>\int^&#123;T&#125;_0p(t)\left[F_y-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;\right]dt+[F_&#123;y&#39;&#125;]_&#123;t=&#123;T&#125;&#125;p(T)+[F]_&#123;t=T&#125;\Delta T =0\tag&#123;3.7&#125;</code></pre>
<p>$$</p>
</li>
</ul>
<p>Of the three terms on the left-hand side of $(3. 7)$, each contains its own independent arbitrary element: </p>
<ul>
<li>$p(t)$ (the entire perturbing curve) in the first term</li>
<li>$p(T)$ (the terminal value on the perturbing curve) in the second term</li>
<li>$\Delta T$ (the arbitrarily chosen change in $T$) in the third term<ul>
<li>Thus we cannot presume any offsetting or cancellation of terms. </li>
<li>Consequently, in order to satisfy the condition (3. 7)​, each of the three terms must individually be set equal to zero.</li>
</ul>
</li>
</ul>
<p>When the first term in (3.7)​ is set equal to zero, the Euler equation emerges.<br>$$<br>F_y-\frac{d}{dt}F_{y’}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.18}<br>$$</p>
<ul>
<li>This establish the fact that the Euler equation remains valid as a necessary condition in the variable-endpoint problem.</li>
</ul>
<p>Getting rid of the arbitrary quantity $p(T)$ by transforming it into terms of $\Delta T$ and $\Delta y_T$<br>$$<br>\Delta y(T)=p(T)+y’(T)\Delta T<br>$$</p>
<ul>
<li>This can be done with the help of Fig. 3.1. </li>
</ul>
<img src="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Transversality-Conditions-for-Variable-Endpoint-Problems/Fig_3.1.png" class title="[title]">

<ul>
<li><p>Rearranging it and get<br>$$</p>
<pre><code>p(T)=\Delta y_T-y&#39;(T)\Delta T\tag&#123;3.8&#125;</code></pre>
<p>$$</p>
<ul>
<li>The result in (3.8)​ is build on condition that $\epsilon=1$ while deriving, but it is still valid if $\epsilon$ is not set equal to one.</li>
</ul>
</li>
<li><p>Eliminating $p(T)$ in (3.7)​, and dropping the first term in ​(3.7)​, we get another <u>necessary condition</u><br>$$</p>
<pre><code>    \begin&#123;aligned&#125;
        &#123;[F_&#123;y&#39;&#125;]&#125;_&#123;t=&#123;T&#125;&#125;p(T)+[F]_&#123;t=T&#125;\Delta T&amp;=[F_&#123;y&#39;&#125;]_&#123;t=&#123;T&#125;&#125;[\Delta y_T-y&#39;(T)\Delta T]+[F]_&#123;t=T&#125;\Delta T 
        \\
        &amp;=[F_&#123;y&#39;&#125;]_&#123;t=T&#125;\Delta y_&#123;T&#125;+[F-y&#39;F_&#123;y&#39;&#125;]_&#123;t=T&#125;\Delta T
    \end&#123;aligned&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Rearranging it and get<br>$$<br>[F-y’F_{y’}]<em>{t=T}\Delta T+[F</em>{y’}]<em>{t=T}\Delta y</em>{T}=0\ \ \ \ \ \ \mbox{  [General Transversality Condition]}\tag{3.9}<br>$$</p>
<ul>
<li>This condition, unlike the Euler equation, is relevant only to one point of time, $T$.</li>
<li>Its role is to take the place of the missing terminal condition in the present problem. </li>
</ul>
</li>
</ul>
<h2 id="Specialized-Transversality-Conditions"><a href="#Specialized-Transversality-Conditions" class="headerlink" title="Specialized Transversality Conditions"></a>Specialized Transversality Conditions</h2><p>Depending on the exact specification of the terminal line or curve, however, the general condition (3.9) can be written in various specialized forms.</p>
<h3 id="Vertical-Terminal-Line"><a href="#Vertical-Terminal-Line" class="headerlink" title="Vertical Terminal Line"></a>Vertical Terminal Line</h3><p>For a fixed $T$, $\Delta T=0$, so we only need<br>$$<br>[F_{y’}]_{t=T}=0\ \ \ \ \ \ \mbox{  [Transversality Condition for Vertical Terminal Line]}\tag{3.10}<br>$$</p>
<h3 id="Horizontal-Terminal-Line"><a href="#Horizontal-Terminal-Line" class="headerlink" title="Horizontal Terminal Line"></a>Horizontal Terminal Line</h3><p>For a fixed $y_T$, $\Delta y_T=0$, so we only need<br>$$<br>[F-y’F_{y’}]_{t=T}=0\ \ \ \ \ \ \mbox{  [Transversality Condition for Hoizontal Terminal Line]}\tag{3.11}<br>$$</p>
<h3 id="Terminal-Curve"><a href="#Terminal-Curve" class="headerlink" title="Terminal Curve"></a>Terminal Curve</h3><p>With a terminal curve $y_T=\phi(T)$, we also have $\Delta y_T=\phi’\Delta T$. </p>
<ul>
<li><p>Transform (3.9)​, we have<br>$$<br>[F-y’F_{y’}+F_{y’}\phi’]_{t=T}\Delta T=0<br>$$</p>
</li>
<li><p>For an arbitrary $\Delta T$, we have</p>
</li>
</ul>
<p>$$<br>[F-y’F_{y’}+F_{y’}\phi’]_{t=T}=0\ \ \ \ \ \ \mbox{  [Transversality Condition for Terminal Curve]}\tag{3.12}<br>$$</p>
<h3 id="Truncated-Vertical-Terminal-Line"><a href="#Truncated-Vertical-Terminal-Line" class="headerlink" title="Truncated Vertical Terminal Line"></a>Truncated Vertical Terminal Line</h3><p>The usual case ofvertical terminal line, with $\Delta T =0$, specializes (3.9) to<br>$$<br>[F_{y’}]<em>{t=T}\Delta</em>{y_T}=0<br>$$</p>
<ul>
<li>When the line is truncated-restricted by the terminal condition $y_T&gt;y_{\min}$ where $y_{\min}$ is a minimum permissible level of $y$<ul>
<li>the optimal solution can have two possible types of outcome: $y_T^*&gt;y_{\min}$ or  $y_T^*=y_{\min}$ </li>
</ul>
</li>
</ul>
<p>For $y_T^*&gt;y_{\min}$, the constraint is not binding, we have<br>$$<br>[F_{y’}]<em>{t=T}=0\ \ \ for\ \ \ y_T^*&gt;y_{\min}\tag{3.14}<br>$$<br>For $y_T^*=y_{min}$, the constraint is binding, with Kuhn-Tucker condition, we have<br>$$<br>[F</em>{y’}]<em>{t=T}\leq0\ \ \ for\ \ \ y_T^*=y_{min}\tag{3.16}<br>$$<br>Combing (3.14)​ and (3.16)​ we have<br>$$<br>[F</em>{y’}]<em>{t=T}\leq0\ \ \ for\ \ \ y_T^<em>\geq y_{min}\ \ \ \ \ \ \ (y_T^</em>-y_{min})[F</em>{y’}]_{t=T}=0\   \ \ \mbox{ [Transversality Condition for Truncated Vertical Terminal Line in the Maximization Problem]}\tag{3.17}<br>$$</p>
<p>$$<br>[F_{y’}]<em>{t=T}\geq0\ \ \ for\ \ \ y_T^<em>\geq y_{min}\ \ \ \ \ \ \ (y_T^</em>-y_{min})[F</em>{y’}]_{t=T}=0\   \ \ \mbox{[Transversality Condition for Truncated Vertical Terminal Line in the Minimization Problem]}\tag{3.18}<br>$$</p>
<h2 id="Three-Generalizations"><a href="#Three-Generalizations" class="headerlink" title="Three Generalizations"></a>Three Generalizations</h2><h3 id="The-Case-of-Several-State-Variables"><a href="#The-Case-of-Several-State-Variables" class="headerlink" title="The Case of Several State Variables"></a>The Case of Several State Variables</h3><p>$$<br>\left[F-(y_1’F_{y_1’}+\cdots+y_n’F_{y_n’})\right]<em>{t=T}\Delta T+\left[F</em>{y_1’}\right]<em>{t=T}\Delta</em>{y_{1T}}+\cdots+\left[F_{y_n’}\right]<em>{t=T}\Delta y</em>{nT}=0\   \ \ \mbox{[The General (Terminal) Transversality Condition]}\tag{3.27}<br>$$</p>
<ul>
<li>When $n=2$<br>$$<br>\left[F-(y’F_{y’}+z’F_{z’})\right]<em>{t=T}\Delta T+\left[F</em>{y’}\right]<em>{t=T}\Delta</em>{y_{T}}+\left[F_{z’}\right]<em>{t=T}\Delta z</em>{T}=0\tag{3.27’}<br>$$</li>
</ul>
<h3 id="The-Case-of-Higher-Derivatives"><a href="#The-Case-of-Higher-Derivatives" class="headerlink" title="The Case of Higher Derivatives"></a>The Case of Higher Derivatives</h3><p>The general transversality condition for the case of $F(t,y’,y’’,y’’’)$ is<br>$$<br>\begin{aligned}<br>            &amp;\left[F-y’F_{y’}-y’’F_{y’’}+y’\frac{d}{dt}F_{y’’}\right]<em>{t=T}\Delta T\&amp;+\left[F</em>{y’}-\frac{d}{dt}F_{y’’}\right]<em>{t=T}\Delta y_T+[F_{y’’}]</em>{t=T}\Delta y’_{T}=0<br>        \end{aligned}\tag{3.28}<br>$$</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-03T14:17:21.000Z" title="1/3/2021, 10:17:21 PM">2021-01-03</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/The-Fundamental-Problem-of-the-Calculus-of-Variations/">The Fundamental Problem of the Calculus of Variations</a></h1><div class="content"><h2 id="The-Fundamental-Problem"><a href="#The-Fundamental-Problem" class="headerlink" title="The Fundamental Problem"></a>The Fundamental Problem</h2><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[y]=\int^T_0 F[t,y(t),y’(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=Z\ &amp;(T,Z\ \mbox{given})<br>\end{array}\tag{2.1}<br>$$</p>
<ul>
<li>The maximization and minimization problems differ from each other in the second-order conditions, but they share the same first-order conditions.</li>
</ul>
<p>The task of variational calculus is to select from a set of admissible $y$ paths (or trajectories) the one that yields an extreme value of $V[y]$.</p>
<ul>
<li><p>Since the calculus of variations is based on the classical methods of calculus, requiring the use of first and second derivatives, we shall restrict the set of admissible paths to those continuous curves with continuous derivatives.</p>
</li>
<li><p>A smooth $y$ path that yields an extremum of $V[y]$ is called an <strong>extremal</strong>.</p>
</li>
<li><p>We shall also assume that the integrand function $F$ is twice differentiable.</p>
</li>
</ul>
<h2 id="The-Euler-Equation"><a href="#The-Euler-Equation" class="headerlink" title="The Euler Equation"></a>The Euler Equation</h2><h3 id="Differentiating-a-Definite-Integral"><a href="#Differentiating-a-Definite-Integral" class="headerlink" title="Differentiating a Definite Integral"></a>Differentiating a Definite Integral</h3><p> Considering the definite integral<br>$$<br>I(x)\equiv \int^b_aF(t,x)dt<br>$$</p>
<ul>
<li>We have the Leibnniz’s Rule</li>
</ul>
<p>$$<br>\frac{dI}{dx}=\int^b_aF_x(t,x)dt<br>$$</p>
<p>Considering another definite integral<br>$$<br>K(x)\equiv\int^{b(x)}_{a(x)}F(t,x)dt<br>$$</p>
<ul>
<li>We have </li>
</ul>
<p>$$<br>\frac{dK}{dx}=\int^{b(x)}_{a(x)}F_x(t,x)dt+F[b(x),x]b’(x)-F[a(x),x]a’(x)<br>$$</p>
<h3 id="Development-of-the-Euler-Equation"><a href="#Development-of-the-Euler-Equation" class="headerlink" title="Development of the Euler Equation"></a>Development of the Euler Equation</h3><p>With reference to Fig. 2.1, let the solid path $y^*(t)$ be a known extremal. </p>
<ul>
<li>We seek to find some property of the extremal that is absent in the (nonextremal) neighboring paths. </li>
<li>Such a property would constitute a necessary condition for an extremal. </li>
<li>To do this, we need for comparison purposes a family of neighboring paths which, by specification in (2.1), must pass through the given endpoints $(0, A)$ and $(T, Z)$. </li>
<li>A simple way of generating such neighboring paths is by using a <strong>perturbing curve</strong>, chosen arbitrarily except for the restrictions that it be smooth and pass through the points $0$ and $T$ on the horizontal axis in Fig. 2.1.</li>
</ul>
<img src="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/The-Fundamental-Problem-of-the-Calculus-of-Variations/Fig_2.1.png" class title="[title]">



<p>Consider a perturbing curve:<br>$$<br>p(0)=p(T)=0\tag{2.2}<br>$$<br>Using the perturbing curve, we can generate neighboring paths of the extremal $y^*(t)$:<br>$$<br>\begin{matrix}<br>        y(t)=y^{<em>}(t)+\epsilon p(t)<br>        &amp; \mbox{ [implying}&amp;<br>        y’(t)={y^</em>}’(t)+\epsilon p’(t)]<br>        \end{matrix}\tag{2.3}<br>$$</p>
<ul>
<li>Instead of considering $V$ as a functional of $y$ path, now consider it as <u>a function of the variable $\epsilon$</u></li>
</ul>
<p>$$<br>V(\epsilon)=\int^T_0 F[y, y^{<em>}(t)+\epsilon p(t), {y^</em>}’(t)+\epsilon p’(t)]dt\tag{2.12}<br>$$</p>
<p>The <u>necessary condition</u> for the extremal is:<br>$$<br>\frac{dV}{d\epsilon}\bigg\vert_{\epsilon=0}=0\tag{2.4}<br>$$</p>
<ul>
<li><p>Then we have<br>$$<br>\begin{aligned}</p>
<pre><code>    \frac&#123;dV&#125;&#123;d\epsilon&#125;&amp;=\int^T_0\frac&#123;\partial F&#125;&#123;\partial \epsilon&#125;dt=\int^T_0\left(\frac&#123;\partial F&#125;&#123;\partial y&#125;\frac&#123;dy&#125;&#123;d\epsilon&#125;+\frac&#123;\partial F&#125;&#123;\partial y&#39;&#125;\frac&#123;dy&#39;&#125;&#123;d\epsilon&#125;\right)dt\\
    &amp;=\int^T_0[F_yp(t)+F_&#123;y&#39;&#125;p&#39;(t)]dt
    \end&#123;aligned&#125;\tag&#123;2.13&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Breaking the integral into two parts<br>$$<br>\int^T_0 F_yp(t)dt+\int^T_0F_{y’}p’(t)dt=0\tag{2.14}<br>$$</p>
<ul>
<li>While this form of necessary condition is already <u>free of the arbitrary variable</u> $\epsilon$, the arbitrary perturbing curve $p(t)$ is still present along with its derivative $p’(t)$. </li>
<li>To make the necessary condition fully operational, we must also eliminate $p(t)$ and $p’(t)$.</li>
</ul>
</li>
<li><p>Integration by parts<br>$$<br>\begin{aligned}</p>
<pre><code>        \int^T_0F_&#123;y&#39;&#125;p&#39;(t)dt&amp;=[F_&#123;y&#39;&#125;p(t)]^T_0-\int^T_0 p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt\\
        &amp;=-\int^T_0p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt
    \end&#123;aligned&#125;\tag&#123;2.16&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Another version of necessary condition for the extremal:<br>$$<br>\int^T_0 p(t)\left[F_y-\frac{d}{dt}F_{y’}\right]dt=0\tag{2.17}<br>$$</p>
<ul>
<li><u>Precisely because $p(t)$ enters in an arbitrary way</u>, we may conclude that the condition $(2.17)$ can only be satisfied only if the bracketed expression is made to vanish for every value of $t$ on the extremal.</li>
</ul>
</li>
<li><p>Consequently, it is a necessary condition for an extremal that<br>$$<br>F_y-\frac{d}{dt}F_{y’}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.18}<br>$$</p>
<ul>
<li><p>Break the second-order derivative into three parts:<br>$$</p>
<pre><code>    \frac&#123;dF_&#123;y&#39;&#125;&#125;&#123;dt&#125;=\frac&#123;\partial F_&#123;y&#39;&#125;&#125;&#123;\partial t&#125;+\frac&#123;\partial F_&#123;y&#39;&#125;&#125;&#123;\partial y&#125;\frac&#123;dy&#125;&#123;dt&#125;+\frac&#123;\partial F_&#123;y&#39;&#125;&#125;&#123;\partial y&#39;&#125;\frac&#123;dy&#39;&#125;&#123;dt&#125;=F_&#123;ty&#39;&#125;+F_&#123;yy&#39;&#125;y&#39;(t)+F_&#123;y&#39;y&#39;&#125;y&#39;&#39;(t)</code></pre>
<p>$$</p>
</li>
<li><p>We get the expanded version of Euler Equation:<br>$$<br>F_{y’y’}y’’(t)+F_{yy’}y’(t)+F_{ty’}-F_y=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.19}<br>$$</p>
<ul>
<li>The Euler equation is in general a second-order nonlinear differential equation. Its general solution will thus contain two arbitrary constants.</li>
<li>Since our problem in (2.1) comes with two boundary conditions (one initial and one terminal), we should normally possess sufficient information to definitize the two arbitrary constants and obtain the definite solution.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Some-Special-Cases"><a href="#Some-Special-Cases" class="headerlink" title="Some Special Cases"></a>Some Special Cases</h2><h3 id="Special-Case-I-F-F-t-y’"><a href="#Special-Case-I-F-F-t-y’" class="headerlink" title="Special Case I: $F=F(t,y’)$"></a>Special Case I: $F=F(t,y’)$</h3><p>$$<br>F_y-\frac{d}{dt}F_{y’}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.18}<br>$$</p>
<ul>
<li>In this special case, the $F$ function is free of $y$, implying that $F_y=0$. Hence the Euler equation reduces  to $\frac{dF_{y’}}{dt}=0$, Hence the solution is<br>$$<pre><code>F_&#123;y&#39;&#125;=\mbox&#123;constant&#125;\tag&#123;2.20&#125;</code></pre>
$$</li>
</ul>
<h3 id="Special-Case-II-F-F-y-y’"><a href="#Special-Case-II-F-F-y-y’" class="headerlink" title="Special Case II: $F=F(y,y’)$"></a>Special Case II: $F=F(y,y’)$</h3><p>$$<br>F_{y’y’}y’’(t)+F_{yy’}y’(t)+F_{ty’}-F_y=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.19}<br>$$</p>
<ul>
<li><p>Since $F$ is free of $t$ in this case,  we have $F_{ty’}=0$, so the Euler equations reduces to<br>$$<br> F_{y’y’}y’’(t)+F_{yy’}y’(t)-F_y=0<br>$$</p>
</li>
<li><p>Note that<br>$$</p>
<pre><code>  \begin&#123;aligned&#125;
      \frac&#123;d&#125;&#123;dt&#125;(y&#39;F_&#123;y&#39;&#125;-F)&amp;=\frac&#123;d&#125;&#123;dt&#125;(y&#39;F_&#123;y&#39;&#125;)-\frac&#123;d&#125;&#123;dt&#125;F(y,y&#39;)\\
      &amp;=F_&#123;y&#39;&#125;&#123;y&#39;&#39;&#125;+y&#39;(F_&#123;yy&#39;&#125;y&#39;+F_&#123;y&#39;y&#39;&#125;y&#39;&#39;)-(F_yy&#39;+F_&#123;y&#39;&#125;y&#39;&#39;)\\
      &amp;=y&#39;(F_&#123;y&#39;y&#39;&#125;y&#39;&#39;+F_&#123;yy&#39;&#125;y&#39;-F_y)
  \end&#123;aligned&#125;</code></pre>
<p>$$</p>
</li>
<li><p>So the Euler equation can be written as $\frac{d}{dt}(y’F_{y’}-F)=0$, which means<br>$$<br>F-y’F_{y’}=\mbox{constant}\tag{2.21}<br>$$</p>
</li>
</ul>
<h3 id="Special-Case-III-F-F-y’"><a href="#Special-Case-III-F-F-y’" class="headerlink" title="Special Case III: $F=F(y’)$"></a>Special Case III: $F=F(y’)$</h3><p>$$<br>F_{y’y’}y’’(t)+F_{yy’}y’(t)+F_{ty’}-F_y=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.19}<br>$$</p>
<ul>
<li><p>When $F$ depends on $y’$ alone, $F_{yy’}=F_{ty’}=F_y=0$. And the Euler equation reduces to<br>$$</p>
<pre><code>F_&#123;y&#39;y&#39;&#125;y&#39;&#39;(t)=0\tag&#123;2.24&#125;</code></pre>
<p>$$</p>
</li>
<li><p>If $y’’(t)=0$, then $y’(t)=c_1$ and $y(t)=c_1t+c_2$.</p>
</li>
<li><p>If $F_{y’y’}=0$, then since $F_{y’y’}$ is, like $F$ itself, a function of $y’$ alone, the solution of $F_{y’y’}=0$ should appear as specific values pf $y’$. </p>
<ul>
<li>Suppose there are one or more real solutions $y’=k_i$, then we can deduce that $y=k_it+c$, which again represents a family of straight lines.</li>
<li>Consequently, given an integrand function that depends on $y’$ alone, we can always take its extermal to be a straight line. </li>
</ul>
</li>
</ul>
<h3 id="Special-Case-IV-F-F-t-y"><a href="#Special-Case-IV-F-F-t-y" class="headerlink" title="Special Case IV: $F=F(t,y)$"></a>Special Case IV: $F=F(t,y)$</h3><p>$$<br>F_y-\frac{d}{dt}F_{y’}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.18}<br>$$</p>
<ul>
<li><p>Since $y’$ is missing from the $F$ function, we have $F_{y’}=0$, the Euler equation reduces to<br>$$</p>
<pre><code>    F_y(t,y)=0</code></pre>
<p>$$</p>
</li>
<li><p>The fact that the derivative $y’$ does not appear in this equation means that the Euler equation is not a differential equation.</p>
</li>
<li><p>Since there are no arbitrary constraints in its solution to be definitized in accordance with the given boundary conditions, the extremal may not satisfy the boundary conditions except by sheer coincidence.</p>
</li>
<li><p>Thus, $F(t,y)$ is, in a special sense, “linear” in $y’$.</p>
</li>
</ul>
<h2 id="Two-generalizations-of-the-Euler-Euqation"><a href="#Two-generalizations-of-the-Euler-Euqation" class="headerlink" title="Two generalizations of the Euler Euqation"></a>Two generalizations of the Euler Euqation</h2><h3 id="The-Case-of-Several-State-Variables"><a href="#The-Case-of-Several-State-Variables" class="headerlink" title="The Case of Several State Variables"></a>The Case of Several State Variables</h3><p>The objective functional with $n&gt;1$ state variables<br>$$<br>V[y_1,…,y_n]=\int^T_0 F(t,y_1,…,y_n,y_1’,…,y_n’)dt\tag{2.26}<br>$$</p>
<ul>
<li><p>Then we have<br>$$<br>F_{y_j}-\frac{d}{dt}F_{y’_j}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.27}<br>$$</p>
<ul>
<li>When $n=2$, we have<br>$$<br>\begin{aligned}<pre><code>    F_&#123;y&#39;y&#39;&#125;y&#39;&#39;(t)+F_&#123;z&#39;y&#39;&#125;z&#39;&#39;(t)+F_&#123;yy&#39;&#125;y&#39;(t)+F_&#123;zy&#39;&#125;z&#39;(t)+F_&#123;ty&#39;&#125;-F_y=0\\
    F_&#123;y&#39;z&#39;&#125;y&#39;&#39;(t)+F_&#123;z&#39;z&#39;&#125;z&#39;&#39;(t)+F_&#123;yz&#39;&#125;y&#39;(t)+F_&#123;zz&#39;&#125;z&#39;(t)+F_&#123;tz&#39;&#125;-F_z=0\\&amp; \mbox&#123;for all&#125;\ t\in[0,T]
    \end&#123;aligned&#125;\tag&#123;2.28&#125;</code></pre>
$$</li>
</ul>
</li>
</ul>
<h3 id="The-Case-of-Higher-Order-Derivatives"><a href="#The-Case-of-Higher-Order-Derivatives" class="headerlink" title="The Case of Higher-Order Derivatives"></a>The Case of Higher-Order Derivatives</h3><p>The objective functional contains high-order derivatives<br>$$<br>V[y]=\int^T_0 F(t,y,y’’,…,y^{(n)})dt\tag{2.29}<br>$$</p>
<ul>
<li>Then we have<br>$$<br>F_y-\frac{d}{dt}F_{y’}+\frac{d^2}{dt^2}F_{y’}-\cdots+(-1)^n\frac{d^n}{dt^n}F_{y^{(n)}}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler-Poisson Equation]}    \tag{2.30}<br>$$</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li>
</ol>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="hqin"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">hqin</p><p class="is-size-6 is-block">A Student in Economics</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Zhengzhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">52</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">5</p></a></div></div></nav></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:22:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Infinite-Planning-Horizon/">Infinite Planning Horizon</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:22:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Constrained-Problems/">Constrained Problems</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:19:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Second-Order-Conditions/">Second-Order Conditions</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:17:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Transversality-Conditions-for-Variable-Endpoint-Problems/">Transversality Conditions for Variable-Endpoint Problems</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:17:21.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/The-Fundamental-Problem-of-the-Calculus-of-Variations/">The Fundamental Problem of the Calculus of Variations</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li></ul></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://ygnmax.github.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Guangnan Yang</span></span><span class="level-right"><span class="level-item tag">ygnmax.github.io</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Abstract-Algebra/"><span class="tag">Abstract Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Control-Theory/"><span class="tag">Control Theory</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Measure-Theoretic-Probability/"><span class="tag">Measure Theoretic Probability</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Real-Analysis/"><span class="tag">Real Analysis</span><span class="tag">24</span></a></div></div></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/1-Mathematics/"><span class="level-start"><span class="level-item">1 Mathematics</span></span><span class="level-end"><span class="level-item tag">52</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/"><span class="level-start"><span class="level-item">1.1 Analysis</span></span><span class="level-end"><span class="level-item tag">44</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-A-Preliminaries-of-Real-Analysis/"><span class="level-start"><span class="level-item">1.1.A Preliminaries of Real Analysis</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/"><span class="level-start"><span class="level-item">1.1.B Metric Spaces and Continuity</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-C-Linear-Spaces-and-Convexity/"><span class="level-start"><span class="level-item">1.1.C Linear Spaces and Convexity</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-D-Metric-Linear-Spaces-and-Normed-Linear-Spaces/"><span class="level-start"><span class="level-item">1.1.D Metric Linear Spaces and Normed Linear Spaces</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-E-Probability-via-Measure-Theory/"><span class="level-start"><span class="level-item">1.1.E Probability via Measure Theory</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/"><span class="level-start"><span class="level-item">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-G-Weak-Convergence-and-Probability-Limit/"><span class="level-start"><span class="level-item">1.1.G Weak Convergence and Probability Limit</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/"><span class="level-start"><span class="level-item">1.1.H Stochastic Independence and Dependence</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-2-Algebra/"><span class="level-start"><span class="level-item">1.2 Algebra</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-2-Algebra/1-2-A-Abstract-Algebra/"><span class="level-start"><span class="level-item">1.2.A Abstract Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-2-Algebra/1-2-B-Linear-Algebra/"><span class="level-start"><span class="level-item">1.2.B Linear Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/"><span class="level-start"><span class="level-item">1.4 Dynamic Optimization</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-A-Preliminaries-of-Dynamic-Optimization/"><span class="level-start"><span class="level-item">1.4.A Preliminaries of Dynamic Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/"><span class="level-start"><span class="level-item">1.4.B The Calculus of Variations</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">hqin</a><p class="is-size-7"><span>&copy; 2020 - 2021 hqin</span>  </p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'folded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>