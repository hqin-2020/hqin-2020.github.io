<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>hqin</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="hqin"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="hqin"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="hqin"><meta property="og:url" content="https://hqin-2020.github.io/"><meta property="og:site_name" content="hqin"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://hqin-2020.github.io/img/og_image.png"><meta property="article:author" content="hqin"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hqin-2020.github.io"},"headline":"hqin","image":["https://hqin-2020.github.io/img/og_image.png"],"author":{"@type":"Person","name":"hqin"},"description":""}</script><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="hqin" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">hqin</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-03T14:24:22.000Z" title="1/3/2021, 10:24:22 PM">2021-01-03</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-C-Optimal-Control-Theory/">1.4.C Optimal Control Theory</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/Optimal-Control-The-Maximum-Principle/">Optimal Control: The Maximum Principle</a></h1><div class="content"><h2 id="The-Simplest-Problem-of-Optimal-Control"><a href="#The-Simplest-Problem-of-Optimal-Control" class="headerlink" title="The Simplest Problem of Optimal Control"></a>The Simplest Problem of Optimal Control</h2><p>$$<br>\begin{array}{llll}<br>        \mbox{Maximize}&amp;V=\int^T_0 F(t,y, u)dt\<br>        \<br>        \mbox{subject to} &amp;\dot{y}={f}(t, y,u)\<br>        \<br>        &amp;y(0)=A&amp;y(T)\ \mbox{free}&amp;(A,T\ \mbox{given})\<br>        \<br>        \mbox{and}&amp;u(t)\in\mathscr{U}&amp;\mbox{for all } t\in[0,T]<br>        \end{array}<br>$$</p>
<ul>
<li> Minimizing $\int^T_0F(t,y,u)dt$ is equivalent to maximizing $\int^T_0-F(t,y,u)dt$</li>
<li> $\dot{y}={f}(t, y,u)$ provides the mechanism whereby our choice of the control $u$ can be translated into a specific pattern of movement of the state variable $y$. For this reason, this equation is referred to  as the equation of motion for the state variable or the state equation for short.</li>
</ul>
<h2 id="The-Maximum-Principle"><a href="#The-Maximum-Principle" class="headerlink" title="The Maximum Principle"></a>The Maximum Principle</h2><h3 id="The-Costate-Variable-and-the-Hamiltonian-Function"><a href="#The-Costate-Variable-and-the-Hamiltonian-Function" class="headerlink" title="The Costate Variable and the Hamiltonian Function"></a>The Costate Variable and the Hamiltonian Function</h3><p>Three types of variables in the problem (7.1): $t$ (time), $y$ (state), $u$ (control). One costate variable: $\lambda$ (Lagrange multiplier)</p>
<ul>
<li> $y,u,\lambda$ take different values at different points of time.</li>
<li>$\lambda$ is a short version of $\lambda(t)$.</li>
</ul>
<p>Define the Hamiltonian function, or simply the Hamiltonian:<br>$$<br>H(t,y,u,\lambda)\equiv F(t,y,u)+\lambda(t)f(t,y,u)\tag{7.3}<br>$$</p>
<ul>
<li><p>Strictly speaking, the Hamiltonian should have been written as<br>$$</p>
<pre><code>    H(t,y,u,\lambda)\equiv\lambda_0 F(t,y,u)+\lambda(t)f(t,y,u)\tag&#123;7.3&#39;&#125;</code></pre>
<p>$$</p>
</li>
<li><p>In reality, zero $\lambda_0$ occurs only in certain situations where the solution of the problem is actually independent of the integrand function $F$, that is the $F$ function does not matter in the solution process.  </p>
</li>
<li><p>Thus, we simply assume $\lambda_0&gt;0$, and then normalize it to unity.</p>
</li>
</ul>
<h3 id="The-Maximum-conditions"><a href="#The-Maximum-conditions" class="headerlink" title="The Maximum conditions"></a>The Maximum conditions</h3><p>$$<br>\begin{array}{ll}<br>        \max_u H(t,y,u,\lambda)&amp;for\ all\ t\in[0,T]\<br>        \<br>        \dot{y}=\frac{\partial H}{\partial \lambda}&amp;[\mbox{equation of motion for }  y]\<br>        \<br>        \dot{\lambda}=-\frac{\partial H}{\partial y}&amp;[\mbox{equation of motion for } \lambda]\<br>        \<br>        \lambda{(T)}=0&amp;[\mbox{transversality condition}]<br>        \end{array}\tag{7.5}<br>$$</p>
<h2 id="The-Rationale-of-the-Maximum-Principle"><a href="#The-Rationale-of-the-Maximum-Principle" class="headerlink" title="The Rationale of the Maximum Principle"></a>The Rationale of the Maximum Principle</h2><p>Since $f(t,y,u)-\dot{y}=0$ is always satisfied for all $t$  in $[0,T]$. Thus we can form a Lagrange objective functional<br>$$<br>\begin{array}{ll}<br>        \mathscr{V}&amp;\equiv V+\int^T_0\lambda(t)[f(t,y,u)-\dot{y}]dt\<br>        \<br>        &amp;=\int^T_0\left{F(t,y,u)+\lambda(t)\left[f(t,y,u)-\dot{y}\right]\right}dt<br>    \end{array}\tag{7.22}<br>$$</p>
<ul>
<li><p>Put the Hamiltonian in (7.22)​<br>$$</p>
<pre><code>\begin&#123;array&#125;&#123;ll&#125;
\mathscr&#123;V&#125;&amp;=\int^T_0[H(t,y,u,\lambda)-\lambda(t)\dot&#123;y&#125;]dt\\
\\
&amp;=\int^T_0H(t,y,u,\lambda)-\int^T_0\lambda(t)\dot&#123;y&#125;dt
\end&#123;array&#125;\tag&#123;7.22&#39;&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Integration by parts<br>$$</p>
<pre><code>    -\int^T_0\lambda(t)\dot&#123;y&#125;dt=\lambda(T)y_T+\lambda(0)y_0+\int^T_0y(t)\dot&#123;\lambda&#125;dt</code></pre>
<p>$$</p>
</li>
<li><p>rewrite (7.22’)<br>$$</p>
<pre><code>    \mathscr&#123;V&#125;=\underbrace&#123;\int^T_0\left[H(t,y,u,\lambda)+y(t)\dot&#123;\lambda&#125;\right]dt&#125;_&#123;\Omega_1&#125;-\underbrace&#123;\lambda(T)y_T&#125;_&#123;\Omega_2&#125;+\underbrace&#123;\lambda(0)y_0&#125;_&#123;\Omega_3&#125;\tag&#123;7.22&#39;&#39;&#125;</code></pre>
<p>$$</p>
</li>
<li><p>To ensure the choice of $\lambda(t)$ path produce no effect on the value of $\mathscr{V}$, we simply impose a necessary condition<br>$$<br>\dot{y}=\frac{\partial H}{\partial \lambda}\ \ \ \ \ \mbox{for all } t\ \in[0,T]\ \ \ \mbox{  [Equation of Motion for }y ]\tag{7.23}<br>$$</p>
</li>
</ul>
<p>Generating neighboring control paths and state paths<br>$$<br>u(t)=u^*(T)+\epsilon p(t)    \tag{7.24}<br>$$</p>
<p>$$<br>y(t)=y^*(T)+\epsilon q(t)    \tag{7.25}<br>$$</p>
<ul>
<li><p>If $T$ and $y_T$ are variable, we also have<br>$$</p>
<pre><code>    T=T^*+\epsilon\Delta T\ \ \ \mbox&#123;and&#125;\ \ \ \ y_T=y_T^*+\epsilon\Delta y_T\tag&#123;7.26&#125;</code></pre>
<p>$$</p>
<ul>
<li>Since the choice of $\lambda(t)$ path produce no effect on the value of $\mathscr{V}$, so we don’t need to generate the neighboring paths of $\lambda(t)$.</li>
</ul>
</li>
<li><p>Expressing $\mathscr{V}$ as a function of $\epsilon$<br>$$</p>
<pre><code>    \begin&#123;array&#125;&#123;ll&#125;
        \mathscr&#123;V&#125;=&amp;\int^&#123;T(\epsilon)&#125;_0\left\&#123;H\left[t,y^*+\epsilon q(t),u^*+\epsilon q(t),\lambda\right]+\dot&#123;\lambda&#125;\left[y^*+\epsilon q(t)\right]\right\&#125;dt\\\\&amp;-\lambda(T)y_T+\lambda(0)y_0
    \end&#123;array&#125;\tag&#123;7.27&#125;</code></pre>
<p>$$</p>
</li>
<li><p>And the necessary condition<br>$$</p>
<pre><code>\frac&#123;dV&#125;&#123;d\epsilon&#125;=0</code></pre>
<p>$$</p>
</li>
<li><p>The differentiation of the first term of (7.27)​<br>$$</p>
<pre><code>\begin&#123;aligned&#125;
    &amp;&#123;&#125;\int^&#123;T(\epsilon)&#125;_0\left\&#123;\left[\frac&#123;\partial H&#125;&#123;\partial y&#125;q(t)+\frac&#123;\partial H&#125;&#123;\partial u&#125;p(t)\right]+\dot&#123;\lambda&#125;q(t)\right\&#125;dt+\left[H+\dot&#123;\lambda&#125;y\right]_&#123;t=T&#125;\frac&#123;dT&#125;&#123;d\epsilon&#125;\\
    =&amp;\int^&#123;T(\epsilon)&#125;_0\left\&#123;\left[\frac&#123;\partial H&#125;&#123;\partial y&#125;q(t)+\frac&#123;\partial H&#125;&#123;\partial u&#125;p(t)\right]+\dot&#123;\lambda&#125;q(t)\right\&#125;dt+[H]_&#123;t=T&#125;\Delta T+\dot&#123;\lambda&#125;(T)y_T\Delta T
\end&#123;aligned&#125;\tag&#123;7.28&#125;</code></pre>
<p>$$</p>
</li>
<li><p>The differentiation of the second term of (7.27)​<br>$$</p>
<pre><code>-\lambda(T)\frac&#123;dy_T&#125;&#123;d\epsilon&#125;-y_T\frac&#123;d\lambda(T)&#125;&#123;dT&#125;\frac&#123;dT&#125;&#123;d\epsilon&#125;=-\lambda(T)\Delta y_&#123;T&#125;-y_T\dot&#123;\lambda&#125;(T)\Delta T\tag&#123;7.29&#125;</code></pre>
<p>$$</p>
</li>
<li><p>On the other hand, $\lambda(0)y_0$ term in (7.27)​ drops out in differentiation. So we add up (7.28)​ and (7.29)​ together<br>$$</p>
<pre><code>\frac&#123;d\mathscr&#123;V&#125;&#125;&#123;d\epsilon&#125;=\int^&#123;T(\epsilon)&#125;_0\left\&#123;\left[\left(\frac&#123;\partial H&#125;&#123;\partial y&#125;+\dot&#123;\lambda&#125;\right)q(t)+\frac&#123;\partial H&#125;&#123;\partial u&#125;p(t)\right]\right\&#125;dt+[H]_&#123;t=T&#125;\Delta T-\lambda(T)\Delta y_&#123;T&#125;=0\tag&#123;7.30&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Three components in (7.30)​ must individually set equal to zero. So we get<br>$$<br>\dot{\lambda}=-\frac{\partial H}{\partial y}\ \ \ \mbox{  [Equation of Motion for }\lambda ]<br>$$</p>
<p>$$<br>\frac{\partial H}{\partial u}=0\ \ \ \mbox{  [Weaker version of }\max_uH \mbox{ condition} ]<br>$$</p>
</li>
<li><p>Since the simplest problem has a fixed $T$ and free $y_T$ the $\Delta T$ term in (7.30)​ is automatically equal to zero, but $\Delta y_T$ is not. So we get<br>$$</p>
<pre><code>    \lambda(T)=0\ \ \ \ \ \ \mbox&#123;  [Transversality Condition]&#125;</code></pre>
<p>$$</p>
<ul>
<li>In order for the maximum principle to work, the $\lambda(T)$ path is not to be arbitrarily chose, but is require to follow a prescribed equation of motion, and it must end with a terminal value of zero if the problem has a free terminal state.</li>
</ul>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-03T14:23:22.000Z" title="1/3/2021, 10:23:22 PM">2021-01-03</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Constrained-Problems/">Constrained Problems</a></h1><div class="content"><h2 id="Four-Basic-Types-of-Constraints"><a href="#Four-Basic-Types-of-Constraints" class="headerlink" title="Four Basic Types of Constraints"></a>Four Basic Types of Constraints</h2><h3 id="Equality-Constraints"><a href="#Equality-Constraints" class="headerlink" title="Equality Constraints"></a>Equality Constraints</h3><p>Let the problem be that of maximizing<br>$$<br>V=\int^T_0 F(t,y_1, \dots, y_n, y_1’,\dots, y_n’)dt\tag{6.1}<br>$$</p>
<ul>
<li><p>subject to a set of $m$ independent but consistent constraints $(m&lt;n)$<br>$$</p>
<pre><code>    \begin&#123;array&#125;&#123;ll&#125;
    &#123;g&#125;^1(t, y_1,\cdots, y_n)=c_1\\
    \vdots\\
    g^m(t, y_1,\cdots, y_n)=c_m
    \end&#123;array&#125;\ \ \ \ \ \ \ \ \ (c_1,\cdots, c_m\mbox&#123; are constants&#125;)\tag&#123;6.2&#125;</code></pre>
<p>$$</p>
</li>
<li><p>and appropriate boundary conditions.</p>
</li>
</ul>
<p>Note that, in this problem, the number of constraints, $m$ , is required to be strictly less than the number of state variables, $n$. </p>
<ul>
<li>Otherwise, with $m=n$, the equation system (6.2)​ would already uniquely determine the $y_j(t)$ paths, and there would remain no degree of freedom for any optimization choice. </li>
<li>In view of this, this type of constrained dynamic optimization problem ought to contain at least two state variables, before a single constraint can meaningfully be accommodated.</li>
</ul>
<p>Form a Largrangian integrand function, $\mathscr{F}$, by augmenting the original integrand $F$ in (6.1)​<br>$$<br>\begin{aligned}<br>        \mathscr{F}&amp;=F+\lambda_1(t)(c_1-g^1)+\cdots+\lambda_m(t)(c_m-g^m)\<br>        &amp;=F+\sum^m_{i=1}\lambda_i(t)(c_i-g^i)<br>        \end{aligned}\tag{6.4}<br>$$</p>
<ul>
<li><p>Replace F by $\mathscr{F}$ in the objective functional<br>$$</p>
<pre><code>\mathscr&#123;V&#125;=\int^T_0\mathscr&#123;F&#125;dt\tag&#123;6.5&#125;</code></pre>
<p>$$</p>
</li>
<li><p>which we can maximize as if it is an unconstrained problem. </p>
</li>
<li><p>As long as all of the constraints in (6.2)​ are satisfied, so that $c_i-g^i=0$ for all $i$, then the value of $\mathscr{V}$ will be identical with that of $F$, and the free extremum of the functional $\mathscr{V}$ will be identical with the constrained extremum of the original functional $V$.</p>
</li>
</ul>
<p>Treat the Lagrange multipliers as additional state variables, each subject to an Euler equation, or the Euler-Lagrange equation.<br>$$<br>F_{y_j}-\frac{d}{dt}F_{y’_j}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \ (j=1,\dots,n)\ \ \ \ \ \ [cf.(2.27)]\tag{6.6}<br>$$</p>
<ul>
<li><p>Applied it to the Lagrange multipliers<br>$$<br>\mathscr{F}_{\lambda_i}-\frac{d}{dt}\mathscr{F}_{\lambda’_i}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \ (i=1,\dots,m)\tag{6.7}<br>$$</p>
</li>
<li><p>However, since $\mathscr{F}$ is independent of any $\lambda_i’$, we have $\mathscr{F}_{\lambda_i’}=0$ for every $i$<br>$$</p>
<pre><code>    \mathscr&#123;F&#125;_&#123;\lambda_i&#125;=0\ \ \ \ \ or\ \ \ \ \ \ c_i-g^i=0\ \ \ \ \mbox&#123;for all&#125;\ t\in[0,T]\tag&#123;6.7&#39;&#125;</code></pre>
<p>$$</p>
</li>
<li><p>So we have a total of $n$ Euler-Lagrange equations for the state variables, plus the $m$ constraints to determine the $n + m$ paths, $y_j(t)$ and $\lambda_i(t)$, with the arbitrary constants to be definitized by the boundary conditions.</p>
</li>
</ul>
<h3 id="Differential-Equation-Constraints"><a href="#Differential-Equation-Constraints" class="headerlink" title="Differential-Equation Constraints"></a>Differential-Equation Constraints</h3><p>Now suppose that the problem is to maximize (6.1)​, subject to a consistent set of $m$ independent constraints $(m &lt; n)$ that are differential equations:<br>$$<br>\begin{array}{ll}<br>        {g}^1(t, y_1,\cdots, y_n,y_1,\cdots, y_n’)=c_1\<br>        \vdots\<br>        {g}^m(t, y_1,\cdots,y_n,y_1’,\cdots, y_n’)=c_m<br>        \end{array}\ \ \ \ \ \ \ \ \ (c_1,\cdots, c_m\mbox{ are constants})\tag{6.8}<br>$$</p>
<ul>
<li><p>and appropriate boundary conditions.</p>
</li>
<li><p>The Lagrangian integrand function is still<br>$$</p>
<pre><code>        \mathscr&#123;F&#125;=F+\lambda_1(t)(c_1-g^1)+\cdots+\lambda_m(t)(c_m-g^m)</code></pre>
<p>$$</p>
</li>
<li><p>and Euler-Lagrange Equations with respect to the state  variables $y_j$ are still in the form of<br>$$</p>
<pre><code>F_&#123;y_j&#125;-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;_j&#125;=0\ \ \ \ \ \  \mbox&#123;for all&#125;\ t\in[0,T]\ \ \ \ \ \ \ (j=1,\dots,n)</code></pre>
<p>$$</p>
</li>
<li><p>Moreover, similar equations with respect to the Lagrange multipliers $\lambda_i$ are again a restatement of the given constraints. </p>
<ul>
<li>So we still have a total of $n$ Euler-Lagrange equations for the state variables, plus the $m$ constraints to determine the $n + m$ paths, $y_j(t)$ and $\lambda_i(t)$, with the arbitrary constants to be definitized by the boundary conditions.</li>
</ul>
</li>
</ul>
<h3 id="Inequality-Constraints"><a href="#Inequality-Constraints" class="headerlink" title="Inequality Constraints"></a>Inequality Constraints</h3><p>Generalized problem<br>$$<br>\begin{array}{lll}<br>            \mbox{Maximize or Minimize}&amp;\int^T_0 F(t,y_1,\dots,y_n,y_1’,\dots,y_n’(t))dt\<br>            \<br>            \mbox{subject to}&amp;<br>            {g}^1(t, y_1,\cdots, y_n,y_1,\cdots, y_n’)\leq c_1\<br>            &amp;\vdots\<br>            &amp;{g}^m(t, y_1,\cdots,y_n,y_1’,\cdots, y_n’)\leq c_m\<br>            \<br>            \mbox{and} &amp;\mbox{appropriate boundary conditions}<br>            \end{array}\tag{6.9}<br>$$</p>
<ul>
<li>Since inequality constraints are much less stringent then equality constraints, there is no need to stipulate that $m &lt; n$ .</li>
<li>Even if the number of constraints exceeds the number of state variables, the inequality constraints as a group will not uniquely determine the $Y_j$ paths, and hence will not eliminate all degrees of freedom from our choice problem. <ul>
<li>However, the inequality constraints do have to be consistent with one another, as well as with the other aspects of the problem.</li>
</ul>
</li>
</ul>
<p>The Lagrangian integrand function is still<br>$$<br>\mathscr{F}=F+\lambda_1(t)(c_1-g^1)+\cdots+\lambda_m(t)(c_m-g^m)<br>$$</p>
<ul>
<li>and Euler-Lagrange Equations with respect to the state  variables $y_j$ are still in the form of<br>$$<pre><code>    F_&#123;y_j&#125;-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;_j&#125;=0\ \ \ \ \ \  \mbox&#123;for all&#125;\ t\in[0,T]\ \ \ \ \ \ \ (j=1,\dots,n)</code></pre>
$$</li>
</ul>
<p>To ensure that all the $\lambda_i(t)(c_i - g^i)$ terms vanish in the solution (so that the optimized values of $\mathscr{F}$ and $F$ are equal), we need a complementary-slackness relationship between the $i$th multiplier and the $i$th constraint, for every $i$ (a set of $m$ equations):<br>$$<br>\lambda_i(t)(c_i - g^i)=0\ \ \ \ \ \ \ \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \ (j=1,\dots,m)\tag{6.10}<br>$$</p>
<ul>
<li>This complementary-slackness relationship guarantees that <ol>
<li>whenever the $i$th Lagrange multiplier is nonzero, the $i$th constraint will be satisfied as a strict equality, and </li>
<li>whenever the $i$th constraint is a strict inequality, the $i$th Lagrange multiplier will be zero.     <ul>
<li>It is this relationship that serves to maintain the identity between the optimal value of the original integrand $F$ and that of the modified integrand $\mathscr{F}$ in (6.4)​.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 id="Isoperimetric-Constraints"><a href="#Isoperimetric-Constraints" class="headerlink" title="Isoperimetric Constraints"></a>Isoperimetric Constraints</h3><p>Generalized problem<br>$$<br>\begin{array}{lll}<br>            \mbox{Maximize or Minimize}&amp;\int^T_0 F(t,y_1,\dots,y_n,y_1’,\dots,y_n’(t))dt\<br>            \<br>            \mbox{subject to}&amp;<br>            {g}^1(t, y_1,\cdots, y_n,y_1,\cdots, y_n’)=k_1\<br>            &amp;\vdots\<br>            &amp;{g}^m(t, y_1,\cdots,y_n,y_1’,\cdots, y_n’)=k_m\<br>            \<br>            \mbox{and} &amp;\mbox{appropriate boundary conditions}<br>            \end{array}\tag{6.12}<br>$$</p>
<ul>
<li>In the isoperimetric problem, there is again no need to require $m &lt; n$ , because even with $m\geq n$, freedom of optimizing choice is not ruled out.</li>
</ul>
<p>Let $m=n=1$, and define<br>$$<br>    \Gamma(t)=\int^t_0G(t,y,y’)dt\tag{6.13}<br>$$</p>
<ul>
<li><p>which is a function of $t$.</p>
</li>
<li><p>At $t=0$, $t=T$, we have</p>
</li>
</ul>
<p>$$<br>\Gamma(0)=\int^0_0Gdt\ \ \ \ \ \ and\ \  \ \ \ \ \Gamma(T)=\int^T_0Gdt=k\tag{6.14}<br>$$</p>
<ul>
<li><p>Take the derivative on (6.13)​ and get<br>$$</p>
<pre><code>G(t,y,y&#39;)-\Gamma&#39;(t)=0\tag&#123;6.15&#125;</code></pre>
<p>$$</p>
<ul>
<li>which conforms to the general structure of the differential constraint $g(t,y,y’)=c$ with $g=G-\Gamma’$ and $c=0$</li>
</ul>
</li>
</ul>
<p>Take the Lagrangian integrand<br>$$<br>\begin{aligned}<br>        \acute{F}&amp;=F(t,y,y’)+\lambda(t)[0-G(t,y,y’)+\Gamma’(t)]\<br>        &amp;=F(t,y,y’)-\lambda(t)G(t,y,y’)+\lambda(t)\Gamma’(t)<br>    \end{aligned}\tag{6.16}<br>$$</p>
<ul>
<li><p>And there are two state variables $y$ and $\Gamma$, so we have two parallel conditions<br>$$</p>
<pre><code>\acute&#123;F&#125;_&#123;y&#125;-\frac&#123;d&#125;&#123;dt&#125;\acute&#123;F&#125;_&#123;y&#39;&#125;=0\ \ \ \ \ \  \mbox&#123;for all&#125;\ t\in[0,T]\tag&#123;6.17&#125;</code></pre>
<p>$$</p>
<p>$$</p>
<pre><code>\acute&#123;F&#125;_&#123;\Gamma&#125;-\frac&#123;d&#125;&#123;dt&#125;\acute&#123;F&#125;_&#123;\Gamma&#39;&#125;=0\ \ \ \ \ \  \mbox&#123;for all&#125;\ t\in[0,T]\tag&#123;6.18&#125;</code></pre>
<p>$$</p>
<ul>
<li><p>However, inasmuch as $\acute{F}$ is independent of $\Gamma$, and since $\acute{F}_{\Gamma’} = \lambda(t)$, we see that (6.18)​ reduces to the condition<br>$$<br>-\frac{d}{dt}\lambda(t)=0\ \ \ \ \ \ \Longrightarrow\ \ \ \ \ \ \ \lambda(t)=\mbox{constant}\tag{6.18’}<br>$$</p>
</li>
<li><p>Then write the Lagrange multiplier of the isoperimetric problem simply as $\lambda$.</p>
</li>
</ul>
</li>
<li><p>Turning next to (6.17)​ and using ​(6.16)​, obtain a more specific version of Euler-Lagrange equation:<br>$$</p>
<pre><code>(F_y-\lambda G_y)-\frac&#123;d&#125;&#123;dt&#125;(F_&#123;y&#39;&#125;-\lambda G_&#123;y&#39;&#125;)=0\tag&#123;6.19&#125;</code></pre>
<p>$$</p>
</li>
<li><p>However, the same condition could have been obtained without $\Gamma’(t)$ from a modified (abridged) version of $\acute{F}$<br>$$<br>\mathscr{F}=F(t,y,y’)-\lambda G(t,y,y’)\ \ \ \ \ (\lambda=\mbox{constant})\tag{6.20}<br>$$</p>
</li>
<li><p>Thus, in the present one-state-variable problem with a single integral constraint, we can as a practical procedure use the modified Lagrange integrand $\mathscr{F}$ in (6.20) innstead of $\acute{F}$ , and apply the Euler-Lagrange equation to $y$ alone, knowing that the Euler-Lagrange equation for $\lambda$ will merely tell us that $\lambda$ is a constant, the value of which can be determined from the isoperimetric constraint.</p>
</li>
<li><p>Generalize it to the $n$-state-variable, $m$-integral-constraint case<br>$$<br>\mathscr{F}=F-(\lambda_1G^1+\cdots+\lambda_mG^m)\ \ \ \ \  \ \ \ \ (\lambda_i\ \mbox{ are all constants} )\tag{6.21}<br>$$</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-03T14:22:22.000Z" title="1/3/2021, 10:22:22 PM">2021-01-03</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Infinite-Planning-Horizon/">Infinite Planning Horizon</a></h1><div class="content"><h2 id="Methodological-Issues-of-Inifinite-Horizon"><a href="#Methodological-Issues-of-Inifinite-Horizon" class="headerlink" title="Methodological Issues of Inifinite Horizon"></a>Methodological Issues of Inifinite Horizon</h2><p>​    </p>
<ul>
<li>The convergence problem arises because the objective functional, now in the form of $\int^\infty_0F(t, y, y’)dt$, is an improper integral which may or may not have a finite value. </li>
<li>In the case where the integral diverges, there may exist more than one $y(t)$ path that yields an infinite value for the objective functional and it would be difficult to determine which among these paths is optimal. </li>
<li>There are several certain conditions that are sufficient for convergence.</li>
</ul>
<h3 id="Condition-I"><a href="#Condition-I" class="headerlink" title="Condition I"></a>Condition I</h3><p>Given the improper integral $\int^\infty_0F(t, y, y’) dt$, if the integrand $F$ is finite throughout the interval of integration, and if $F$ attains a zero value at some finite point of time, say, $t_0$, and remains at zero for all $t &gt; t_0$, then the integral will converge.</p>
<ul>
<li>Although the integral nominally has an infinite horizon, the effective upper limit of integration is a finite value, $t_0$. Thus, the given improper integral reduces in effect to a proper one, with assurance that it will integrate to a finite value.</li>
</ul>
<h3 id="Condition-III"><a href="#Condition-III" class="headerlink" title="Condition III"></a>Condition III</h3><p>In the integral $\int^\infty_0F(t,y,y’)dt$, if the integrand takes the form of $G(t,y,y’)e^{-\rho t}$, where $\rho$ is a positive rate of discount, and the $G$ function is bounded, then the integral will converge.</p>
<ul>
<li><p>A distinguishing feature of this integral is the presence of the discount factor $e^{\rho t}$ which, ceteris paribus, provides a dynamic force to drive the integrand down toward zero over time at a good speed. </p>
</li>
<li><p>When the $G(t,y,y’)$ component of the integrand is positive (as in most economic applications) and has an upper hound, say, $\hat{G}$, then the downward force of $e^\rho t$ is sufficient to make the integral converge.</p>
</li>
<li><p>More formally, since the value of the $G$ function can never exceed the value of the constant $\hat{G}$, we can write<br>$$</p>
<pre><code>    \int^\infty_0G(t,y,y&#39;)e^&#123;-\rho t&#125;dt\leq\int^\infty_0\hat&#123;G&#125;e^&#123;-\rho t&#125;dt=\frac&#123;\hat&#123;G&#125;&#125;&#123;\rho&#125;\tag&#123;5.3&#125;</code></pre>
<p>$$</p>
</li>
<li><p>It follows that the first integral must also be convergent.</p>
</li>
</ul>
<h3 id="The-Matter-of-Transversality-Conditions"><a href="#The-Matter-of-Transversality-Conditions" class="headerlink" title="The Matter of Transversality Conditions"></a>The Matter of Transversality Conditions</h3><p>When the planning horizon is infinite, there is no longer a specific terminal $T$ value for us to adhere to. And the terminal state may also be left open. Thus transversality conditions are needed.</p>
<ul>
<li><p>Recalling<br>$$<br>[F-y’F_{y’}]<em>{t=T}\Delta T+[F</em>{y’}]<em>{t=T}\Delta y</em>{T}=0\ \ \ \ \ \ \mbox{  [General Transversality Condition]}\tag{3.9}<br>$$</p>
</li>
<li><p>In the present context, $(3.9)$ must be modified to<br>$$</p>
<pre><code>    [F-y&#39;F_&#123;y&#39;&#125;]_&#123;t\to \infty&#125;\Delta T+[F_&#123;y&#39;&#125;]_&#123;t\to \infty&#125;\Delta y_&#123;T&#125;=0\tag&#123;5.4&#125;</code></pre>
<p>$$</p>
<ul>
<li>where each of the two terms must individually vanish.</li>
</ul>
</li>
</ul>
<p>Since there is no fixed $T$ in the present context, $\Delta T$ is perforce nonzero, and this necessitates the condition<br>$$<br>\lim_{t\to \infty}(F-y’F_{y’})=0\ \ \ \ \ \ [\mbox{Transversalilty Condition for the Infinite Horizon}]\tag{5.5}<br>$$</p>
<p>As to the second term in $(5.4)$, if an asymptotic terminal state is specified in the problem:<br>$$<br>\lim_{t\to \infty}y(t)=y_{\infty}=\mbox{a given constant}\tag{5.6}<br>$$</p>
<ul>
<li><p>then the second term in $(5.4)$ will vanish on its own $(\Delta y_T = 0)$ and no transversality condition is needed. </p>
</li>
<li><p>But if the terminal state is free, then we should impose the additional condition<br>$$<br>\lim_{t\to\infty} F_{y’}=0\ \ \ \ \ \ [\mbox{Transversalilty Condition for Free Terminal State}]\tag{5.7}<br>$$</p>
</li>
</ul>
<h2 id="The-Optimal-Social-Saving-Behavior"><a href="#The-Optimal-Social-Saving-Behavior" class="headerlink" title="The Optimal Social Saving Behavior"></a>The Optimal Social Saving Behavior</h2><h3 id="The-Ramsey-Model"><a href="#The-Ramsey-Model" class="headerlink" title="The Ramsey Model"></a>The Ramsey Model</h3><p>The central question addressed by Ramsey is that of intertemporal resource allocation: </p>
<ol>
<li>How much of the national output at any point of time should be for current consumption to yield current utility, and </li>
<li>how much should be saved (and invested) so as to enhance future production and consumption, and hence yield future utility?</li>
</ol>
<p>$$<br>\mbox{Maximize}\ \int^\infty_0[U(C)-D(L)]dt\tag{5.26}<br>$$</p>
<ul>
<li><p>where<br>$$</p>
<pre><code>C=Q(K,L)-K&#39;\tag&#123;5.25&#125;</code></pre>
<p>$$</p>
</li>
<li><p>since we assuming away depreciation.<br>$$<br>Q(K,L)=C+S=C+K’<br>$$</p>
<ul>
<li>This implies the output can be consumed or saved, but what is saved always results in investment and capital accumulation. </li>
<li>Utility function $U(C)$ has nonincreasing marginal utility, $U’’(C)\leq 0$</li>
</ul>
</li>
<li><p>In order to produce its consumption goods, society incurs disutility of labor $D(L)$, with nondecreasing marginal utility, $D’’(L)\geq 0$</p>
</li>
</ul>
<h3 id="The-Question-of-Convergence"><a href="#The-Question-of-Convergence" class="headerlink" title="The Question of Convergence"></a>The Question of Convergence</h3><p>The absence of discount factor unfortunately forfeits the opportunity to take advantage of Condition III to establish convergence, even if the integrand has an upper bound. </p>
<ul>
<li>Thus replace (5.26)​ with the following substitute problem:</li>
</ul>
<p>$$<br>\begin{array}{lll}<br>        \mbox{Minimize} &amp;\int^\infty_0[B-U(C)+D(L)]dt\<br>        \<br>        \mbox{subject to} &amp;K(0)=K_0&amp;(K_0)\ \mbox{given}<br>    \end{array}\tag{5.26’}<br>$$</p>
<ul>
<li>where $B$ (for Bliss) is a postulated maximum attainable level of net utility.</li>
<li>Condition I guarantees the convergence.</li>
</ul>
<h3 id="The-Solution-of-the-Model"><a href="#The-Solution-of-the-Model" class="headerlink" title="The Solution of the Model"></a>The Solution of the Model</h3><p>From (5.26’)​ we get<br>$$<br>F=B-U(C)+D(L)\ \ \ \ \ \ \ \mbox{where}\ C=Q(K,L)-K’<br>$$</p>
<ul>
<li><p>The derivative with respect to $L$ and $L’$:<br>$$<br>\begin{aligned}</p>
<pre><code>    &amp;F_L=-U&#39;(C)\frac&#123;\partial C&#125;&#123;\partial L&#125;+D&#39;(L)\equiv-\mu Q_L+D&#39;(L)\\
    &amp;F_&#123;L&#39;&#125;=0
\end&#123;aligned&#125;</code></pre>
<p>$$</p>
</li>
<li><p>The derivative with respect to $K$ and $K’$:<br>$$</p>
<pre><code>\begin&#123;aligned&#125;
&amp;F_K=-U&#39;(C)\frac&#123;\partial C&#125;&#123;\partial K&#125;\equiv-\mu Q_K\\
&amp;F_&#123;K&#39;&#125;=-U&#39;(C)\frac&#123;\partial C&#125;&#123;\partial K&#125;=-U&#39;(C)(-1)=\mu
\end&#123;aligned&#125;</code></pre>
<p>$$</p>
</li>
</ul>
<p>According to the Euler Equation<br>$$<br>F_{y_j}-\frac{d}{dt}F_{y’_j}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.27}<br>$$</p>
<ul>
<li><p>we have<br>$$</p>
<pre><code>    \begin&#123;aligned&#125;
        &amp;F_L-\frac&#123;dF_&#123;L&#39;&#125;&#125;&#123;dt&#125;=0\\
        &amp;F_K-\frac&#123;dF_&#123;K&#39;&#125;&#125;&#123;dt&#125;=0
    \end&#123;aligned&#125;</code></pre>
<p>$$</p>
</li>
<li><p>which means<br>$$</p>
<pre><code>    D&#39;(L)=\mu Q_L\ \ \ \ \mbox&#123;for all &#125; t\geq0\tag&#123;5.27&#125;</code></pre>
<p>$$</p>
<p>$$<br>\frac{d\mu/dt}{\mu}=-Q_K\ \ \ \ \mbox{for all } t\geq0\tag{5.28}<br>$$</p>
</li>
<li><p>The marginal disutility of labor must, at each point of time, be equated to the product of the marginal utility of consumption and the marginal product of labor.</p>
</li>
<li><p>The marginal utility of consumption, must at every point of time have a growth rate equal to the negative of the marginal product of capital. </p>
</li>
</ul>
<h3 id="A-Look-at-the-Transversality-Conditions"><a href="#A-Look-at-the-Transversality-Conditions" class="headerlink" title="A Look at the Transversality Conditions"></a>A Look at the Transversality Conditions</h3><p>$$<br>\lim_{t\to \infty}(F-y’F_{y’})=0\ \ \ \ \ \ [\mbox{Transversalilty Condition for the Infinite Horizon}]\tag{5.5}<br>$$</p>
<ul>
<li>When it is applied to two state variables, then we have </li>
</ul>
<p>$$<br>\lim_{t\to \infty}(F-L’F_{L’})=0\ \ \ \ \ \ \ \ \ and\ \ \ \ \ \ \ \         \lim_{t\to \infty}(F-K’F_{K’})=0<br>$$</p>
<ul>
<li><p>In view of the fact that $F_{L’} = 0$, the first of these conditions reduces to the condition that $F\to 0$ as $t\to\infty$. </p>
<ul>
<li>This would mean the net utility $U(C) - D(L)$ must tend to Bliss. </li>
<li>Note that, by itself, this condition still leaves the constant in (5.29)​ uncertain. </li>
<li>However, the other condition will fix the constant at zero because $F - K’F_{K’}$ is nothing but the left-hand-side expression in (5.29)​.</li>
</ul>
</li>
<li><p>The present problem implicitly specifies the terminal state at Bliss. Consequently, the transversality condition (5.7) is not needed.</p>
</li>
</ul>
<h3 id="The-Optimal-Investment-and-Capital-Paths"><a href="#The-Optimal-Investment-and-Capital-Paths" class="headerlink" title="The Optimal Investment and Capital Paths"></a>The Optimal Investment and Capital Paths</h3><p>Of greater interest to us is the optimal $K$ path and the related investment (and savings) path $K’$.</p>
<ul>
<li>Instead of deducing these from the previous results, let us find this information by taking advantage of the fact that the present problem also falls under the Special Case II: $F=F(y,y’)$, which means</li>
</ul>
<p>$$<br>F-K’F_{K’}=constant<br>$$</p>
<ul>
<li><p>or<br>$$<br>B-U(C)+D(L)-K’\mu=constant\ \ \ \ \ \ \ \ \mbox{for all } t\geq0\tag{5.29}<br>$$</p>
</li>
<li><p>This equation can be solved for $K’$ as soon as the (arbitrary) constant on the right-hand side can be assigned a specific value. </p>
<ul>
<li>To find this value, we note that this constant is to hold for all $t$, including $t\to\infty$.</li>
<li>We can thus make use of the fact that as $t\to\infty$, the economic objective of the model is to have $U(C) - D(L)$ tend to Bliss. </li>
</ul>
</li>
</ul>
<p>As $t\to\infty$, $B-U(C)+D(L)\to0$, $\mu\to0$,  then we have the arbitrary constant has to be $0$. </p>
<ul>
<li>Thus</li>
</ul>
<p>$$<br>    {K^*}’=\frac{B-U(C)+D(L)}{\mu}\tag{5.30}<br>$$</p>
<ul>
<li><p>With the time argument explicitly written out,<br>$$<br>{K^*}’(t)=\frac{B-U[C(t)]+D[L(t)]}{\mu(t)}\tag{5.30’}<br>$$</p>
</li>
<li><p>This result is known as the Ramsey rule. </p>
<ul>
<li>It stipulates that, optimally, the rate of capital accumulation must at any point of time be equal to the ratio of the shortfall of net utility from Bliss to the marginal utility of consumption. </li>
</ul>
</li>
</ul>
<p>From the Ramsey rule, it is possible to go one step further to find the $K^*(t)$ path by integrating (5.30’)​. </p>
<ul>
<li><p>For that, however, we need specific forms of $U(C)$ and $D(L)$ functions. </p>
</li>
<li><p>The general solution of (5.30’)​ will contain one arbitrary constant, which can be definitized by the initial condition $K(0) = K_0$. </p>
<ul>
<li>And that would complete the solution of the model. </li>
</ul>
</li>
</ul>
<h2 id="The-Concavity-Convexity-Sufficient-Condition-Again"><a href="#The-Concavity-Convexity-Sufficient-Condition-Again" class="headerlink" title="The Concavity / Convexity Sufficient Condition Again"></a>The Concavity / Convexity Sufficient Condition Again</h2><h3 id="Sufficient-Conditions"><a href="#Sufficient-Conditions" class="headerlink" title="Sufficient Conditions"></a>Sufficient Conditions</h3><p>​    </p>
<p>Recall that if the integrand function $F(t,y,y’)$ in a fixed-endpoint problem is concave / convex in the variables $(y, y’)$, then the Euler equation is sufficient for an absolute maximum / minimum of $V[y]$. </p>
<ul>
<li><p>Moreover, this sufficiency condition remains applicable when the terminal time is fixed but the terminal state is variable, provided that the supplementary condition<br>$$</p>
<pre><code>[F_&#123;y&#39;&#125;(y-y^*)]_&#123;t=&#123;T&#125;&#125;\leq 0</code></pre>
<p>$$</p>
<ul>
<li>is satisfied. </li>
</ul>
</li>
<li><p>For the infinite-horizon case, this supplementary becomes<br>$$</p>
<pre><code>\lim_&#123;t\to \infty&#125;[F_&#123;y&#39;&#125;(y-y^*)]\leq 0\tag&#123;5.44&#125;</code></pre>
<p>$$</p>
</li>
<li><p>In this condition, $F_{y’}$ is to be evaluated along the optimal path, and $(y-y^*)$ represents the deviation of any admissible neighboring path $y(t)$ from the optimal path $y^*(t)$.</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-03T14:19:22.000Z" title="1/3/2021, 10:19:22 PM">2021-01-03</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Second-Order-Conditions/">Second-Order Conditions</a></h1><div class="content"><h2 id="Second-Order-Conditions"><a href="#Second-Order-Conditions" class="headerlink" title="Second-Order Conditions"></a>Second-Order Conditions</h2><h3 id="Second-Order-Necessary-Conditions"><a href="#Second-Order-Necessary-Conditions" class="headerlink" title="Second-Order Necessary Conditions"></a>Second-Order Necessary Conditions</h3><p>$$<br>\frac{d^2 V}{d\epsilon ^2}\leq 0\ \ \ \ \ \ [\mbox{for maximization of  } V]<br>$$</p>
<p>$$<br>\frac{d^2 V}{d\epsilon ^2}\geq 0\ \ \ \ \ \ [\mbox{for minimization of } V]<br>$$</p>
<h3 id="Second-Order-Sufficient-Conditions"><a href="#Second-Order-Sufficient-Conditions" class="headerlink" title="Second-Order Sufficient Conditions"></a>Second-Order Sufficient Conditions</h3><p>$$<br>\frac{d^2 V}{d\epsilon ^2}&lt; 0\ \ \ \ \ \ [\mbox{for maximization of } V]<br>$$</p>
<p>$$<br>\frac{d^2 V}{d\epsilon ^2}&gt; 0\ \ \ \ \ \ [\mbox{for minimization of } V]<br>$$</p>
<h3 id="The-Second-Derivative-of-V"><a href="#The-Second-Derivative-of-V" class="headerlink" title="The Second Derivative of $V$"></a>The Second Derivative of $V$</h3><p>Recalling the first derivative of $V$<br>$$<br>\begin{aligned}<br>        \frac{dV}{d\epsilon}&amp;=\int^T_0\frac{\partial F}{\partial \epsilon}dt=\int^T_0\left(\frac{\partial F}{\partial y}\frac{dy}{d\epsilon}+\frac{\partial F}{\partial y’}\frac{dy’}{d\epsilon}\right)dt\<br>        &amp;=\int^T_0[F_yp(t)+F_{y’}p’(t)]dt<br>        \end{aligned}\tag{2.13}<br>$$</p>
<ul>
<li><p>Since all the partial derivatives of $F(t, y, y’)$ are, like $F$ itself, functions of $t$, $y$, and $y’$</p>
</li>
<li><p>And recalling<br>$$<br>\begin{matrix}</p>
<pre><code>    y(t)=y^&#123;*&#125;(t)+\epsilon p(t)
    &amp; \ &amp;
    y&#39;(t)=&#123;y^*&#125;&#39;(t)+\epsilon p&#39;(t)
    \end&#123;matrix&#125;\tag&#123;2.3&#125;</code></pre>
<p>$$</p>
<ul>
<li>So $y$ and $y’$ are, in turn, both functions of $\epsilon$, with derivatives<br>$$<pre><code>    \frac&#123;dy&#125;&#123;d\epsilon&#125;=p(t)\ \ \ \ and\ \ \ \ \ \ \frac&#123;dy&#39;&#125;&#123;d\epsilon&#125;=p&#39;(t)\tag&#123;4.1&#125;</code></pre>
$$</li>
</ul>
</li>
<li><p>Thus we have<br>$$</p>
<pre><code>\begin&#123;aligned&#125;
    \frac&#123;d^2V&#125;&#123;d\epsilon^2&#125;&amp;=\frac&#123;d&#125;&#123;d\epsilon&#125;\left(\frac&#123;dV&#125;&#123;d\epsilon&#125;\right)=\frac&#123;d&#125;&#123;d\epsilon&#125;\int^T_0[F_yp(t)]+F_&#123;y&#39;&#125;p&#39;(t)]dt
    \\&amp;=\int^T_0\left[p(t)\frac&#123;d&#125;&#123;d\epsilon&#125;F_y+p&#39;(t)\frac&#123;d&#125;&#123;d\epsilon&#125;F_&#123;y&#39;&#125;\right]dt
\end&#123;aligned&#125;\tag&#123;4.2&#125;</code></pre>
<p>$$</p>
<ul>
<li>In view of fact that</li>
</ul>
<p>$$<br>\frac{d}{d\epsilon}F_y=F_{yy}\frac{dy}{d\epsilon}+F_{yy’}\frac{dy’}{d\epsilon}=F_{yy}p(t)+F_{y’y}p’(t)<br>$$</p>
<ul>
<li><p>and similarly<br>$$</p>
<pre><code>\frac&#123;d&#125;&#123;d\epsilon&#125;F_&#123;y&#39;&#125;=F_&#123;yy&#39;&#125;p(t)+F_&#123;y&#39;y&#39;&#125;p&#39;(t)</code></pre>
<p>$$</p>
<ul>
<li>The second derivative (4.2)​ can be simplified as<br>$$<pre><code>    \frac&#123;d^2V&#125;&#123;d\epsilon^2&#125;=\int^T_0\left[F_&#123;yy&#125;p^2(t)+2F_&#123;yy&#39;&#125;p(t)p&#39;(t)+F_&#123;y&#39;y&#39;&#125;&#123;p&#39;&#125;^2(t)\right]dt\tag&#123;4.2&#39;&#125;</code></pre>
$$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="The-Quadratic-Form-Test"><a href="#The-Quadratic-Form-Test" class="headerlink" title="The Quadratic-Form Test"></a>The Quadratic-Form Test</h3><p>The second derivative in (4.2’)​ is a definite integral with a quadratic form as its integrand.</p>
<ul>
<li>Since $t$ spans the interval $[0, T ]$, we have, of course, not one, but an infinite number of quadratic forms in the integral.</li>
<li>Nevertheless, if it can be established that the quadratic form-with $F_{yy’}$, $F_{yy’}$, and $F_{y’y’}$ evaluated on the extremal-is negative definite for every $t$, then $\frac{d^2V}{d\epsilon^2} &lt; 0$, and the extremal maximizes $V$. </li>
<li>Similarly, positive definiteness of the quadratic form for every $t$ is sufficient for minimization of $V$.</li>
<li>Even if we can only establish sign semidefiniteness, we can at least have the second-order necessary conditions checked.</li>
</ul>
<p>For some reason, however, the quadratic-form test was totally ignored in the historical development of the classical calculus of variations.</p>
<ul>
<li>In a more recent development, however, the concavity / convexity of the integrand function $F$ is used in a sufficient condition.</li>
<li>While concavity / convexity does not per se require differentiability, it is true that if the $F$ function does possess continuous second derivatives, then its concavity / convexity can be checked by means of the sign semidefiniteness of the second-order total differential of $F$. </li>
<li>So the quadratic-form test definitely has a role to play in the calculus of variations.</li>
</ul>
<h2 id="The-Concavity-Convexity-Sufficient-Condition"><a href="#The-Concavity-Convexity-Sufficient-Condition" class="headerlink" title="The Concavity / Convexity Sufficient Condition"></a>The Concavity / Convexity Sufficient Condition</h2><h3 id="A-Sufficiency-Theorem-for-Fixed-Endpoint-Problems"><a href="#A-Sufficiency-Theorem-for-Fixed-Endpoint-Problems" class="headerlink" title="A Sufficiency Theorem for Fixed-Endpoint Problems"></a>A Sufficiency Theorem for Fixed-Endpoint Problems</h3><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[y]=\int^T_0 F[t,y(t),y’(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=Z\ &amp;(T,Z\ \mbox{given})<br>\end{array}\tag{2.1}<br>$$</p>
<p>Just as a concave / convex objective function in a static optimization prob­ lem is sufficient to identify an extremum as an absolute maximum / mini­mum, a similar sufficiency theorem holds in the calculus of variations:<br>$$<br>\begin{aligned}<br>            &amp;\mbox{For the fixed-endpoint problem } (2.1), \mbox{if the integrand function}\ F(t,y,y’)\ \mbox{is concave} \&amp;\mbox{in the variables}\ (y,y’), \mbox{then the Euler equation is sufficient for an absolute}\&amp;\mbox{maximum of}\ V[y].\mbox{ Similarly, if}\ F(t, y, y’)\ \mbox{is convex in}\ (y, y’), \mbox{then the Euler}\&amp; \mbox{equation is sufficient for an absolute minimum of}\ V[y].<br>        \end{aligned}\tag{4.3}<br>$$</p>
<ul>
<li>It should be pointed out that concavity / convexity in $(y,y’)$ means concavity / convexity in the two variables $y$ and $y’$ jointly, not in each variable separately.</li>
</ul>
<p>The proof of this theorem for the concave case.<br>$$<br>\begin{aligned}<br>            F(t,y,y’)-F(t,y^*,{y^*}’)&amp;\leq F_y(t,y^*,{y^*}’) (y-y^*)+F_{y’}(t,y^*,{y^*}’)(y’-{y^*}’)\<br>            &amp;= F_y(t,y^*,{y^*}’)\epsilon p(t)+F_{y’}(t,y^*,{y^*})’\epsilon p’(t)<br>        \end{aligned}\tag{4.4}<br>$$</p>
<ul>
<li><p>Recalling that<br>$$</p>
<pre><code>\begin&#123;aligned&#125;
\int^T_0F_&#123;y&#39;&#125;p&#39;(t)dt&amp;=[F_&#123;y&#39;&#125;p(t)]^T_0-\int^T_0 p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt\\
&amp;=-\int^T_0p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt
\end&#123;aligned&#125;\tag&#123;2.16&#125;</code></pre>
<p>$$</p>
</li>
<li><p>And the Euler Equation<br>$$</p>
<pre><code>    F_y-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;=0\ \ \ \ \ \  \mbox&#123;for all&#125;\ t\in[0,T]</code></pre>
<p>$$</p>
</li>
<li><p>Integrate both sides of (4.4) with respect to $t$ over the interval $[0,T]$<br>$$</p>
<pre><code>    \begin&#123;aligned&#125;
    V[y]-V[y^*]&amp;\leq \epsilon\int^T_0\left[F_y(t,y^*,&#123;y^*&#125;&#39;)p(t)+F_&#123;y&#39;&#125;(t,y^*,&#123;y^*&#125;&#39;)p&#39;(t)\right]dt\\
    &amp;=\epsilon \int^T_0 p(t)\left[F_y(t,y^*,&#123;y^*&#125;&#39;)-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;(t,y^*,&#123;y^*&#125;&#39;)\right]dt\\
    &amp;=0 
    \end&#123;aligned&#125;\tag&#123;4.5&#125;</code></pre>
<p>$$</p>
<ul>
<li>The first equation follows from (2.16)​, the last equation follows from the fact that $y^*(t)$ satisfies the Euler Equation ​(2.18)​</li>
<li>Note that if the $F$ function is strictly concave in $(y, y’)$, then the weak inequality $\leq$ in (4.4)​ and (4.5)​ will become the strict inequality $&lt;$. <ul>
<li>The result, $V[y] &lt; V[y^*]$, will then establish $V[y^*]$ to be a unique absolute maximum of $V$. </li>
</ul>
</li>
</ul>
</li>
<li><p>By the same token, a strictly convex $F$ will make $V[y^*]$ a unique absolute minimum.    </p>
</li>
</ul>
<h3 id="Generalization-to-Variable-Terminal-Point"><a href="#Generalization-to-Variable-Terminal-Point" class="headerlink" title="Generalization to Variable Terminal Point"></a>Generalization to Variable Terminal Point</h3><p>Recalling<br>$$<br>\begin{aligned}<br>        \int^{T}<em>0F</em>{y’}p’(t)dt&amp;=[F_{y’}p(t)]^{T}<em>0-\int^{T}<em>0 p(t)\frac{d}{dt}F</em>{y’}dt\<br>        &amp;=[F</em>{y’}]_{t={T}} p(T)-\int^{T}<em>0p(t)\frac{d}{dt}F</em>{y’}dt<br>        \end{aligned}\tag{2.16’}<br>$$</p>
<ul>
<li><p>Thus the Equation (4.5)​ becomes<br>$$<br>\begin{aligned}</p>
<pre><code>    V[y]-V[y^*]&amp;\leq \epsilon\int^T_0\left[F_y(t,y^*,&#123;y^*&#125;&#39;)p(t)+F_&#123;y&#39;&#125;(t,y^*,&#123;y^*&#125;&#39;)p&#39;(t)\right]dt\\
    &amp;=\epsilon \int^T_0 p(t)\left[F_y(t,y^*,&#123;y^*&#125;&#39;)-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;(t,y^*,&#123;y^*&#125;&#39;)\right]dt+\epsilon[F_&#123;y&#39;&#125;p(t)]_&#123;t=&#123;T&#125;&#125;\\
    &amp;=\epsilon[F_&#123;y&#39;&#125;p(t)]_&#123;t=&#123;T&#125;&#125;
    \\
    &amp;=[F_&#123;y&#39;&#125;(y-y^*)]_&#123;t=&#123;T&#125;&#125;
\end&#123;aligned&#125;\tag&#123;4.5&#39;&#125;</code></pre>
<p>$$</p>
</li>
<li><p>To ensure the maximum, $F(t, y, y’)$ in (4.3)​ only needs to be supplemented in the present case by a nonpositivity condition on the expression $[F_{y’}(y-y^*)]_{t={T}}$.</p>
<ul>
<li>But this supplementary condition is automatically met when the transversality condition is satisfied for the vertical-terminal-line problem, namely, $[F_{y’}]=0$. </li>
<li>As for the truncated case, the transversality condition calls for either  $[F_{y’}]=0$ (when the minimum acceptable terminal value is nobinding), or $y^*=y_{\min}$ (when that terminal value is binding), thereby in effect turning the problem into one with a fixed terminal point).</li>
<li>Either way, the supplementary condition is met. </li>
<li>Thus, if the integrand function $F$ is concave / convex in the variables $(y,y’)$ in a problem with a vertical terminal lime or truncated vertical terminal line, then the Euler equation plus the transversality condition are sufficient for an absolute maximum / minimum of $V[y]$.    </li>
</ul>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-03T14:17:22.000Z" title="1/3/2021, 10:17:22 PM">2021-01-03</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Transversality-Conditions-for-Variable-Endpoint-Problems/">Transversality Conditions for Variable-Endpoint Problems</a></h1><div class="content"><h2 id="The-General-Transversality-Condition"><a href="#The-General-Transversality-Condition" class="headerlink" title="The General Transversality Condition"></a>The General Transversality Condition</h2><h3 id="The-Variable-Terminal-Point-Problem"><a href="#The-Variable-Terminal-Point-Problem" class="headerlink" title="The Variable-Terminal-Point Problem"></a>The Variable-Terminal-Point Problem</h3><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[y]=\int^T_0 F[t,y(t),y’(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=y_T\ &amp;(T,y_T\ \mbox{free})<br>\end{array}\tag{3.1}<br>$$</p>
<ul>
<li><p>Using the variable $\epsilon$ to express any value of $T$ in the neighborhood of $T^*$.<br>$$</p>
<pre><code>T=T^*+\epsilon\Delta T\tag&#123;3.2&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Consider $T$ as a function of $\epsilon$, with derivative:<br>$$</p>
<pre><code>\frac&#123;dT&#125;&#123;d\epsilon&#125;=\Delta T\tag&#123;3.3&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Generating neighboring paths of the extremal $y^*(t)$ with the same $\epsilon$<br>$$<br>\begin{matrix}</p>
<pre><code>    y(t)=y^&#123;*&#125;(t)+\epsilon p(t)
    &amp; \mbox&#123; [implying&#125;&amp;
    y&#39;(t)=&#123;y^*&#125;&#39;(t)+\epsilon p&#39;(t)]
    \end&#123;matrix&#125;\tag&#123;3.4&#125;</code></pre>
<p>$$</p>
<ul>
<li>However, although the $p(t)$ curve must still satisfy the condition $p(0) = 0$, to force the neighboring paths to pass through the fixed·initial point, the other condition-$p(T)= 0$-should now be dropped, because $Y_T$ is free.</li>
</ul>
</li>
<li><p>Consider $V$ as a function of the variable $\epsilon$, the upper limit of integration in the $V$ function will also vary with $\epsilon$<br>$$<br>V(\epsilon)=\int^{T(\epsilon)}_0 F[y, y^{<em>}(t)+\epsilon p(t), {y^</em>}’(t)+\epsilon p’(t)]dt\tag{3.5}<br>$$</p>
</li>
</ul>
<h3 id="Deriving-the-General-Transversality-Condition"><a href="#Deriving-the-General-Transversality-Condition" class="headerlink" title="Deriving the General Transversality Condition"></a>Deriving the General Transversality Condition</h3><p>The <u>necessary condition</u>:<br>$$<br>\frac{dV}{d\epsilon}=\int^{T(\epsilon)}_0\frac{\partial F}{\partial \epsilon}dt+F[T,y(T),y’(T)]\frac{dT}{d\epsilon}=0\tag{3.6}<br>$$</p>
<ul>
<li><p>Recalling previous equations<br>$$</p>
<pre><code>\begin&#123;aligned&#125;
\int^&#123;T&#125;_0\frac&#123;\partial F&#125;&#123;\partial \epsilon&#125;dt&amp;=\int^&#123;T&#125;_0\left(\frac&#123;\partial F&#125;&#123;\partial y&#125;\frac&#123;dy&#125;&#123;d\epsilon&#125;+\frac&#123;\partial F&#125;&#123;\partial y&#39;&#125;\frac&#123;dy&#39;&#125;&#123;d\epsilon&#125;\right)dt\\
&amp;=\int^&#123;T&#125;_0[F_yp(t)+F_&#123;y&#39;&#125;p&#39;(t)]dt
\end&#123;aligned&#125;\tag&#123;2.13&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Breaking the integral into two parts<br>$$<br>\int^{T}<em>0 F_yp(t)dt+\int^{T}_0F</em>{y’}p’(t)dt=0\tag{2.14}<br>$$</p>
</li>
<li><p>Integration by parts<br>$$</p>
<pre><code>    \begin&#123;aligned&#125;
        \int^&#123;T&#125;_0F_&#123;y&#39;&#125;p&#39;(t)dt&amp;=[F_&#123;y&#39;&#125;p(t)]^&#123;T&#125;_0-\int^&#123;T&#125;_0 p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt\\
            &amp;=[F_&#123;y&#39;&#125;]_&#123;t=&#123;T&#125;&#125;p(T)-\int^&#123;T&#125;_0p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt
    \end&#123;aligned&#125;\tag&#123;2.16&#39;&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Thus we have<br>$$<br>\mbox{First term in } (3.6)=\int^{T}<em>0p(t)\left[F_y-\frac{d}{dt}F_{y’}\right]dt+[F</em>{y’}]_{t={T}}p(T)<br>$$</p>
</li>
<li><p>According to (3.3) we also have<br>$$<br>\mbox{Second term in } (3.6)=[F]_{t=T}\Delta T<br>$$</p>
</li>
<li><p>Then we can transform (3.6)​ into<br>$$</p>
<pre><code>\int^&#123;T&#125;_0p(t)\left[F_y-\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;\right]dt+[F_&#123;y&#39;&#125;]_&#123;t=&#123;T&#125;&#125;p(T)+[F]_&#123;t=T&#125;\Delta T =0\tag&#123;3.7&#125;</code></pre>
<p>$$</p>
</li>
</ul>
<p>Of the three terms on the left-hand side of $(3. 7)$, each contains its own independent arbitrary element: </p>
<ul>
<li>$p(t)$ (the entire perturbing curve) in the first term</li>
<li>$p(T)$ (the terminal value on the perturbing curve) in the second term</li>
<li>$\Delta T$ (the arbitrarily chosen change in $T$) in the third term<ul>
<li>Thus we cannot presume any offsetting or cancellation of terms. </li>
<li>Consequently, in order to satisfy the condition (3. 7)​, each of the three terms must individually be set equal to zero.</li>
</ul>
</li>
</ul>
<p>When the first term in (3.7)​ is set equal to zero, the Euler equation emerges.<br>$$<br>F_y-\frac{d}{dt}F_{y’}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.18}<br>$$</p>
<ul>
<li>This establish the fact that the Euler equation remains valid as a necessary condition in the variable-endpoint problem.</li>
</ul>
<p>Getting rid of the arbitrary quantity $p(T)$ by transforming it into terms of $\Delta T$ and $\Delta y_T$<br>$$<br>\Delta y(T)=p(T)+y’(T)\Delta T<br>$$</p>
<ul>
<li>This can be done with the help of Fig. 3.1. </li>
</ul>
<img src="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Transversality-Conditions-for-Variable-Endpoint-Problems/Fig_3.1.png" class title="[title]">

<ul>
<li><p>Rearranging it and get<br>$$</p>
<pre><code>p(T)=\Delta y_T-y&#39;(T)\Delta T\tag&#123;3.8&#125;</code></pre>
<p>$$</p>
<ul>
<li>The result in (3.8)​ is build on condition that $\epsilon=1$ while deriving, but it is still valid if $\epsilon$ is not set equal to one.</li>
</ul>
</li>
<li><p>Eliminating $p(T)$ in (3.7)​, and dropping the first term in ​(3.7)​, we get another <u>necessary condition</u><br>$$</p>
<pre><code>    \begin&#123;aligned&#125;
        &#123;[F_&#123;y&#39;&#125;]&#125;_&#123;t=&#123;T&#125;&#125;p(T)+[F]_&#123;t=T&#125;\Delta T&amp;=[F_&#123;y&#39;&#125;]_&#123;t=&#123;T&#125;&#125;[\Delta y_T-y&#39;(T)\Delta T]+[F]_&#123;t=T&#125;\Delta T 
        \\
        &amp;=[F_&#123;y&#39;&#125;]_&#123;t=T&#125;\Delta y_&#123;T&#125;+[F-y&#39;F_&#123;y&#39;&#125;]_&#123;t=T&#125;\Delta T
    \end&#123;aligned&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Rearranging it and get<br>$$<br>[F-y’F_{y’}]<em>{t=T}\Delta T+[F</em>{y’}]<em>{t=T}\Delta y</em>{T}=0\ \ \ \ \ \ \mbox{  [General Transversality Condition]}\tag{3.9}<br>$$</p>
<ul>
<li>This condition, unlike the Euler equation, is relevant only to one point of time, $T$.</li>
<li>Its role is to take the place of the missing terminal condition in the present problem. </li>
</ul>
</li>
</ul>
<h2 id="Specialized-Transversality-Conditions"><a href="#Specialized-Transversality-Conditions" class="headerlink" title="Specialized Transversality Conditions"></a>Specialized Transversality Conditions</h2><p>Depending on the exact specification of the terminal line or curve, however, the general condition (3.9) can be written in various specialized forms.</p>
<h3 id="Vertical-Terminal-Line"><a href="#Vertical-Terminal-Line" class="headerlink" title="Vertical Terminal Line"></a>Vertical Terminal Line</h3><p>For a fixed $T$, $\Delta T=0$, so we only need<br>$$<br>[F_{y’}]_{t=T}=0\ \ \ \ \ \ \mbox{  [Transversality Condition for Vertical Terminal Line]}\tag{3.10}<br>$$</p>
<h3 id="Horizontal-Terminal-Line"><a href="#Horizontal-Terminal-Line" class="headerlink" title="Horizontal Terminal Line"></a>Horizontal Terminal Line</h3><p>For a fixed $y_T$, $\Delta y_T=0$, so we only need<br>$$<br>[F-y’F_{y’}]_{t=T}=0\ \ \ \ \ \ \mbox{  [Transversality Condition for Hoizontal Terminal Line]}\tag{3.11}<br>$$</p>
<h3 id="Terminal-Curve"><a href="#Terminal-Curve" class="headerlink" title="Terminal Curve"></a>Terminal Curve</h3><p>With a terminal curve $y_T=\phi(T)$, we also have $\Delta y_T=\phi’\Delta T$. </p>
<ul>
<li><p>Transform (3.9)​, we have<br>$$<br>[F-y’F_{y’}+F_{y’}\phi’]_{t=T}\Delta T=0<br>$$</p>
</li>
<li><p>For an arbitrary $\Delta T$, we have</p>
</li>
</ul>
<p>$$<br>[F-y’F_{y’}+F_{y’}\phi’]_{t=T}=0\ \ \ \ \ \ \mbox{  [Transversality Condition for Terminal Curve]}\tag{3.12}<br>$$</p>
<h3 id="Truncated-Vertical-Terminal-Line"><a href="#Truncated-Vertical-Terminal-Line" class="headerlink" title="Truncated Vertical Terminal Line"></a>Truncated Vertical Terminal Line</h3><p>The usual case ofvertical terminal line, with $\Delta T =0$, specializes (3.9) to<br>$$<br>[F_{y’}]<em>{t=T}\Delta</em>{y_T}=0<br>$$</p>
<ul>
<li>When the line is truncated-restricted by the terminal condition $y_T&gt;y_{\min}$ where $y_{\min}$ is a minimum permissible level of $y$<ul>
<li>the optimal solution can have two possible types of outcome: $y_T^*&gt;y_{\min}$ or  $y_T^*=y_{\min}$ </li>
</ul>
</li>
</ul>
<p>For $y_T^*&gt;y_{\min}$, the constraint is not binding, we have<br>$$<br>[F_{y’}]<em>{t=T}=0\ \ \ for\ \ \ y_T^*&gt;y_{\min}\tag{3.14}<br>$$<br>For $y_T^*=y_{min}$, the constraint is binding, with Kuhn-Tucker condition, we have<br>$$<br>[F</em>{y’}]<em>{t=T}\leq0\ \ \ for\ \ \ y_T^*=y_{min}\tag{3.16}<br>$$<br>Combing (3.14)​ and (3.16)​ we have<br>$$<br>[F</em>{y’}]<em>{t=T}\leq0\ \ \ for\ \ \ y_T^<em>\geq y_{min}\ \ \ \ \ \ \ (y_T^</em>-y_{min})[F</em>{y’}]_{t=T}=0\   \ \ \mbox{ [Transversality Condition for Truncated Vertical Terminal Line in the Maximization Problem]}\tag{3.17}<br>$$</p>
<p>$$<br>[F_{y’}]<em>{t=T}\geq0\ \ \ for\ \ \ y_T^<em>\geq y_{min}\ \ \ \ \ \ \ (y_T^</em>-y_{min})[F</em>{y’}]_{t=T}=0\   \ \ \mbox{[Transversality Condition for Truncated Vertical Terminal Line in the Minimization Problem]}\tag{3.18}<br>$$</p>
<h2 id="Three-Generalizations"><a href="#Three-Generalizations" class="headerlink" title="Three Generalizations"></a>Three Generalizations</h2><h3 id="The-Case-of-Several-State-Variables"><a href="#The-Case-of-Several-State-Variables" class="headerlink" title="The Case of Several State Variables"></a>The Case of Several State Variables</h3><p>$$<br>\left[F-(y_1’F_{y_1’}+\cdots+y_n’F_{y_n’})\right]<em>{t=T}\Delta T+\left[F</em>{y_1’}\right]<em>{t=T}\Delta</em>{y_{1T}}+\cdots+\left[F_{y_n’}\right]<em>{t=T}\Delta y</em>{nT}=0\   \ \ \mbox{[The General (Terminal) Transversality Condition]}\tag{3.27}<br>$$</p>
<ul>
<li>When $n=2$<br>$$<br>\left[F-(y’F_{y’}+z’F_{z’})\right]<em>{t=T}\Delta T+\left[F</em>{y’}\right]<em>{t=T}\Delta</em>{y_{T}}+\left[F_{z’}\right]<em>{t=T}\Delta z</em>{T}=0\tag{3.27’}<br>$$</li>
</ul>
<h3 id="The-Case-of-Higher-Derivatives"><a href="#The-Case-of-Higher-Derivatives" class="headerlink" title="The Case of Higher Derivatives"></a>The Case of Higher Derivatives</h3><p>The general transversality condition for the case of $F(t,y’,y’’,y’’’)$ is<br>$$<br>\begin{aligned}<br>            &amp;\left[F-y’F_{y’}-y’’F_{y’’}+y’\frac{d}{dt}F_{y’’}\right]<em>{t=T}\Delta T\&amp;+\left[F</em>{y’}-\frac{d}{dt}F_{y’’}\right]<em>{t=T}\Delta y_T+[F_{y’’}]</em>{t=T}\Delta y’_{T}=0<br>        \end{aligned}\tag{3.28}<br>$$</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-03T14:17:21.000Z" title="1/3/2021, 10:17:21 PM">2021-01-03</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/The-Fundamental-Problem-of-the-Calculus-of-Variations/">The Fundamental Problem of the Calculus of Variations</a></h1><div class="content"><h2 id="The-Fundamental-Problem"><a href="#The-Fundamental-Problem" class="headerlink" title="The Fundamental Problem"></a>The Fundamental Problem</h2><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[y]=\int^T_0 F[t,y(t),y’(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=Z\ &amp;(T,Z\ \mbox{given})<br>\end{array}\tag{2.1}<br>$$</p>
<ul>
<li>The maximization and minimization problems differ from each other in the second-order conditions, but they share the same first-order conditions.</li>
</ul>
<p>The task of variational calculus is to select from a set of admissible $y$ paths (or trajectories) the one that yields an extreme value of $V[y]$.</p>
<ul>
<li><p>Since the calculus of variations is based on the classical methods of calculus, requiring the use of first and second derivatives, we shall restrict the set of admissible paths to those continuous curves with continuous derivatives.</p>
</li>
<li><p>A smooth $y$ path that yields an extremum of $V[y]$ is called an <strong>extremal</strong>.</p>
</li>
<li><p>We shall also assume that the integrand function $F$ is twice differentiable.</p>
</li>
</ul>
<h2 id="The-Euler-Equation"><a href="#The-Euler-Equation" class="headerlink" title="The Euler Equation"></a>The Euler Equation</h2><h3 id="Differentiating-a-Definite-Integral"><a href="#Differentiating-a-Definite-Integral" class="headerlink" title="Differentiating a Definite Integral"></a>Differentiating a Definite Integral</h3><p> Considering the definite integral<br>$$<br>I(x)\equiv \int^b_aF(t,x)dt<br>$$</p>
<ul>
<li>We have the Leibnniz’s Rule</li>
</ul>
<p>$$<br>\frac{dI}{dx}=\int^b_aF_x(t,x)dt<br>$$</p>
<p>Considering another definite integral<br>$$<br>K(x)\equiv\int^{b(x)}_{a(x)}F(t,x)dt<br>$$</p>
<ul>
<li>We have </li>
</ul>
<p>$$<br>\frac{dK}{dx}=\int^{b(x)}_{a(x)}F_x(t,x)dt+F[b(x),x]b’(x)-F[a(x),x]a’(x)<br>$$</p>
<h3 id="Development-of-the-Euler-Equation"><a href="#Development-of-the-Euler-Equation" class="headerlink" title="Development of the Euler Equation"></a>Development of the Euler Equation</h3><p>With reference to Fig. 2.1, let the solid path $y^*(t)$ be a known extremal. </p>
<ul>
<li>We seek to find some property of the extremal that is absent in the (nonextremal) neighboring paths. </li>
<li>Such a property would constitute a necessary condition for an extremal. </li>
<li>To do this, we need for comparison purposes a family of neighboring paths which, by specification in (2.1), must pass through the given endpoints $(0, A)$ and $(T, Z)$. </li>
<li>A simple way of generating such neighboring paths is by using a <strong>perturbing curve</strong>, chosen arbitrarily except for the restrictions that it be smooth and pass through the points $0$ and $T$ on the horizontal axis in Fig. 2.1.</li>
</ul>
<img src="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/The-Fundamental-Problem-of-the-Calculus-of-Variations/Fig_2.1.png" class title="[title]">



<p>Consider a perturbing curve:<br>$$<br>p(0)=p(T)=0\tag{2.2}<br>$$<br>Using the perturbing curve, we can generate neighboring paths of the extremal $y^*(t)$:<br>$$<br>\begin{matrix}<br>        y(t)=y^{<em>}(t)+\epsilon p(t)<br>        &amp; \mbox{ [implying}&amp;<br>        y’(t)={y^</em>}’(t)+\epsilon p’(t)]<br>        \end{matrix}\tag{2.3}<br>$$</p>
<ul>
<li>Instead of considering $V$ as a functional of $y$ path, now consider it as <u>a function of the variable $\epsilon$</u></li>
</ul>
<p>$$<br>V(\epsilon)=\int^T_0 F[y, y^{<em>}(t)+\epsilon p(t), {y^</em>}’(t)+\epsilon p’(t)]dt\tag{2.12}<br>$$</p>
<p>The <u>necessary condition</u> for the extremal is:<br>$$<br>\frac{dV}{d\epsilon}\bigg\vert_{\epsilon=0}=0\tag{2.4}<br>$$</p>
<ul>
<li><p>Then we have<br>$$<br>\begin{aligned}</p>
<pre><code>    \frac&#123;dV&#125;&#123;d\epsilon&#125;&amp;=\int^T_0\frac&#123;\partial F&#125;&#123;\partial \epsilon&#125;dt=\int^T_0\left(\frac&#123;\partial F&#125;&#123;\partial y&#125;\frac&#123;dy&#125;&#123;d\epsilon&#125;+\frac&#123;\partial F&#125;&#123;\partial y&#39;&#125;\frac&#123;dy&#39;&#125;&#123;d\epsilon&#125;\right)dt\\
    &amp;=\int^T_0[F_yp(t)+F_&#123;y&#39;&#125;p&#39;(t)]dt
    \end&#123;aligned&#125;\tag&#123;2.13&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Breaking the integral into two parts<br>$$<br>\int^T_0 F_yp(t)dt+\int^T_0F_{y’}p’(t)dt=0\tag{2.14}<br>$$</p>
<ul>
<li>While this form of necessary condition is already <u>free of the arbitrary variable</u> $\epsilon$, the arbitrary perturbing curve $p(t)$ is still present along with its derivative $p’(t)$. </li>
<li>To make the necessary condition fully operational, we must also eliminate $p(t)$ and $p’(t)$.</li>
</ul>
</li>
<li><p>Integration by parts<br>$$<br>\begin{aligned}</p>
<pre><code>        \int^T_0F_&#123;y&#39;&#125;p&#39;(t)dt&amp;=[F_&#123;y&#39;&#125;p(t)]^T_0-\int^T_0 p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt\\
        &amp;=-\int^T_0p(t)\frac&#123;d&#125;&#123;dt&#125;F_&#123;y&#39;&#125;dt
    \end&#123;aligned&#125;\tag&#123;2.16&#125;</code></pre>
<p>$$</p>
</li>
<li><p>Another version of necessary condition for the extremal:<br>$$<br>\int^T_0 p(t)\left[F_y-\frac{d}{dt}F_{y’}\right]dt=0\tag{2.17}<br>$$</p>
<ul>
<li><u>Precisely because $p(t)$ enters in an arbitrary way</u>, we may conclude that the condition $(2.17)$ can only be satisfied only if the bracketed expression is made to vanish for every value of $t$ on the extremal.</li>
</ul>
</li>
<li><p>Consequently, it is a necessary condition for an extremal that<br>$$<br>F_y-\frac{d}{dt}F_{y’}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.18}<br>$$</p>
<ul>
<li><p>Break the second-order derivative into three parts:<br>$$</p>
<pre><code>    \frac&#123;dF_&#123;y&#39;&#125;&#125;&#123;dt&#125;=\frac&#123;\partial F_&#123;y&#39;&#125;&#125;&#123;\partial t&#125;+\frac&#123;\partial F_&#123;y&#39;&#125;&#125;&#123;\partial y&#125;\frac&#123;dy&#125;&#123;dt&#125;+\frac&#123;\partial F_&#123;y&#39;&#125;&#125;&#123;\partial y&#39;&#125;\frac&#123;dy&#39;&#125;&#123;dt&#125;=F_&#123;ty&#39;&#125;+F_&#123;yy&#39;&#125;y&#39;(t)+F_&#123;y&#39;y&#39;&#125;y&#39;&#39;(t)</code></pre>
<p>$$</p>
</li>
<li><p>We get the expanded version of Euler Equation:<br>$$<br>F_{y’y’}y’’(t)+F_{yy’}y’(t)+F_{ty’}-F_y=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.19}<br>$$</p>
<ul>
<li>The Euler equation is in general a second-order nonlinear differential equation. Its general solution will thus contain two arbitrary constants.</li>
<li>Since our problem in (2.1) comes with two boundary conditions (one initial and one terminal), we should normally possess sufficient information to definitize the two arbitrary constants and obtain the definite solution.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Some-Special-Cases"><a href="#Some-Special-Cases" class="headerlink" title="Some Special Cases"></a>Some Special Cases</h2><h3 id="Special-Case-I-F-F-t-y’"><a href="#Special-Case-I-F-F-t-y’" class="headerlink" title="Special Case I: $F=F(t,y’)$"></a>Special Case I: $F=F(t,y’)$</h3><p>$$<br>F_y-\frac{d}{dt}F_{y’}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.18}<br>$$</p>
<ul>
<li>In this special case, the $F$ function is free of $y$, implying that $F_y=0$. Hence the Euler equation reduces  to $\frac{dF_{y’}}{dt}=0$, Hence the solution is<br>$$<pre><code>F_&#123;y&#39;&#125;=\mbox&#123;constant&#125;\tag&#123;2.20&#125;</code></pre>
$$</li>
</ul>
<h3 id="Special-Case-II-F-F-y-y’"><a href="#Special-Case-II-F-F-y-y’" class="headerlink" title="Special Case II: $F=F(y,y’)$"></a>Special Case II: $F=F(y,y’)$</h3><p>$$<br>F_{y’y’}y’’(t)+F_{yy’}y’(t)+F_{ty’}-F_y=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.19}<br>$$</p>
<ul>
<li><p>Since $F$ is free of $t$ in this case,  we have $F_{ty’}=0$, so the Euler equations reduces to<br>$$<br> F_{y’y’}y’’(t)+F_{yy’}y’(t)-F_y=0<br>$$</p>
</li>
<li><p>Note that<br>$$</p>
<pre><code>  \begin&#123;aligned&#125;
      \frac&#123;d&#125;&#123;dt&#125;(y&#39;F_&#123;y&#39;&#125;-F)&amp;=\frac&#123;d&#125;&#123;dt&#125;(y&#39;F_&#123;y&#39;&#125;)-\frac&#123;d&#125;&#123;dt&#125;F(y,y&#39;)\\
      &amp;=F_&#123;y&#39;&#125;&#123;y&#39;&#39;&#125;+y&#39;(F_&#123;yy&#39;&#125;y&#39;+F_&#123;y&#39;y&#39;&#125;y&#39;&#39;)-(F_yy&#39;+F_&#123;y&#39;&#125;y&#39;&#39;)\\
      &amp;=y&#39;(F_&#123;y&#39;y&#39;&#125;y&#39;&#39;+F_&#123;yy&#39;&#125;y&#39;-F_y)
  \end&#123;aligned&#125;</code></pre>
<p>$$</p>
</li>
<li><p>So the Euler equation can be written as $\frac{d}{dt}(y’F_{y’}-F)=0$, which means<br>$$<br>F-y’F_{y’}=\mbox{constant}\tag{2.21}<br>$$</p>
</li>
</ul>
<h3 id="Special-Case-III-F-F-y’"><a href="#Special-Case-III-F-F-y’" class="headerlink" title="Special Case III: $F=F(y’)$"></a>Special Case III: $F=F(y’)$</h3><p>$$<br>F_{y’y’}y’’(t)+F_{yy’}y’(t)+F_{ty’}-F_y=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.19}<br>$$</p>
<ul>
<li><p>When $F$ depends on $y’$ alone, $F_{yy’}=F_{ty’}=F_y=0$. And the Euler equation reduces to<br>$$</p>
<pre><code>F_&#123;y&#39;y&#39;&#125;y&#39;&#39;(t)=0\tag&#123;2.24&#125;</code></pre>
<p>$$</p>
</li>
<li><p>If $y’’(t)=0$, then $y’(t)=c_1$ and $y(t)=c_1t+c_2$.</p>
</li>
<li><p>If $F_{y’y’}=0$, then since $F_{y’y’}$ is, like $F$ itself, a function of $y’$ alone, the solution of $F_{y’y’}=0$ should appear as specific values pf $y’$. </p>
<ul>
<li>Suppose there are one or more real solutions $y’=k_i$, then we can deduce that $y=k_it+c$, which again represents a family of straight lines.</li>
<li>Consequently, given an integrand function that depends on $y’$ alone, we can always take its extermal to be a straight line. </li>
</ul>
</li>
</ul>
<h3 id="Special-Case-IV-F-F-t-y"><a href="#Special-Case-IV-F-F-t-y" class="headerlink" title="Special Case IV: $F=F(t,y)$"></a>Special Case IV: $F=F(t,y)$</h3><p>$$<br>F_y-\frac{d}{dt}F_{y’}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.18}<br>$$</p>
<ul>
<li><p>Since $y’$ is missing from the $F$ function, we have $F_{y’}=0$, the Euler equation reduces to<br>$$</p>
<pre><code>    F_y(t,y)=0</code></pre>
<p>$$</p>
</li>
<li><p>The fact that the derivative $y’$ does not appear in this equation means that the Euler equation is not a differential equation.</p>
</li>
<li><p>Since there are no arbitrary constraints in its solution to be definitized in accordance with the given boundary conditions, the extremal may not satisfy the boundary conditions except by sheer coincidence.</p>
</li>
<li><p>Thus, $F(t,y)$ is, in a special sense, “linear” in $y’$.</p>
</li>
</ul>
<h2 id="Two-generalizations-of-the-Euler-Euqation"><a href="#Two-generalizations-of-the-Euler-Euqation" class="headerlink" title="Two generalizations of the Euler Euqation"></a>Two generalizations of the Euler Euqation</h2><h3 id="The-Case-of-Several-State-Variables"><a href="#The-Case-of-Several-State-Variables" class="headerlink" title="The Case of Several State Variables"></a>The Case of Several State Variables</h3><p>The objective functional with $n&gt;1$ state variables<br>$$<br>V[y_1,…,y_n]=\int^T_0 F(t,y_1,…,y_n,y_1’,…,y_n’)dt\tag{2.26}<br>$$</p>
<ul>
<li><p>Then we have<br>$$<br>F_{y_j}-\frac{d}{dt}F_{y’_j}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler Equation]}\tag{2.27}<br>$$</p>
<ul>
<li>When $n=2$, we have<br>$$<br>\begin{aligned}<pre><code>    F_&#123;y&#39;y&#39;&#125;y&#39;&#39;(t)+F_&#123;z&#39;y&#39;&#125;z&#39;&#39;(t)+F_&#123;yy&#39;&#125;y&#39;(t)+F_&#123;zy&#39;&#125;z&#39;(t)+F_&#123;ty&#39;&#125;-F_y=0\\
    F_&#123;y&#39;z&#39;&#125;y&#39;&#39;(t)+F_&#123;z&#39;z&#39;&#125;z&#39;&#39;(t)+F_&#123;yz&#39;&#125;y&#39;(t)+F_&#123;zz&#39;&#125;z&#39;(t)+F_&#123;tz&#39;&#125;-F_z=0\\&amp; \mbox&#123;for all&#125;\ t\in[0,T]
    \end&#123;aligned&#125;\tag&#123;2.28&#125;</code></pre>
$$</li>
</ul>
</li>
</ul>
<h3 id="The-Case-of-Higher-Order-Derivatives"><a href="#The-Case-of-Higher-Order-Derivatives" class="headerlink" title="The Case of Higher-Order Derivatives"></a>The Case of Higher-Order Derivatives</h3><p>The objective functional contains high-order derivatives<br>$$<br>V[y]=\int^T_0 F(t,y,y’’,…,y^{(n)})dt\tag{2.29}<br>$$</p>
<ul>
<li>Then we have<br>$$<br>F_y-\frac{d}{dt}F_{y’}+\frac{d^2}{dt^2}F_{y’}-\cdots+(-1)^n\frac{d^n}{dt^n}F_{y^{(n)}}=0\ \ \ \ \ \  \mbox{for all}\ t\in[0,T]\ \ \ \ \ \ \mbox{  [Euler-Poisson Equation]}    \tag{2.30}<br>$$</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-01-02T14:17:12.000Z" title="1/2/2021, 10:17:12 PM">2021-01-02</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-A-Preliminaries-of-Dynamic-Optimization/">1.4.A Preliminaries of Dynamic Optimization</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/">The Nature of Dynamic Optimization</a></h1><div class="content"><h2 id="Salient-Features-of-Dynamic-Optimization-Problems"><a href="#Salient-Features-of-Dynamic-Optimization-Problems" class="headerlink" title="Salient Features of Dynamic Optimization Problems"></a>Salient Features of Dynamic Optimization Problems</h2><p>Dynamic optimization can be viewed as a problem of multistage decision making.</p>
<h3 id="The-Discrete-Variable-Version"><a href="#The-Discrete-Variable-Version" class="headerlink" title="The Discrete-Variable Version"></a>The Discrete-Variable Version</h3><p>Suppose that a firm engages in transforming a certain substance from an initial state $A$ (raw material state) into a termi­nal state $Z$ (finished product state) through a five-stage production process. </p>
<ul>
<li>In every stage, the firm faces the problem of choosing among several possible alternative subprocesses, each entailing a specific cost. </li>
<li>The ques­tion is: How should the firm select the sequence of subprocesses through the five stages in order to minimize the total cost?</li>
</ul>
<p>In Fig. 1.1, our problem is transformed to choose a connected sequence of arcs going from left to right, starting at $A$ and terminating at $Z$, such that the sum of the values of the component arcs is minimized. </p>
<img src="/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/Fig_1.1.png" class title="[title]">



<ul>
<li>The optimal solution for the present example is the path $ACEHJZ$, with $14 as the minimum cost of production. </li>
</ul>
<h3 id="The-Continuous-Variable-Version"><a href="#The-Continuous-Variable-Version" class="headerlink" title="The Continuous-Variable Version"></a>The Continuous-Variable Version</h3><p>We can visualize Fig. 1.2 to be a map of an open terrain, with the stage variable representing the longitude, and the state variable representing the latitude. </p>
<ul>
<li>Our assigned task is to transport a load of cargo from location $A$ to location $Z$ at minimum cost by selecting an appropriate travel path. </li>
<li>The cost associated with each possible path de­pends, in general, not only on the distance traveled, but also on the topography on that path. </li>
</ul>
<img src="/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/Fig_1.2.png" class title="[title]">

<p>Regardless of whether the variables are discrete or continuous, a simple type of dynamic optimization problem would contain the following basic ingredients:</p>
<ol>
<li>a given <strong>initial point</strong> and a given <strong>terminal point</strong>;</li>
<li>a set of <strong>admissible paths</strong> from the initial point to the terminal point;</li>
<li>a set of <strong>path values</strong> serving as performance indices (cost, profit, etc.) associated with the various paths; and</li>
<li>a specified objective-either to maximize or to minimize the path value or performance index by choosing the <strong>optimal path</strong>.<ul>
<li>For most of the problems, the stage variable will represent time; then the optimal path is actually optimal <strong>time path</strong>.</li>
</ul>
</li>
</ol>
<h3 id="The-Concept-of-a-Functional"><a href="#The-Concept-of-a-Functional" class="headerlink" title="The Concept of a Functional"></a>The Concept of a Functional</h3><p>The relationship between paths and path values represents a special sort of mapping</p>
<ul>
<li>not a mapping from real numbers to real numbers as in the usual function</li>
<li>but a mapping from <u>paths (curves)</u> to <u>real numbers</u> (performance indices). </li>
</ul>
<p>Let us denote time paths by $y_I(t), y_{II}(t)$, and so on. Then the mapping is as shown in Fig. 1.3, where $V_I$, $V_{II}$ represent the associated path values. </p>
<ul>
<li>The general notation for the mapping should there­ fore be $V[y(t)]$. But this symbol fundamentally differs from the composite-function symbol $g[f(x)]$. </li>
<li>In the latter, $g$ is a function of $f$, and $f$ is in turn a function of $x$; thus, $g$ is in the final analysis a function of $x$. </li>
<li>In the symbol $V[y(t)]$, on the other hand, the $y(t)$ component comes as an integral unit-to indicate time paths-and there­fore $V$ is actually a <strong>functional</strong> of $y(t)$</li>
</ul>
<img src="/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/Fig_1.3.png" class title="[title]">

<h2 id="Variable-Endpoints-and-Transversality-Conditions"><a href="#Variable-Endpoints-and-Transversality-Conditions" class="headerlink" title="Variable Endpoints and Transversality Conditions"></a>Variable Endpoints and Transversality Conditions</h2><h3 id="Types-of-Variable-Terminal-Points"><a href="#Types-of-Variable-Terminal-Points" class="headerlink" title="Types of Variable Terminal Points"></a>Types of Variable Terminal Points</h3><p>In Fig. 1.5a, while the planning horizon is fixed at time $T$, any point on the vertical line $t = T$ is acceptable as a terminal point, such as $Z_1$, $Z_2$, and $Z_3$• </p>
<ul>
<li>This type of problem is commonly referred to in the literature as a <strong>fixed-time-horizon problem</strong>, <strong>fixed-time problem</strong>, or the <strong>vertical-terminal-line</strong> problem, meaning that the termi­nal time of the problem is fixed rather than free. </li>
</ul>
<p>In Fig. 1.5b, the horizontal line $y = Z$ constitutes the set of admissible terminal points. Each of these, depending on the path chosen, may be associated with a different terminal time, as exemplified by $T_1$, $T_2$, and $T_3$•</p>
<ul>
<li>This type of problem is commonly referred to as a <strong>fixed-endpoint problem</strong>, or the <strong>horizontal-terminal-line problem</strong>.</li>
</ul>
<p>In the third type, neither the termi­nal time $T$ nor the terminal state $Z$ is individually preset, but the two are tied together via a constraint equation of the form $Z = \phi(T)$, as illustrated in Fig. 1.5c.</p>
<ul>
<li>We shall call this type of problem the <strong>terminal-curve</strong> (or <strong>terminal-surface</strong>) <strong>problem</strong> .</li>
</ul>
<img src="/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/Fig_1.5.png" class title="[title]">

<h3 id="Transversality-Condition"><a href="#Transversality-Condition" class="headerlink" title="Transversality Condition"></a>Transversality Condition</h3><p>The common feature of <strong>variable-terminal-point</strong> problems is that the planner has <u>one more degree of freedom</u> than in the <strong>fixed-terminal-point</strong> case. </p>
<ul>
<li><p>But this fact automatically implies that, in deriving the optimal solution, <u>an extra condition is needed to pinpoint the exact path chosen</u>. </p>
</li>
<li><p>In the former, the optimal path must satisfy the boundary (initial and terminal) conditions<br>$$<br>y(0) =A\mbox{ and }y(T)=Z\ \ \ \  (T,A, \mbox{ and }Z \mbox{ all given})<br>$$</p>
</li>
<li><p>In the latter case, the initial condition $y(O)=A$ still applies by assumption.</p>
</li>
<li><p>But since $T$ and / or $Z$ are now variable, the terminal condition $y(T) = Z$ is no longer capable of pinpointing the optimal path for us. </p>
</li>
<li><p>As Fig. 1.5 shows, all admissible paths, ending at $Z_1, Z_2$, or other possible terminal positions, equally satisfy the condition $y(T) = Z$.</p>
</li>
</ul>
<p>What is needed, therefore, is a terminal condition that can conclusively distinguish the optimal path from the other admissible paths. </p>
<ul>
<li>Such a condition is referred to as a <strong>transversal­ity condition</strong>, because it normally appears as a description of <u>how the optimal path crosses the terminal line or the terminal curve</u> (to “transverse” means to “to go across”).</li>
</ul>
<h2 id="The-Objective-Functional"><a href="#The-Objective-Functional" class="headerlink" title="The Objective Functional"></a>The Objective Functional</h2><h3 id="The-Integral-Form-of-Functional"><a href="#The-Integral-Form-of-Functional" class="headerlink" title="The Integral Form of Functional"></a>The Integral Form of Functional</h3><p><strong>The standard form with one state variable</strong>:<br>$$<br>V[y]=\int^T_0 F[t,y(t),y’(t)]dt<br>$$</p>
<p><strong>The standard form with two state variables</strong>:<br>$$<br>V[y,z]=\int^T_0 F[t,y(t),z(t),y’(t),z’(t)]dt<br>$$</p>
<h3 id="Other-Forms-of-Functional"><a href="#Other-Forms-of-Functional" class="headerlink" title="Other Forms of Functional"></a>Other Forms of Functional</h3><p><strong>Rely exclusively on the terminal point with one state variable</strong>:</p>
<ul>
<li>Problem of Mayer:</li>
</ul>
<p>$$<br>V[y]=G[T,y(T)]<br>$$</p>
<p><strong>Rely both on path values and the terminal point</strong>:</p>
<ul>
<li>Problem of Bolza:</li>
</ul>
<p>$$<br>V[y]=\int^T_0 F[t,y(t),y’(t)]dt+G[T,y(T)]<br>$$</p>
<p>Although the problem of Bolza may seem to be the more general formulation, the truth is that the three types of problems-standard, Mayer, and Bolza-are all convertible into one another.</p>
<p><strong>Rely exclusively on the terminal point with two state variables</strong>:</p>
<ul>
<li>Problem of Mayer with two state variables</li>
</ul>
<p>$$<br>V[y,z]=G[T, y(T),z(T)]<br>$$</p>
<h2 id="Alternative-Approaches-to-Dynamic-Optimization"><a href="#Alternative-Approaches-to-Dynamic-Optimization" class="headerlink" title="Alternative Approaches to Dynamic Optimization"></a>Alternative Approaches to Dynamic Optimization</h2><h3 id="The-Calculus-of-Variations"><a href="#The-Calculus-of-Variations" class="headerlink" title="The Calculus of Variations"></a>The Calculus of Variations</h3><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[y]=\int^T_0 F[t,y(t),y’(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=Z\ &amp;(T,Z\ \mbox{given})<br>\end{array}<br>$$</p>
<p>In order to make such problems meaningful, it is necessary that the functional be <u>integrable</u> (i.e., the integral must be convergent).</p>
<p>Furthermore, we shall assume that all the functions that appear in the problem are <u>continuous</u> and <u>continuously differentiable</u>.</p>
<ul>
<li>This assumption is needed because the basic methodology underlying the calcu­lus of variations closely parallels that of the classical differential calculus.<ul>
<li>The main difference is that, instead of dealing with the differential $dx$ that changes the value of $y=f(x)$, we will now deal with <strong>the</strong> <strong>“variation” of an entire curve $y(t)$ that affects the value of the functional $V[y]$.</strong> </li>
</ul>
</li>
</ul>
<h3 id="Optimal-Control-Theory"><a href="#Optimal-Control-Theory" class="headerlink" title="Optimal Control Theory"></a>Optimal Control Theory</h3><p>$$<br>\begin{array}{lll}<br>                \mbox{Maximize or Minimize} &amp;V[u]=\int^T_0 F[t,y(t),u(t)]dt\<br>                \<br>                \mbox{subject to}&amp; y’(t)=f[t,y(t),u(t))]\<br>                \<br>                &amp;y(0)=A\ &amp;(A\ \mbox{given})\<br>                \<br>                \mbox{and }&amp;y(T)=Z\ &amp;(T,Z\ \mbox{given})<br>\end{array}<br>$$</p>
<ul>
<li>Additional constraint:<br>$$<br>u(t)\in\mathscr{U}\mbox{ for }0\leq t\leq T<br>$$</li>
</ul>
<p>Aside from the time variable $t$ and the state variable $y(t)$, consideration is given to a control variable $u(t)$. </p>
<ul>
<li>Indeed, it is the latter type of variable that gives optimal control theory its name and occupies the center of stage in this new approach to dynamic optimiza­tion.</li>
</ul>
<p>To <u>focus attention on the control variable</u> implies that <u>the state variable is relegated to a secondary status</u>.</p>
<ul>
<li><p>This would be acceptable only if the decision on a control path $u(t)$ will, once given an initial condition on $y$, unambiguously determine a state-variable path $y(t)$ as a by-product. </p>
</li>
<li><p>For this reason, an optimal control problem must contain an equation that relates $y$ to $u$:<br>$$<br>y’(t)=f[t,y(t),u(t))]<br>$$</p>
<ul>
<li>Such an equation, called an <strong>equation of motion</strong> (or <strong>transition equation</strong> or <strong>state equation</strong>), shows how, at any moment of time, given the value of the state variable, the planner’s choice of $u$ will drive the state variable y over time.</li>
<li>Once we have found the optimal control-variable path $u^*(t)$, the equation of motion would make it possible to construct the related optimal state-variable path $y^*(t)$.</li>
</ul>
</li>
</ul>
<h3 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming:"></a>Dynamic Programming:</h3><p>The most important distinguishing characteristics of this approach are two: </p>
<ul>
<li>First, it <u>embeds the given control problem in a family of control problems</u>, with the consequence that in solving the given problem, we are actually solving the entire family of problems. </li>
<li>Second, for each member of this family of problems, <u>primary attention is focused on the optimal value of the functional</u>, $V^*$, rather than on the properties of the optimal state path $y^*(t)$ (as in the calculus of variations) or the optimal control path $u^*(t)$ (as in optimal control theory).</li>
</ul>
<p>Referring to Fig. 1.6 (adapted from Fig. 1.1), given the original problem of finding the least-cost path from point $A$ to point $Z$, we consider the larger problem of finding the least-cost path from each point in the set ${A, B, C, . . . , Z}$ to the terminal point $Z$. </p>
<ul>
<li><p>Since every component problem has a unique optimal path value, it is possible to write an <strong>optimal value function</strong><br>$$<br>V^* = V^*(i)\ \ \ \  ( i = A, B, .. . , Z)<br>$$</p>
<ul>
<li>which says that we can determine an optimal path value for every possible initial point.</li>
</ul>
</li>
<li><p>From this, we can also construct an <strong>optimal policy function</strong>, which will tell us how best to proceed from any specific initial point $i$, in order to attain $V^*(i)$ by the proper selection of a sequence of arcs leading from point $i$ to the terminal point $Z$.</p>
</li>
</ul>
<img src="/2021/01/02/Mathematics/Dynamic%20Optimization/1%20Preliminaries%20of%20Dynamic%20Optimization/The-Nature-of-Dynamic-Optimization/Fig_1.6.png" class title="[title]">

<p>Returning to Fig. 1.6, imagine that our immediate problem is merely that of determining the optimal values for stage 5, associated with the three initial points $I$, $J$, and $K$. </p>
<ul>
<li><p>The answer is easily seen to be<br>$$<br>V^*(I) = 3,\ V^*(J) = 1,\ V^*(K) =2\tag{1.10}<br>$$</p>
</li>
<li><p>Having found the optimal values for $I$, $J$, and $K$, the task of finding the least-cost values $V^*(G)$ and $V^*(H)$ becomes easier. </p>
</li>
<li><p>Moving back to stage 4 and utilizing the previously obtained optimal-value information in (1.10), we can determine $V^*(G)$ as well as the optimal path $GZ$ (from $G$ to $Z$) as follows:</p>
</li>
</ul>
<p>$$<br>\begin{array}{}</p>
<p>V^*(G)<br>&amp;= \min{\mbox{value of arc }GI + V^*(I),\mbox{ value of arc }GJ + V^*(J)}<br>\<br>&amp;=\min{2+ 3,8+1}=5 \mbox{ [The optimal path GZ is GIZ.]}<br>\end{array}\tag{1.11}<br>$$</p>
<ul>
<li><p>By the same token, we find<br>$$<br>\begin{array}{}</p>
<p>V^*(H)<br>&amp;= \min{\mbox{value of arc }HJ + V^*(J),\mbox{ value of arc }HK + V^*(K)}<br>\<br>&amp;=\min{4+ 1,6+2}=5 \mbox{ [The optimal path HZ is HJZ.]}<br>\end{array}\tag{1.12}<br>$$</p>
</li>
</ul>
<p>With the knowledge of $V^*(G)$ and $V^*(H)$, we can then move back one more stage to calculate $V^*(D), V^*(E)$, and $V^*(F)$-and the optimal paths $DZ, EZ$, and $FZ$-in a similar manner. </p>
<ul>
<li>And, with two more such steps, we will be back to stage 1, where we can determine $V^*(A)$ and the optimal path $AZ$, that is, solve the original given problem.</li>
</ul>
<p>The essence of the iterative solution procedure is captured in <strong>Bellman’s principle of optimality</strong>, which states, roughly, that if you chop off the first arc from an optimal sequence of arcs, the remaining abridged sequence must still be optimal in its own right-as an optimal path from its own initial point to the terminal point. </p>
<ul>
<li>If $EHJZ$ is the optimal path from $E$ to $Z$, for example, then $HJZ$ must be the optimal path from $H$ to $Z$. </li>
<li>Conversely, if $HJZ$ is already known to be the optimal path from $H$ to $Z$, then a longer optimal path that passes through $H$ must use the sequence $HJZ$ at the tail end. <ul>
<li>This reasoning is behind the calculations in (1.11) and (1.12). </li>
<li>But note that in order to apply the principle of optimality and the iterative procedure to delineate the optimal path from $A$ to $Z$, we must find the optimal value associated with every possible point in Fig. 1.6. </li>
<li>This explains why we must embed the original problem.</li>
</ul>
</li>
</ul>
<p>Even though the essence of dynamic programming is sufficiently clari­fied by the discrete example in Fig. 1.6, the full version of dynamic program­ ming includes the continuous-time case. </p>
<ul>
<li>Unfortunately, the solution of continuous-time problems of dynamic programming involves the more ad­vanced mathematical topic of partial differential equations. </li>
<li>Besides, partial differential equations <u>often do not yield analytical solutions</u>. </li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Alpha C. Chiang, 1992, “Elements of Dynamic Optimization”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-24T14:52:59.000Z" title="12/24/2020, 10:52:59 PM">2020-12-24</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-2-Algebra/">1.2 Algebra</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-2-Algebra/1-2-B-Linear-Algebra/">1.2.B Linear Algebra</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/24/Mathematics/Algebra/2%20Linear%20Algebra/Geometric-Linear-Algebra/">Geometric Linear Algebra</a></h1><div class="content"><h2 id="Vectors"><a href="#Vectors" class="headerlink" title="Vectors"></a>Vectors</h2><p>In physics, the vectors can be arrows in the space.</p>
<p>In computer science, the vectors can be ordered list of numbers.</p>
<p>Mathematicians consider a more general definition form of vectors.</p>
<ul>
<li>Anything where there is a sensible notion of <strong>addition</strong> and <strong>scalar multiplication</strong> can be viewed as vectors.</li>
</ul>
<p>Consider vectors as arrows with its tails sitting at the origin, the coordinates of vectors tells how to get from the tails of vectors to its tip.</p>
<ul>
<li>The addition of vectors is moving along the sum of vectors</li>
<li>The scalar multiplication of vectors is scaling the vectors.</li>
</ul>
<p>Consider vectors as ordered lists of numbers, each component of ordered lists of numbers describe how to move parallel to the axes, getting from the tails of vectors to its tip.</p>
<ul>
<li>The addition of vectors is matching up their trems and add each one together.</li>
<li>The scalar muliplication of vectors is multiplying each one of their component by that scalar.</li>
</ul>
<p>We usually think of individual vectors as arrows, and think of a set of vectors as points.</p>
<h2 id="Linear-Combinations"><a href="#Linear-Combinations" class="headerlink" title="Linear Combinations"></a>Linear Combinations</h2><p>The <strong>linear combination</strong> refers to the vector as an <strong>operation result</strong> of <strong>addition</strong> and <strong>scalar multiplication</strong> between multiple vectors.</p>
<p><strong><font color=941751>For an arbitrary vector, the vector this coordinate describe is actually the sum of multiple scaled vectors / bases of vectors / unit vectors.</font></strong></p>
<ul>
<li><strong><font color=941751>The coordinate as scalars describe how each one stretches and squishes vectors / bases of vectors / unit vectors.</font></strong></li>
<li>As a vector, it also has the orginal meaning like the arrow movement or the ordered list of numbers.</li>
<li>Any linear combination is relevant to its basis vectors.</li>
</ul>
<p>How to understand linearity?</p>
<ul>
<li>In 2-D linear spaces, fix one of these scalars, and let the other one change its value freely, the tip of resulting vector draws a straight line.</li>
</ul>
<p>The <strong>span</strong> of vectors refers to what are all the possible vectors we can reach using only these (two) vectors.</p>
<ul>
<li><strong>Linear dependent</strong> <ul>
<li>Whenever we have multiple vectors and we can remove one without reducing the span.</li>
<li>One of these vectors could be represented as a linear combination of others, since it is already in the span of the others.</li>
</ul>
</li>
<li><strong>Linear independent</strong><ul>
<li>Each vector add a new dimension to the span.</li>
</ul>
</li>
</ul>
<h2 id="Matrix-as-Linear-Transformations"><a href="#Matrix-as-Linear-Transformations" class="headerlink" title="Matrix as Linear Transformations"></a>Matrix as Linear Transformations</h2><p><strong>Linear transformation</strong> is a function from a linear space to another linear space.</p>
<ul>
<li><p>Linear transformation is not called as function because <strong>it is to be suggestive of a way to visualize the input-output relation</strong>, that is, the movement – move the input vector to output vector.</p>
</li>
<li><p>Linear transformation is also called the <strong>linear operator</strong>, since it can be represented by matrix multiplication, which is an binary operation defined on an algebra.</p>
</li>
<li><p><strong>Linear function</strong> / Linear funtional refers to a real map which maps a linear space to $\mathbb{R}$.</p>
</li>
</ul>
<p>The movement decribed by the linear transformation has two characteristics.</p>
<ol>
<li>All lines must remain lines, without getting curved.</li>
<li>The origin remain fixed.<ul>
<li>The affine transformation is a movement where the origin moves.</li>
</ul>
</li>
</ol>
<p>In other words, the linear transformation <strong>keeps grid lines paraller and evenly spaced</strong>.</p>
<p>Consider taking every possible input vector to some output vector, it is equivalent to consider every point in space move to some other point.</p>
<p>How to decribe one of these linear transformations numerically?</p>
<ul>
<li>Only need to record where the (two) basis vectors each land.</li>
</ul>
<p>Consider a linear transformation<br>$$<br>\mathbf{i}=\left[\begin{matrix}1 \\ 0\end{matrix}\right]\to L(\mathbf{i})=\left[\begin{matrix}1 \\ -2\end{matrix}\right]<br>$$</p>
<p>$$<br>\mathbf{j}=\left[\begin{matrix}0 \\ 1\end{matrix}\right]\to L(\mathbf{j})=\left[\begin{matrix}3 \\ 0\end{matrix}\right]<br>$$</p>
<p>Since every vector can be decribed as the sum of scaled bases of vectors. Since the linear transfomation moves the basis vectors, then the vector is the sum of scaled new bases of vectors. </p>
<p>let’s write this linear transformation in a linear combination form.<br>$$<br>\left[\begin{matrix}x \\ y\end{matrix}\right]\to L(\left[\begin{matrix}x \\ y\end{matrix}\right])=x\left[\begin{matrix}1 \\ -2\end{matrix}\right]+y\left[\begin{matrix}3 \\ 0\end{matrix}\right]=\left[\begin{matrix}1x+3y \\ -2x+0y\end{matrix}\right]<br>$$<br>Intuitively, we can write it as a matrix multiplication form for convenience<br>$$<br>\left[\begin{matrix}x \\ y\end{matrix}\right]\to L(\left[\begin{matrix}x \\ y\end{matrix}\right])=\left[\begin{matrix}L(\mathbf{i}) &amp;  L(\mathbf{j})\end{matrix}\right]\left[\begin{matrix}x \\ y\end{matrix}\right]=\left[\begin{matrix}1 &amp; 3\\ -2 &amp; 0\end{matrix}\right]\left[\begin{matrix}x \\ y\end{matrix}\right]=\left[\begin{matrix}1x+3y \\ -2x+0y\end{matrix}\right]<br>$$<br>In conclusion, linear transformation can be decribed using only the coordinates of where each basis vector lands.</p>
<ul>
<li>Matrices give us a language to describe these transformation where the column represent those coordinates.</li>
<li>Any metrix-vector multiplication is just a way to compute what that trnsformation does to a given vector.</li>
<li><strong><font color=941751>Every matrix can be interpreted as a certain transformation of space.</font></strong></li>
</ul>
<h2 id="Matrix-Multiplication-as-Composition"><a href="#Matrix-Multiplication-as-Composition" class="headerlink" title="Matrix Multiplication as Composition"></a>Matrix Multiplication as Composition</h2><p>Matrix multiplcation is a <strong>compostion</strong> of linear transformation.</p>
<p>Multipliy two matrices has the geometric meaning of applying one transformation then another.</p>
<h2 id="The-Determinant"><a href="#The-Determinant" class="headerlink" title="The Determinant"></a>The Determinant</h2><p>The linear transformation is accompanied by streching or squishing space. </p>
<p>To measure exactly how much are things being scretched is equivalent to measure how much are areas scaled.</p>
<p>Similarly, we only need to consider the unit square, take 2-D linear space as an example.</p>
<p>The very special scaling vector, by whcih a linear transformation changes any area are called the <strong>determinant</strong> of a transformation.</p>
<ul>
<li><p>The determinant of a 2-D transformation is 0, if it squishes all sapce onto a line or even onto a single point.</p>
</li>
<li><p>Negative determinant imlies orientation-flipping.</p>
</li>
</ul>
<h2 id="Linear-Equation-Systems"><a href="#Linear-Equation-Systems" class="headerlink" title="Linear Equation Systems"></a>Linear Equation Systems</h2><p>Any linear equation system can be written as the form $\mathbf{Ax}=\mathbf{y}$, it is equivalent to find a vector $\mathbf{x}$ after linear transformation $\mathbf{A}$ lands on $\mathbf{y}$. </p>
<ul>
<li>To solve the linear equation system, we need to find a inverse linear transformation, which is to find the inverse matrix $\mathbf{A}^{-1}$.</li>
</ul>
<p>If the determinant of $\mathbf{A}$ doesn’t equal to 0, then we can find a unique $\mathbf{x}$, which is the unique solution, implying the existence of $\mathbf{A}^{-1}$.</p>
<p>If the determinant of $\mathbf{A}$ equal to 0, for a 2-D linear space, it implies this linear transformation squish the whole space into a line, the solution exists iff $\mathbf{y}$ lies on this line. </p>
<ul>
<li>$\mathbf{A}^{-1}$ doesn’t exist if the determinant of $\mathbf{A}$ equal to 0, because we cannot find a inverse linear transformation map a low dimensional space to a high dimensional space, since the linear transformation is a function. <ul>
<li>For example, we cannot map a point into a line.</li>
</ul>
</li>
<li>Let’s review the definition of function:<ul>
<li>By a <strong><font color=0096FF>function / map</font></strong> $f$ that maps $X$ into $Y$, denoted as $f:X→Y$, we mean a relation $f∈X×Y$ such that<ol>
<li>For every $x∈X$, there exists a $y∈Y$ such that $xfy$,</li>
<li>For every $y,z∈Y$ with $xfy$ and $xfz$, we have $y=z$.</li>
</ol>
</li>
<li>The second condition implies we cannot unsquish a linear space.</li>
</ul>
</li>
</ul>
<p>The <strong>rank</strong> of a matrix is the number of the dimension in the output vector.</p>
<ul>
<li>Since the columns tell us where the basis vector lands after the transformation.<ul>
<li>The set of all possible outputs of $\mathbf{Ax}$ is the span of column vectors, called the <strong>column space</strong> of $\mathbf{A}$</li>
<li>Rank is actually the dimension of the column space.<ul>
<li>$\mathbf{0}$ must lie in the column space since the origin doesn’t change in the linear transformation.</li>
<li>For the full rank matrix, only $\mathbf{0}$ in the input space is mapped into $\mathbf{0}$.</li>
<li>For the non-full rank matrix, multiple vector lands in $\mathbf{0}$ finally, due to the squishing.<ul>
<li>These vectors are called the <strong>null space</strong> or the <strong>kernel</strong> of the matrix.</li>
<li>The <strong>null space</strong> gives all possible solution to $\mathbf{Ax=0}$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Eigenvectors-and-Eigenvalues"><a href="#Eigenvectors-and-Eigenvalues" class="headerlink" title="Eigenvectors and Eigenvalues"></a>Eigenvectors and Eigenvalues</h2><p>The <strong>eigenvectors</strong> refer to vectors which are not knocked off from its span, or still remain in its span after the linear transformation.</p>
<p>The <strong>eigenvalues</strong> refer to how much the eigenvectors being screched or squished after the liner transformation.</p>
<p>This can be represented as equations.<br>$$<br>\mathbf{Av}=\lambda\mathbf{v}\Longleftrightarrow(\mathbf{A}-\lambda\mathbf{I})\mathbf{v}=\mathbf{0}<br>$$<br>We are interested in non-zero eigenvectors $\mathbf{v}$, which requires $\mbox{det}(\mathbf{A}-\lambda\mathbf{I})=0$.</p>
<ul>
<li>It means $\mathbf{A}-\lambda\mathbf{I}$ squish non-zero vectors onto $\mathbf{0}$</li>
<li>Otherwise the null space, which is the solution of $(\mathbf{A}-\lambda\mathbf{I})\mathbf{v}=\mathbf{0}$ is unique $\mathbf{0}$.</li>
</ul>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.3blue1brown.com/">https://www.3blue1brown.com</a></li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-24T14:52:00.000Z" title="12/24/2020, 10:52:00 PM">2020-12-24</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-2-Algebra/">1.2 Algebra</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-2-Algebra/1-2-A-Abstract-Algebra/">1.2.A Abstract Algebra</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/24/Mathematics/Algebra/1%20Abstract%20Algebra/Elements-of-Abstract-Algebra/">Elements of Abstract Algebra</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Binary Operation</strong></p>
<ul>
<li>A <strong>binary operation</strong> on a set $G$ is a function </li>
</ul>
<p>$$<br>∗ : G ×G → G<br>$$</p>
<blockquote>
<p>As with any function, a binary operation is well-defined; when one says this explicitly, it is usually called the <strong>law of substitution</strong>:</p>
<p>If $x = x’$ and $y = y’$, then $x ∗ y = x’ ∗ y’$.</p>
</blockquote>
<p><strong>Group</strong></p>
<ul>
<li>A <strong>group</strong> is a set $G$ equipped with a binary operation $∗$ such that<ol>
<li>the <strong>associative law</strong> holds: for every $x, y, z ∈ G$, $x ∗ (y ∗ z) = (x ∗ y) ∗ z$</li>
<li>there is an element $e∈G$, called the <strong>identity</strong>, with $e∗x=x=x∗e$ for all $x∈G$</li>
<li>every $x∈G$ has an <strong>inverse</strong>; there is $x’∈G$ with $x∗x’=e=x’∗x$.</li>
</ol>
</li>
</ul>
<p><strong>Abelian Group</strong></p>
<ul>
<li>A group $G$ is called <strong>abelian</strong> if it satisfies the <strong>commutative law</strong>: $x∗y=y∗x$ holds for every <em>x</em>, <em>y</em> ∈ <em>G</em>.</li>
</ul>
<blockquote>
<p>We will usually denote the product $x ∗ y$ in a group by $xy$, and we will denote the identity by $1$ instead of by $e$. </p>
<p>When a group is abelian, however, we will often use the <strong>additive notation</strong> $x + y$; in this case, we will denote the identity by $0$, and we will denote the inverse of an element $x$ by $−x$ instead of by $x^{−1}$.</p>
</blockquote>
<p><strong>Ring</strong>:</p>
<ul>
<li>A <strong>ring</strong> $(R, +, ·)$ is an abelian group $(R, +)$ endowed with a second binary operation $·$, satisfying on its own the requirements of being associative and having a two-sided identity and further interacting with $+$ via the distributive properties:, i.e.,<ol>
<li>$∀r,s,t∈R$, $(r·s)·t=r·(s·t)$,</li>
<li>$∃1_R ∈ R, ∀r ∈ R$, $r · 1_R = r = 1_R · r$ </li>
<li>$∀r,s,t∈R$, $(r+s)·t=r·s+r·t$ and $t·(r+s)=t·r+t·s$.</li>
</ol>
</li>
</ul>
<blockquote>
<p>What we are calling a ring, others may call a <strong>ring with identity</strong> or a <strong>ring with 1</strong>: it is not uncommon to exclude the axiom of existence of a multiplicative identity from the list of axioms defining a ring.</p>
</blockquote>
<p><strong>Commutative Ring</strong></p>
<ul>
<li>A <strong>commutative ring</strong> $R$ is a set with two binary operations, addition and multiplication, such that<ol>
<li>$R$ is an abelian group under addition;</li>
<li>(<strong>commutativity</strong>) $ab = ba$ for all $a, b ∈ R$;</li>
<li>(<strong>associativity</strong>) $a(bc) = (ab)c$ for every $a,b, c ∈ R$;</li>
<li>there is an element $1∈R$ with $1a=a$ for every $a∈R$;</li>
<li>(<strong>distributivity</strong>) $a(b + c) = ab + ac$ for every $a, b, c ∈ R$.</li>
</ol>
</li>
</ul>
<p><strong>Subring</strong></p>
<ul>
<li>A subset <em>S</em> of a commutative ring <em>R</em> is a <strong>subring</strong> of <em>R</em> if<ol>
<li>$1 ∈ S$;</li>
<li>if $a,b∈S$, then $a−b∈S$;</li>
<li>if $a,b∈S$, then $ab∈S$.</li>
</ol>
</li>
</ul>
<p><strong>Domain / Integral Domain</strong></p>
<ul>
<li>A <strong>domain / Integral domain</strong> is a commutative ring $R$ that satisfies two extra axioms: <ol>
<li>$1\neq 0$;</li>
<li>The <strong>cancellation law</strong> for multiplication: For all $a, b, c ∈ R$, if $ca = cb$ and $c\neq 0$, then $a = b$.</li>
</ol>
</li>
</ul>
<p><strong>Divide</strong>:</p>
<ul>
<li>Let $a$ and $b$ be elements of a commutative ring $R$. Then $a$ <strong>divides</strong> $b$ <strong>in $R$</strong> (or $a$ is a <strong>divisor</strong> of $b$ or $b$ is a <strong>multiple</strong> of $a$), denoted by $a | b$, if there exists an element $c ∈ R$ with $b = ca$.</li>
</ul>
<p><strong>Unit, Inverse</strong>:</p>
<ul>
<li>An element $u$ in a commutative ring $R$ is called a <strong>unit</strong> if $u | 1$ in $R$</li>
<li>that is, if there exists $v ∈ R$ with $uv = 1$; the element $v$ is called the <strong>inverse</strong> of $u$ and $v$ is often denoted by $u^{−1}$.</li>
</ul>
<p><strong>Group of Units</strong>:</p>
<ul>
<li>If $R$ is a commutative ring, then the <strong>group of units</strong> of $R$ is $U(R) = {\mbox{all units in }R}$.</li>
</ul>
<p><strong>Field</strong>:</p>
<ul>
<li>A <strong>field</strong> $F$ is a commutative ring in which $1\neq 0$ and every nonzero element $a$ is a unit; that is, there is $a^{−1} ∈ F$ with $a^{−1}a = 1$.</li>
</ul>
<p><strong>Subfield</strong>:</p>
<ul>
<li>A <strong>subfield</strong> of a field $K$ is a subring $k$ of $K$ that is also a field.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Lemma 2.16.</strong> Let G be a group.</p>
<ol>
<li>The <strong>cancellation laws</strong> hold: If either $x∗a=x∗b$ or $a∗x=b∗x$, then $a=b$.</li>
<li>The element $e$ is the unique element in $G$ with $e∗x=x=x∗e$ for all $x∈G$.</li>
<li>Each $x ∈ G$  has a unique inverse: There is only one element $x’∈ G$ with $x ∗ x’ = e = x′ ∗ x$  (henceforth, this element will be denoted by $x^{−1}$).</li>
<li>$(x^{-1})^{−1} = x$ for all $x ∈ G$.</li>
</ol>
<p><strong>Proposition 3.2.</strong> </p>
<ul>
<li>Let $R$ be a commutative ring.<ol>
<li>$0·a=0$ for every $a∈R$.</li>
<li>If $1 = 0,$ then $R$ consists of the single element $0$. In this case, $R$ is called the zero ring.</li>
<li>If $−a$ is the additive inverse of $a$, then $(−1)(−a) = a$.</li>
<li>$(−1)a = −a$ for every $a ∈ R$.</li>
<li>If $n∈N$ and $n1=0$, then $na=0$ for all $a∈R$.</li>
</ol>
</li>
</ul>
<p><strong>Proposition 3.3.</strong> </p>
<ul>
<li>A subring $S$ of a commutative ring $R$ is itself a commutative ring.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Paolo Aluffi, 2009, “Algebra: Chapter 0”;</li>
<li>Michael Artin, 2010, “Algebra”;</li>
<li>Joseph J. Rotman, 2003, “Advanced Modern Algebra”;</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:50:59.000Z" title="12/15/2020, 10:50:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/">1.1.H Stochastic Independence and Dependence</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/15%20Stochastic%20Dependence/Propertities-of-Conditional-Expectation/">Propertities of Conditional Expectation</a></h1><div class="content"><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 2.1</strong>: </p>
<ul>
<li><p>Let $x$ and $y$ be integrable random variables on a probability space $(X,\Sigma, \mathbf{p})$ and $\Sigma_0$ a sub-$\sigma$-algebra of $\Sigma$. Then, for every real number $\alpha$<br>$$<br>\mathbb{E}(\alpha x|\Sigma_0)=<em>{a.s.}\alpha\mathbb{E}(x|\Sigma_0)<br>$$<br>and<br>$$<br>\mathbb{E}(x+y|\Sigma_0)=</em>{a.s.}\mathbb{E}(x|\Sigma_0)+\mathbb{E}(y|\Sigma_0)<br>$$</p>
</li>
<li><p>Moreover,<br>$$<br>x=_{a.s.}y\mbox{ implies }\mathbb{E}(x|\Sigma_0)+\mathbb{E}(y|\Sigma_0)<br>$$</p>
</li>
</ul>
<p><strong>Proposition 2.2</strong>: </p>
<ul>
<li>Let $x$ and $y$ be integrable random variables on a probability space $(X,\Sigma, \mathbf{p})$ and $\Sigma_0$ a sub-$\sigma$-algebra of $\Sigma$. Then,<br>$$<br>\mathbb{E}(\mathbb{E}(x|\Sigma_0))=\mathbb{E}(x)<br>$$<br>And if $\Sigma_1$ is another sub-$\sigma$-algebra of $\Sigma$, then<br>$$<br>\Sigma_0\subseteq \Sigma_1\mbox{ implies }\mathbb{E}(\mathbb{E}(x|\Sigma_1)|\Sigma_0)=\mathbb{E}(x|\Sigma_0)<br>$$</li>
</ul>
<p><strong>Proposition 2.4</strong>:</p>
<ul>
<li><p>Let $x$ and $y$ be integrable random variables on a probability space $(X,\Sigma, \mathbf{p})$ and $\Sigma_0$ a sub-$\sigma$-algebra of $\Sigma$.</p>
</li>
<li><p>If $y\in\mathcal{L}^0(X,\Sigma_0)$ and $\mathbb{E}(|xy|)&lt;\infty$,then<br>$$<br>\mathbb{E}(xy|\Sigma_0)=_{a.s.}y\mathbb{E}(x|\Sigma_0)<br>$$</p>
</li>
</ul>
<p><strong>The Conditional Monotone Convergence Theorem</strong>: </p>
<ul>
<li><p>Let $x, x_1,x_2,\dots$ be nonnegative integrable random variables on a probability space $(X,\Sigma, \mathbf{p})$ such that $x_m\nearrow_{a.s.} x$</p>
</li>
<li><p>If $\Sigma_0$ is a sub-$\sigma$-algebra of $\Sigma$, then<br>$$<br>\mathbb{E}(x_m|\Sigma_0)\nearrow_{a.s.}\mathbb{E}(x|\Sigma_0)<br>$$</p>
</li>
</ul>
<p><strong>The Conditional Dominated Convergence Theorem</strong>: </p>
<ul>
<li><p>Let $x, y,x_1,x_2,\dots$ be nonnegative integrable random variables on a probability space $(X,\Sigma, \mathbf{p})$ such that $x_m\leq_{a.s.} x$ for each $m$</p>
</li>
<li><p>If $\Sigma_0$ is a sub-$\sigma$-algebra of $\Sigma$, then<br>$$<br>\mathbb{E}(x_m|\Sigma_0)\to_{a.s.}\mathbb{E}(x|\Sigma_0)<br>$$</p>
</li>
</ul>
<p><strong>The Conditional Jensen’s Inequality</strong>:</p>
<ul>
<li>Let $x$ be an integrable random variable on a probability space $(X,\Sigma, \mathbf{p})$, and $\Sigma_0$ a sub-$\sigma$-algebra of $\Sigma$. If $\varphi$ is a concave self-map on $\mathbb{R}$ such that $\mathbb{E}(|\varphi\circ x|) &lt;\infty$, then<br>$$<br>\varphi(\mathbb{E}(x|\Sigma_0))\geq_{a.s.}\mathbb{E}(\varphi\circ x|\Sigma_0)<br>$$</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:49:59.000Z" title="12/15/2020, 10:49:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/">1.1.H Stochastic Independence and Dependence</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/15%20Stochastic%20Dependence/Conditional-Expectation/">Conditional Expectation</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Conditional Expectation on a Known Event</strong>:</p>
<ul>
<li><p>Let $x$ be an integrable random variable on a probability space $(X,\Sigma,\mathbf{p})$ </p>
</li>
<li><p>That is, $x\in\mathcal{L}^0(X,\Sigma$) and $\mathbb{E}(|x|) &lt; \infty$, or put differently, $x\in\mathcal{L}^1(X,\Sigma,\mathbf{p})$</p>
</li>
<li><p>For any event $C\in\Sigma$ with $\mathbf{p}(C) &gt; 0$, the <strong>conditional expectation</strong> <strong>of $x$ given</strong> $C$ is defined as<br>$$<br>\mathbb{E}(x|C):=\frac{1}{\mathbf{p}(C)}\int_Cxd\mathbf{p}<br>$$</p>
</li>
</ul>
<blockquote>
<p>When it is known that the event $C$ has occurred, one is less uncertain about the outcome of the underlying experiment. </p>
<p>The relevant probability space in this case can be thought of as $(C,\Sigma\cap C,\mathbf{q})$, where $\Sigma\cap C := {A\cap C:A\in\Sigma}$ and<br>$$<br>\mathbf{q}:=\frac{1}{\mathbf{p}(C)}\mathbf{p}|_{\Sigma\cap C}<br>$$<br>Consequently, we compute the conditional expectation of $x$ given $C$ simply as the expectation of $x|_C$ on this space.</p>
</blockquote>
<p><strong>Conditional Probability on a Known Event</strong>:</p>
<ul>
<li>The <strong>conditional probability</strong> of an event $A\in\Sigma$ given $C$ is $\mathbf{p}(A|C) := \frac{\mathbf{p}(A \cap C)}{\mathbf{p}(C)}$</li>
</ul>
<blockquote>
<p>Just as the probability of an event is the expectation of the indicator function of that event, the conditional probability of an event is the conditional expectation of the indicator function of that event:<br>$$<br>\mathbb{p}(A|C)=\frac{\mathbb{p}(A\cap C)}{\mathbf{p}(C)}\mathbb{E}(\mathbf{1}_A|C)<br>$$<br>Note that these notions apply only when we know that an event has already occurred.</p>
</blockquote>
<p><strong>Countable (finite) $\Sigma$-decomposition</strong>:</p>
<ul>
<li>By a <strong>countable (finite) $\Sigma$-decomposition</strong> of $X$ we mean a countable (finite) partition $\mathcal{C}$ of $X$ such that $\mathcal{C}\subseteq \Sigma$ and $\mathbf{p}(C) &gt; 0$ for all $C\in\mathcal{C}$.</li>
</ul>
<p><strong>Conditional Expectation on a Countable Decomposition</strong>:</p>
<ul>
<li><p>We define the <strong>conditional expectation of $x$ given $\mathcal{C}$</strong> as the <u>discrete random variable</u> $\mathbb{E}(x|{\mathcal{C}}) : X \to \mathbb{R}$ with<br>$$<br>\mathbb{E}(x|\mathcal{C})(\omega):=\mathbb{E}(x|C_\omega)<br>$$<br>or<br>$$<br>\int_{C_\omega}\mathbb{E}(x|\mathcal{C})d\mathbf{p}=\int_{C_\omega}xd\mathbf{p}\mbox{ for every }\omega\in X<br>$$</p>
</li>
<li><p>where $C_\omega$ is the member of $\mathcal{C}$ that contains the outcome $\omega$.</p>
</li>
<li><p>More compactly, we may write<br>$$<br>\mathbb{E}(x|\mathcal{C}):=\sum_{C\in\mathcal{C}}\mathbb{E}(x|C)\mathbf{1}_C<br>$$</p>
</li>
</ul>
<blockquote>
<p>Given that we will observe which member of $\mathcal{C}$ has occurred once the experiment is performed, we then calculate the conditional expectation.</p>
<p>A gambler, for instance, would like to have a strategy which is contingent on what will happen through the sequence of games she will repeatedly participate in.</p>
</blockquote>
<p><strong>Conditional Probability on a Countable Decomposition</strong>:</p>
<ul>
<li><p>The <strong>conditional probability of an event $A$ in $\Sigma$ given $\mathcal{C}$</strong> is similarly defined as the <u>discrete random variable</u> $\mathbf{p}(A|\mathcal{C}) : X\to\mathbb{R}$ with<br>$$<br>\mathbf{p}(A|\mathcal{C}) (\omega):=\frac{\mathbf{p}(A\cap C_\omega)}{\mathbf{p}(C_\omega)}<br>$$</p>
</li>
<li><p>Or equivalently,<br>$$<br>\mathbf{p}(A|\mathcal{C}):=\sum_{C\in\mathcal{C}}\mathbf{p}(A\cap C)\mathbf{1}_C<br>$$</p>
</li>
</ul>
<blockquote>
<p>Of course, we have $\mathbf{p}(A|\mathcal{C})=\mathbb{E}(\mathbf{1}_A|\mathcal{C})$</p>
</blockquote>
<p><strong>Sub-$\sigma$-algebra</strong>:</p>
<ul>
<li>Let $(X,\Sigma,\mathbf{p})$ be a probability space. By a <strong>sub-$\sigma$-algebra</strong> of $\Sigma$, we mean a subset of $\Sigma$ which is itself a $\sigma$-algebra.</li>
</ul>
<p><strong>Conditional Expectation on an Arbitrary $\sigma$-algebra, Conditional Probability on an Arbitrary $\sigma$-algebra</strong>:</p>
<ul>
<li><p>For any sub-$\sigma$-algebra $\Sigma_0$ of $\Sigma$, and any $x\in\mathcal{L}^1(X,\Sigma,\mathbf{p})$, the <strong>conditional expectation of $x$ given $\Sigma_0$</strong> is defined as <u>an extended real function</u> $\mathbb{E}(x|\Sigma_0)$ on $X$ which satisfies<br>$$<br>\mathbb{E}(x | \Sigma_0)\mbox{ is }\Sigma_0\mbox{-measurable}<br>$$</p>
</li>
<li><p>and<br>$$<br>\int_B\mathbb{E}(x|\Sigma_0)d\mathbf{p}=\int_Bxd\mathbf{p}\mbox{ for every }B\in\Sigma_0<br>$$</p>
</li>
<li><p>For any $y \in \mathcal{L}^0(X,\Sigma)$ we define the <strong>conditional expectation of $x$ given $y$,</strong> denoted $\mathbb{E}(x| y)$ as $\mathbb{E}(x | y) := \mathbb{E}(x |\sigma (y))$ </p>
</li>
<li><p>For any $A\in\Sigma$, we define <strong>the conditional probability of $A$ given $\Sigma_0$</strong>, denoted $\mathbf{p}(A|\Sigma_0)$ as <u>an extended real function</u> on $X$ that equals $\mathbb{E}(\mathbf{1}_A| \Sigma_0)$ or equivalently, that satisfies<br>$$<br>\mathbf{p}(x | \Sigma_0)\mbox{ is }\Sigma_0\mbox{-measurable}<br>$$</p>
</li>
<li><p>and<br>$$<br>\int_B\mathbf{p}(x|\Sigma_0)d\mathbf{p}=\mathbf{p}(A\cap B)\mbox{ for every }B\in\Sigma_0<br>$$</p>
</li>
</ul>
<p><strong>Conditional Density</strong>:</p>
<ul>
<li><p>Let $x$ and $y$ be two integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$ and let $f$ be a joint density for $x$ and $y$ </p>
</li>
<li><p>The <strong>conditional density of $x$ given $y$</strong> is defined as<br>$$<br>f_{x|y}(s,t):=\left{\begin{matrix}\frac{f(s,t)}{f_y(t)}&amp;\mbox{if }f_y(t)&gt;0\0&amp;\mbox{otherwise}\end{matrix}\right.<br>$$</p>
</li>
<li><p>where $f_y$ is the marginal density of $y$.</p>
</li>
</ul>
<blockquote>
<p>We can show that<br>$$<br>\mathbb{E}(x|y)=<em>{a.s.}\int^\infty</em>{-\infty}sf_{x|y}(s,y(\cdot))ds<br>$$</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Therorem 1.1</strong>:</p>
<ul>
<li>If $x$ is an integrable random variable on the probability space $(X,\Sigma,\mathbf{p})$, and $\Sigma_0$ is a sub-$\sigma$-algebra of $\Sigma$, then $\mathbb{E}(x|\Sigma_0)$ exists.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:48:59.000Z" title="12/15/2020, 10:48:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-G-Weak-Convergence-and-Probability-Limit/">1.1.G Weak Convergence and Probability Limit</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/14%20A%20Primer%20on%20Probability%20Limit%20Theorems/Law-of-Large-Numbers/">Law of Large Numbers</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Random Sample, Statistic</strong>:</p>
<ul>
<li>Let $x$ be a random variable on a probability space $(X,\Sigma,\mathbf{p})$. </li>
<li>A <strong>random sample</strong> for $x$ is a finite collection $x_1,\dots,x_m$ of i.i.d. random variables on $(X,\Sigma,\mathbf{p})$ such that $x_1 =_{a.s.} x$. </li>
<li>A (real-valued) <strong>statistic</strong> based on such a random sample is a <u>random variable</u> of the form $\varphi(x_1,\dots, x_m)$, where $\varphi$ is <u>a Borel measurable real function</u> on $\mathbb{R}\cup \mathbb{R}^2\cdots $ </li>
<li>Notice that $\varphi$ can accomodate any random sample regardless of its size.</li>
</ul>
<p><strong>Unbiased estimator</strong>:</p>
<ul>
<li><p>The statistics based on a random sample are used to derive inferences about the characteristics of the random variable of interest. We would then surely wish them to satisfy certain properties. </p>
</li>
<li><p>For instance, a desirable property in this regard is that of unbiasedness: We say that a statistic $\varphi(x_1,\dots,x_m)$ based on the random sample $x_1,\dots, x_m$ is an <strong>unbiased estimator</strong> of $x$ if<br>$$<br>\mathbb{E}(\varphi(x_1,\dots,x_m))=\theta_x<br>$$</p>
</li>
<li><p>where $\theta_x$ is a characteristic of $x$, such as its mean or another moment. </p>
</li>
</ul>
<blockquote>
<p>The property of unbiasedness is well-defined for any random sample, regardless of its size. As such, it is said to be a small-sample property. </p>
</blockquote>
<p><strong>Consistent estimator, Strongly consistent esitimator</strong>:</p>
<ul>
<li>We say that a statistic $\varphi(x_1,\dots,x_m)$ based on the random sample $x_1,\dots, x_m$ is a <strong>consistent estimator</strong> of a characteristic $\theta_x$ of $x$ if<br>$$<br>\mathbf{p}\mbox{-}\lim\varphi(x_1,\dots,x_m)=\theta_x<br>$$<br>And that it is a <strong>strongly consistent estimator</strong> of $\theta_x$ if<br>$$<br>\mathbf{p}{\varphi(x_1,\dots,x_m)\to\theta_x}=1<br>$$</li>
</ul>
<blockquote>
<p>A large-sample property of a statistic would instead be based on the limiting properties of this statistic as the sample size gets large. </p>
<p>Of particular interest in this regard are the properties of consistency.</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>The Weak Law of Large Numbers</strong>: (Chebyshev) </p>
<ul>
<li>Let $(x_m)$ be a sequence of independent random variables on a probability space $(X,\Sigma, \mathbf{p})$ with $\mathbb{E}(x_1) = \mathbb{E}(x_2) = \cdots\in \mathbb{R}$ and $\sup \mathbb{V}(x_m) &lt; \infty$. Then,<br>$$<br>\mathbb{E}\left(\left|\frac{1}{m}\sum_{i\in[m]}x_i-\mathbb{E}(x_1)\right|\right)\to 0\mbox{ and }\frac{1}{m}\sum_{i\in[m]}x_i\to\mathbb{E}(x_1)\mbox{ in probability}<br>$$</li>
</ul>
<p><strong>Corollary 2.1</strong>: </p>
<ul>
<li>Let $(x_m)$ be a sequence of i.i.d. random variables with finite expectation and variance. Then,<br>$$<br>\frac{1}{m}\sum_{i\in[m]}x_i\to\mathbb{E}(x_1)\mbox{ in probability}<br>$$</li>
</ul>
<p><strong>Khinchine’s Weak Law of Large Numbers</strong>:</p>
<ul>
<li>Let $(x_m)$ be a sequence of i.i.d. random variables with finite expectation. Then,</li>
</ul>
<p>$$<br>\frac{1}{m}\sum_{i\in[m]}x_i\to\mathbb{E}(x_1)\mbox{ in probability}<br>$$</p>
<p><strong>Markov’s Weak Law of Large Numbers</strong>: </p>
<ul>
<li><p>Let $(x_m)$ be a sequence of uncorrelated random variables on a probability space $(X,\Sigma,\mathbf{p})$ such that $\sup\mathbb{E}(x_m) &lt; 1$ and $\sup \mathbb{V}_(x_m) &lt; 1$.</p>
</li>
<li><p>Assume further that<br>$$<br>\lim\frac{1}{m}\sum_{i\in[m]}\mathbb{E}(x_i)\in\mathbb{R}\mbox{  and }\frac{1}{m^2}\sum_{i\in[m]}\mathbb{V}(x_i)\to0<br>$$</p>
</li>
<li><p>Then<br>$$<br>\frac{1}{m}\sum_{i\in[m]}x_i\to\lim\frac{1}{m}\sum_{i\in[m]}\mathbb{E}(x_i)\mbox{ in probability}<br>$$</p>
</li>
</ul>
<p><strong>The Strong Law of Large Numbers</strong>: (Kolmogorov) </p>
<ul>
<li>For any sequence $(x_m)$ of integrable i.i.d. random variables, we have<br>$$<br>\frac{1}{m}\sum_{i\in[m]}x_i\to_{a.s.}\mathbb{E}(x_1)<br>$$</li>
</ul>
<p><strong>Corollary 2.2</strong>: (Kolmogorov) </p>
<ul>
<li>For any sequence $(x_m)$ of integrable i.i.d. random variables such that $\mathbb{E}(x_1)\in{-\infty,\infty}$, we have<br>$$<br>\frac{1}{m}\sum_{i\in[m]}x_i\to_{a.s.}\mathbb{E}(x_1)<br>$$</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Sample Mean, Sample Variance</strong>:</p>
<ul>
<li><p>For a random sample with a finite collection $a_1,\dots,a_m$ of i.i.d. random variables on $(X,\Sigma,\mathbf{p})$ such that $a_1 =_{a.s.} a$. </p>
</li>
<li><p>If<br>$$<br>\varphi(a_1,\dots,a_m):=\frac{1}{m}\sum_{i\in[m]}a_i<br>$$<br>Then $\varphi(a_1,\dots,a_m)$ corresponds to the statistic of the sample mean</p>
</li>
<li><p>If<br>$$<br>\varphi(a_1,\dots,a_m):=\frac{1}{m}\sum_{i\in[m]}\left(a_i-\frac{1}{m}\sum_{i\in[m]}a_i\right)^2<br>$$<br>Then $\varphi(a_1,\dots,a_m)$ corresponds to the statistic of the sample variance.</p>
</li>
<li><p>Sample mean is an unbiased estimator of $\mathbb{E}(x)$, for any postive integer $m$.<br>$$<br>\mathbb{E}\left(\frac{1}{m}\sum_{i\in[m]}x_i\right)=\frac{1}{m}\sum_{i\in[m]}\mathbb<a href="x_i">E</a>=\mathbb{E}(x)<br>$$<br>as $\mathbb{E}(x_i) = \mathbb{E}(x)$ for each $i$.</p>
</li>
<li><p>By contrast, the sample variance is not an unbiased estimator of $\mathbb{V}(x)$.</p>
</li>
</ul>
<p><strong>Example 2.2</strong>:</p>
<ul>
<li><p>The sample mean is a strongly consistent estimator of $\mathbb{E}(x)$ provided that $\mathbb{E}(x)$ is finite. This is the same thing as saying that<br>$$<br>\frac{1}{m}\sum_{i\in[m]}x_i\to_{a.s.}\mathbb{E}(x)<br>$$</p>
</li>
<li><p>when $\mathbb{E}(x)$ is finite. </p>
</li>
</ul>
<p><strong>Example 2.3</strong>:</p>
<ul>
<li>The sample variance is a strongly consistent estimator of $\mathbb{V}(x)$ provided that both $\mathbb{E}(x)$ and $\mathbb{E}(x^2)$ are finite. This is the same thing as saying that<br>$$<br>\frac{1}{m}\sum_{i\in[m]}\left(x_i-\frac{1}{m}\sum_{i\in[m]}x_i\right)^2\to_{a.s.}\mathbb{V}(x)<br>$$</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:47:59.000Z" title="12/15/2020, 10:47:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-G-Weak-Convergence-and-Probability-Limit/">1.1.G Weak Convergence and Probability Limit</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/14%20A%20Primer%20on%20Probability%20Limit%20Theorems/Preliminaries-of-Probability-Limit-Theorems/">Preliminaries of Probability Limit Theorems</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Converge in Probability</strong>:</p>
<ul>
<li><p>Let $Y$ be a separable metric space, and $x,x_1,x_2,\dots$ be $Y$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$. </p>
</li>
<li><p>We say that $(x_m)$ <strong>converges to $x$ in probability</strong>, and write $x_m\to x$ in probability, or $\mathbf{p}$-$\lim x_m = x$ if<br>$$<br>\mathbf{p}{d_Y(x_m,x)&gt;\varepsilon}\to 0 \mbox{ for every }\varepsilon&gt;0<br>$$</p>
</li>
<li><p>That is, $x_m \to x$ in probability iff, for every positive real numbers $\varepsilon$ and $\delta$ there exists a positive integer $M$ such that<br>$$<br>\mathbf{p}{d_Y(x_m,x)&gt;\varepsilon}&lt;\delta \mbox{ for all }m&gt;M<br>$$</p>
</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1.1</strong>：</p>
<ul>
<li>Let $Y$ be a separable metric space, and $x,x_1,x_2,\dots$ be $Y$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$. </li>
<li>If $(x_m)$ converges to $x$ in probability, then $x_m\stackrel{D}\to x$.</li>
</ul>
<p><strong>Proposition 1.2</strong>: (Kolmogorov) </p>
<ul>
<li>Let $Y$ be a separable metric space, and $x,x_1,x_2,\dots$ be $Y$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$. </li>
<li>If $x_m\to_{a.s.}x$, then $(x_m)$ converges to $x$ in probability.</li>
</ul>
<blockquote>
<p>$$<br>\mbox{almost sure convergence}\Longrightarrow\mbox{convergence in probability}\Longrightarrow\mbox{convergence in distribution}<br>$$</p>
</blockquote>
<p><strong>Theorem 3.8</strong>:</p>
<ul>
<li>Let $Y$ be a separable metric space</li>
<li>For any given real number $p\geq1$, let $x, x_1,x_2,\dots$ be $Y$-valued random variables on $\mathcal{L}^p(X,\Sigma,\mathbf{p})$</li>
<li>If $(x_m)$ is $\mathcal{L}^p$-convergent, then $(x_m)$ <strong>converges to $x$ in probability</strong>.</li>
</ul>
<blockquote>
<p>$$<br>\mathcal{L}^p\mbox{-convergence}\Longrightarrow\mbox{convergence in probability}\Longrightarrow\mbox{convergence in distribution}<br>$$</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:46:59.000Z" title="12/15/2020, 10:46:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/">1.1.H Stochastic Independence and Dependence</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/13%20Stochastic%20Independence/Independence-of-Random-Variables/">Independence of Random Variables</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Independence of Random Variables</strong>:</p>
<ul>
<li><p>For any positive integer $i$, let $Y_i$ be a metric space and $x_i$ a $Y_i$-valued random variable on a probability space $(X,\Sigma,\mathbf{p})$. </p>
</li>
<li><p>For any $m &gt; 1$, we say that $x_1,\dots,x_m$ are (stochastically) <strong>independent</strong> when $(x_1),\dots,(x_m)$ are independent, that is,<br>$$<br>\mathbf{p}\left(\bigcap_{i\in[m]}{x_i\in A_i}\right)=\prod_{i\in[m]}\mathbf{p}{x_i\in A_i}\mbox{ for every }A_i\in\mathcal{B}(Y_i),\ i=1,\dots,m<br>$$</p>
</li>
<li><p>and we denote this situation by writting $\coprod [x_1,\dots,x_m]$ </p>
</li>
<li><p>If $\coprod [x,x]$ for any $Y$-valued random variable $x$ on $(x,\Sigma,\mathbf{p})$, we say that $x$ is <strong>independent of itself</strong>.</p>
</li>
<li><p>More generally, if $\coprod [\sigma(x_1),\sigma(x_2),\dots]$ we say that $x_1,x_2,\dots$ are <strong>independent</strong> or $(x_m)$ is a <strong>sequence of independent of random variables</strong>, and write $\coprod [x_1,x_2,\dots]$ </p>
</li>
</ul>
<p><strong>Uncorrelated</strong>:</p>
<ul>
<li>Let $x$ and $y$ be two integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$.</li>
<li>Clearly, $xy$ is also a random variable on $(X,\Sigma,\mathbf{p})$</li>
<li>We say that $x$ and $y$ are <strong>uncorrelated</strong> if $\mathbb{E}(xy) = \mathbb{E}(x)\mathbb{E}(y)$.</li>
</ul>
<p><strong>Independently and Identically Distributed Random Variables</strong>:</p>
<ul>
<li>Let $Y$ be a metric space, and $(x_m)$ a sequence of $Y$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$. </li>
<li>For any $m &gt; 1$, we say that $x_1,\dots, x_m$ are <strong>independently and identically distributed</strong>, or simply <strong>i.i.d.</strong>, if $\coprod [x_1,\dots,x_m]$ and $\mathbf{p}_{x_i} = \mathbf{p}_{x_j}$ for every $i,j\in[m]$.</li>
<li>More generally, we say that $x_1,x_2,\dots$ are <strong>i.i.d</strong>. (or that $(x_m)$ is a <strong>sequence of i.i.d. random variables</strong> when $\coprod [x_1,\dots,x_m]$ and $\mathbf{p}_{x_i} = \mathbf{p}_{x_j}$ for every postive integers $i$ and $j$.</li>
</ul>
<blockquote>
<p>If $Y=\mathbb{R}$, then $\mathbf{p}_{x_i} = \mathbf{p}_{x_j}$ implies they have the same distribution and distribution functions. </p>
<p>Take the normal distribution as an example, the $\mu$ and $\sigma$ should be the same for all i.i.d. random variables.</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 2.1</strong>: </p>
<ul>
<li>Let $m$ be a positive integer, and $x_1,\dots,x_m$ integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$. Then,<br>$$<br>\coprod [x_1,\dots,x_m]\mbox{ implies }\mathbb{E}\left(\prod_{i\in[m]}x_i\right)=\prod_{i\in[m]}\mathbb{E}(x_i)<br>$$</li>
</ul>
<blockquote>
<p>The expectation of the product of a finite number of independent random variables (each with finite expectation) equals the product of the expectations of these random variables. </p>
<p>In particular, any two independent random variables are uncorrelated.</p>
</blockquote>
<p><strong>Proposition 2.2</strong>: </p>
<ul>
<li>Let $m$ be a positive integer, and $x_1,\dots,x_m$ random variables on a probability space $(X,\Sigma, \mathbf{p})$. Then, $\coprod [x_1,\dots,x_m]$ iff<br>$$<br>F_{x_1,\dots,x_m}(a_1,\dots,a_m)=\prod_{i\in[m]}F_{x_i}(a_i)\mbox{ for every }(a_1,\dots,a_m)\in\mathbb{R}^m.<br>$$</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:45:59.000Z" title="12/15/2020, 10:45:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/">1.1.H Stochastic Independence and Dependence</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/13%20Stochastic%20Independence/Independence-of-Collection-of-Events/">Independence of Collection of Events</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Independence of Events</strong>:</p>
<ul>
<li><p>Let $(X,\Sigma,\mathbf{p})$ be a probability space. </p>
</li>
<li><p>The events $A,B\in\Sigma$ are said to be <strong>(stochastically) independent</strong> if $\mathbf{p}(A \cap B) = \mathbf{p}(A)\mathbf{p}(B)$.</p>
</li>
<li><p>If $A = B$ here, we say that $A$ is independent of itself.</p>
</li>
<li><p>In turn, a nonempty collection $\mathcal{A}\subseteq \Sigma$ is called <strong>independent</strong> – we denote this by writing $\coprod \mathcal{A}$ – if<br>$$<br>\mathbf{p}(\bigcap \mathcal{S})=\prod_{A\in\mathcal{S}}\mathbf{p}(A)<br>$$</p>
</li>
<li><p>for every nonempty finite subset $\mathcal{S}$ of $\mathcal{A}$.</p>
</li>
</ul>
<p><strong>Independence of Finitely Many Familites of Events</strong>:</p>
<ul>
<li><p>Let $(X,\Sigma,\mathbf{p})$ be a probability space and $m$ an integrer with $m&gt;1$. </p>
</li>
<li><p>The nonempty collection $\mathcal{A}_1,\cdots,\mathcal{A}_m\subseteq \Sigma$ are said to be <strong>(stochastically) independent</strong> – we denote this by writing $\coprod [\mathcal{A}_1,\dots,\mathcal{A}_m]$ – if<br>$$<br>\coprod {A_1,\dots,A_m}\mbox{ for every }(A_1,\dotsm,A_m)\in\mathcal{A}_1\times\cdots\times\mathcal{A}_m<br>$$</p>
</li>
</ul>
<blockquote>
<p>The definition of independence of events is a special case of that of independence of collections of events.</p>
</blockquote>
<p><strong>Independence of Infinitely Many Familites of Events</strong>:</p>
<ul>
<li>Let $(X,\Sigma,\mathbf{p})$ be a probability space, $I$ any (index) set with $|I|&gt;1$, and $\mathcal{A}_i$ a nonempty subset of $\Sigma$ for each $i\in I$. </li>
<li>We say that $\mathcal{A}_i$s are <strong>independent</strong> – we denote this by writing $\coprod [\mathcal{A}<em>i, i\in I]$ – if  $\coprod [\mathcal{A}</em>{i_1},\dots,\mathcal{A}_{i_m}]$ for every integrer with $m&gt;1$ and every distinct $i_1,\dots,i_m\in I$.</li>
<li>If $I=\mathbb{N}$ here, we write $\coprod[\mathcal{A}_1,\mathcal{A}_2,\dots]$ to denote $\coprod[\mathcal{A}_i:i\in I]$.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1.1</strong>: </p>
<ul>
<li><p>Let $(X,\Sigma,\mathbf{p})$ be a probabilty space, and $m$ an integer with $m &gt; 1$.</p>
</li>
<li><p>If $\mathcal{A}<em>i\subseteq\Sigma$ contains $X$ for each $i\in[m]$, then $\coprod [\mathcal{A}_1,\dots,\mathcal{A}_m]$ iff<br>$$<br>\mathbf{p}\left(\bigcap</em>{i\in[m]}A_i\right)=\prod_{i\in[m]}\mathbf{p}(A_i)\mbox{ for every }(A_1,\dotsm,A_m)\in\mathcal{A}_1\times\cdots\times\mathcal{A}_m<br>$$</p>
</li>
</ul>
<p><strong>Proposition 1.2</strong>: </p>
<ul>
<li><p>Let $(X,\Sigma,\mathbf{p})$ be a probabilty space, and $m$ an integer with $m &gt; 1$.</p>
</li>
<li><p>If $\mathcal{A}_i\subseteq\Sigma$ contains $X$, and is closed under taking finite intersections, for each $i\in[m]$, then<br>$$<br>\coprod [\mathcal{A}_1,\dots,\mathcal{A}_m]\mbox{ implies }\coprod [\sigma(\mathcal{A}_1),\dots,\sigma(\mathcal{A}_m)]<br>$$</p>
</li>
</ul>
<p><strong>Proposition 1.3</strong>: </p>
<ul>
<li><p>Let $(X,\Sigma,\mathbf{p})$ be a probabilty space, $I$ any (index) set with $|I|&gt;1$, and $\mathcal{A}_i$ a nonempty subset of $\Sigma$ for each $i\in I$. </p>
</li>
<li><p>If $\mathcal{A}_i\subseteq\Sigma$ contains $X$, and is closed under taking finite intersections, for each $i\in[m]$, then<br>$$<br>\coprod[\mathcal{A}_i:i\in I]\mbox{ implies }\coprod[\sigma(\mathcal{A}_i):i\in I]<br>$$</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:44:59.000Z" title="12/15/2020, 10:44:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-G-Weak-Convergence-and-Probability-Limit/">1.1.G Weak Convergence and Probability Limit</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/12%20Weak%20Convergence/Convergence-of-Random-Variables/">Convergence of Random Variables</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Converge in distribution</strong>: </p>
<ul>
<li>Let $Y$ be a metric space. </li>
<li>For any sequence of $Y $-valued random variables $(x_m)$ and a $Y $-valued random variable $x$, we say that $x_m$ <strong>converges to $x$ in distribution</strong>, denoted $x_m\stackrel{D}\to x$, if $\mathbf{p}<em>{x_m}\stackrel{w}{\to} \mathbf{p}_x$, where $\mathbf{p}</em>{x_m}$ and $\mathbf{p}_x$ are the distributions of $x_m$ and $x$, respectively, $m = 1,2,\cdots$.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 2.1</strong>: </p>
<ul>
<li>Let $Y$ be a metric space. </li>
<li>For any $Y $-valued random variables $x, x_1, x_2,\cdots$, we have $x_m \stackrel{D}\to x$ iff,</li>
</ul>
<p>$$<br>\mathbb{E}(\varphi\circ x_m)\to \mathbb{E}(\phi\circ x)\ \mbox{ for every } \in\mathbf{CB}(Y).<br>$$</p>
<p><strong>Corollary 2.2</strong>: </p>
<ul>
<li>Let $Y$ be a metric space. </li>
<li>For any $Y $-valued random variables $x, x_1,x_2,\cdots$, we have $x_m \stackrel{D}\to x$ iff,</li>
</ul>
<p>$$<br>E(\phi\circ x_m) \to \mathbb{E}(\phi\circ x)<br>$$</p>
<ul>
<li>for every bounded and Lipschitz continuous real map $\phi$ on $Y$.</li>
</ul>
<p><strong>Proposition 2.3</strong>: </p>
<ul>
<li>Let $Y$ be a metric space, and let $x, x_1, x_2,\cdots$ be $Y $-valued random variables on a common probability space. Then</li>
</ul>
<p>$$<br>x_m\to_{a.s.} x\ \mbox{ implies } x_m \stackrel{D}{\to} x,<br>$$</p>
<ul>
<li>but not conversely.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:44:39.000Z" title="12/15/2020, 10:44:39 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-G-Weak-Convergence-and-Probability-Limit/">1.1.G Weak Convergence and Probability Limit</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/12%20Weak%20Convergence/Weak-Convergence/">Weak Convergence</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Converge Weakly, Weak Limit</strong>:  </p>
<ul>
<li>Let $X$ be a metric space and $(\mathbf{p}_m)$ a sequence in $\Delta(X)$. </li>
<li>For any $\mathbf{p}\in \Delta(X)$, we say that  $(\mathbf{p}_m)$ <strong>converges weakly</strong> to $\mathbf{p}$, denoted $\mathbf{p}_m \stackrel{w}{\to} \mathbf{p}$, if</li>
</ul>
<p>$$<br>\int_X\varphi d\mathbf{p}_m\to\int_X\varphi d\mathbf{p}\ \ \mbox{ for every } \varphi\in\mathbf{CB}(X)<br>$$</p>
<ul>
<li><p>In this case $\mathbf{p}$ is said to be the <strong>weak limit</strong> of $(\mathbf{p}_m)$.</p>
</li>
<li><p>Put differently, we have<br>$$<br>\mathbf{p}<em>m \stackrel{w}{\to} \mathbf{p} \mbox{ iff }\mathbb{E}</em>{\mathbf{p}_m}(x)\to\mathbb{E}_\mathbf{p}(x)<br>$$</p>
</li>
<li><p>for every $x\in\mathcal{L}^0(X,\mathcal{B}(X))$.</p>
</li>
</ul>
<p><strong>Converge Weakly, Weak Limit</strong>:  </p>
<ul>
<li>The Lebesgue-Stieltjes measure induced by a distribution function $F$ is denoted by $\mathbf{p}_F$. And the class of all distribution functions is denoted by $\mathfrak{F}$.</li>
<li>For any distribution functions $F, F_1, F_2,\cdots$, if $F_m(t)\to F(t)$ holds for every $t$ at which $F$ is continuous, we say the sequence $(F_m)$ <strong>weakly converges</strong> to an $F\in\mathfrak{F}$.</li>
<li>$F$ is said to be the <strong>weak limit</strong> of $(F_m)$.</li>
<li>Naturally enough, we denote this situation by writing $F_m\stackrel{w}\to F$.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1.1</strong>: </p>
<ul>
<li>Let $X$ be a metric space and ($\mathbf{p}_m$) a sequence in $\Delta(X)$. </li>
<li>If $\mathbf{p}_m \stackrel{w}{\to} \mathbf{p}$ and $\mathbf{p}_m\stackrel{w}{\to} \mathbf{q} $ for some $\mathbf{p},\mathbf{q}\in\Delta(X)$, then $\mathbf{p}=\mathbf{q}$.</li>
</ul>
<p><strong>Corollary 1.2</strong>: </p>
<ul>
<li>Given any metric space $X$ and $\mathbf{p}, \mathbf{q}\in \Delta(X)$, we have</li>
</ul>
<p>$$<br>\int_X\varphi d\mathbf{p}=\int_X\varphi d\mathbf{q}\ \mbox{ for all } \varphi\in\mathbf{CB}(X),\ \mbox{ iff } \mathbf{p}=\mathbf{q}<br>$$</p>
<blockquote>
<p>If two Borel probability measures are distinct, then the expectations of at least one continuous and bounded random variable with respect to these measures are not equal.</p>
</blockquote>
<p><strong>Proposition 1.7</strong>: </p>
<ul>
<li>For any distribution functions $F, F_1, F_2,\cdots$, we have $\mathbf{p}_{F_m}\stackrel{w}{\to} \mathbf{p}_F$ iff $F_m\stackrel{w}\to F$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:43:59.000Z" title="12/15/2020, 10:43:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/11%20Expectation%20via%20the%20Stieltjes%20Integral/Integration-By-Parts/">Integration By Parts</a></h1><div class="content"><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Integration by Parts Formula</strong>: (Stieltjes) </p>
<ul>
<li>Let $F$ and $\varphi$ be two increasing real functions on $[a,b]$. If $\varphi$ is $F$-integrable, then $F$ is $\varphi$-integrable, and</li>
</ul>
<p>$$<br>\int^b_a\varphi dF=\varphi(b)F(b)-\varphi(a)F(a)-\int^b_aFd\varphi<br>$$</p>
<p><strong>Proposition 3.1</strong>: </p>
<ul>
<li>Let $x$ be a nonnegative random variable on a probability space $(X,\Sigma, \mathbf{p})$. If $\varphi$ is an increasing and $F_x$-integrable self-map on $\mathbb{R} $, then</li>
</ul>
<p>$$<br>\mathbb{E}(\varphi\circ x)=\varphi(0)+\int^\infty_0(1-F_x)d\varphi<br>$$</p>
<ul>
<li>In particular,<br>$$<br>\mathbb{E}(x)=\int^\infty_0(1-F_x(t))dt<br>$$</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:42:59.000Z" title="12/15/2020, 10:42:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/11%20Expectation%20via%20the%20Stieltjes%20Integral/Expectation-as-a-Stieltjes-Integral/">Expectation as a Stieltjes Integral</a></h1><div class="content"><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Theorem 2.1</strong>: </p>
<ul>
<li>Let $x$ be a random variable and $\varphi$ an $F_x$-integrable self-map on $\mathbb{R} $. </li>
<li>If $\mathbb{E}(\varphi\circ x)$ exists, then</li>
</ul>
<p>$$<br>\mathbb{E}(\varphi\circ x)=\int_X(\varphi\circ x)d\mathbf{p}=\int_\mathbb{R}\varphi d\mathbf{p}<em>x=\int^\infty</em>{-\infty}\varphi dF_x<br>$$</p>
<p><strong>Corollary 2.2</strong>: </p>
<ul>
<li>Let $x$ be a nonnegative random variable and $\varphi$ an $F_x$-integrable self-map on $\mathbb{R} $. If $\mathbb{E}(\varphi\circ x)$ exists, then</li>
</ul>
<p>$$<br>\mathbb{\varphi\circ x}=\varphi(0)F_x(0)+\int^\infty_0\varphi dF_x<br>$$</p>
<p><strong>Corollary 2.3</strong>: </p>
<ul>
<li>Let $x$ be a random variable with a continuously differentiable distribution function $F_x$, and $f$ a density function for $F_x$. </li>
<li>Let $\varphi$ be an almost everywhere continuous and locally bounded self-map on $\mathbb{R} $. </li>
<li>If $\varphi$ is $F_x$-integrable and $\mathbb{E}(\varphi\circ x)$ exists, then</li>
</ul>
<p>$$<br>\mathbb{E}(\varphi\circ x)=\int^\infty_{-\infty}\varphi dF_x=\int^\infty_{-\infty}\varphi(t)f(t)dt<br>$$</p>
<ul>
<li><p>In particular, above equation holds for every continuous $\varphi : \mathbb{R}\to \mathbb{R}_+$. </p>
</li>
<li><p>Moreover, if $\mathbb{E}(x)$ exists,<br>$$<br>\mathbb{E}(x)=\int_Xxd\mathbf{p}=\int^\infty_{-\infty}tdF_x(t)=\int^\infty_{-\infty}tf(t)dt<br>$$</p>
</li>
</ul>
<p><strong>Proposition 2.4</strong>: </p>
<ul>
<li>Let $f : \mathbb{R}\to \mathbb{R}_+$ be a Borel measurable function. </li>
<li>If $f$ is Riemann integrable, then</li>
</ul>
<p>$$<br>\int_{\mathbb{R}}fd\ell=\int^\infty_{-\infty}f(t)dt<br>$$</p>
<ul>
<li>In particular, above equation holds when $f$ is bounded and continuous almost everywhere.</li>
</ul>
<p><strong>Corollary 2.5</strong>: </p>
<ul>
<li>Let $a$ and $b$ be real numbers with $a &lt; b$. </li>
<li>If $f : \mathbb{R}\to \mathbb{R}_+$ is Borel measurable and Riemann integrable (or bounded and continuous almost everywhere), then</li>
</ul>
<p>$$<br>\int_{[a,b]}fd\ell=\int^b_af(t)dt<br>$$</p>
<ul>
<li><p>Similarly, we have<br>$$<br>\int_{(-\infty,a]}fd\ell=\int^a_{-\infty}f(t)dt, \ \ \ and \ \ \ \int_{[b,\infty)}fd\ell=\int^\infty_bf(t)dt<br>$$</p>
</li>
<li><p>The Riemann and Lebesgue integrals of a Riemann integrable and nonnegative Borel measurable real function (defined on an interval) are equal.</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T14:41:59.000Z" title="12/15/2020, 10:41:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/11%20Expectation%20via%20the%20Stieltjes%20Integral/The-Stieltjes-Integral/">The Stieltjes Integral</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Dissection, Subinterval, Division Point, Mesh</strong>: </p>
<ul>
<li><p>For two arbitrarily fixed real numbers $a$ and $b$ with $a &lt; b$, for any positive integer $m$ and real numbers $a_0,\cdots,a_m$ with $a=a_0 &lt;\cdots &lt;a_m =b$, we refer to the collection<br>$$<br>{[a_0,a_1], [a_1, a_2],\cdots,  [a_{m-1},a_m]}<br>$$</p>
</li>
<li><p>as a <strong>dissection</strong> of $[a,b]$, and denote it by either $\mathbf{a}$ or $[a_0,\cdots,a_m]$. </p>
</li>
<li><p>Any one of the intervals $[a_{i-1},a_i] $ is called a <strong>subinterval</strong> of $\mathbf{a}$, while any one of $a_i$s is called a <strong>division point</strong> of $\mathbf{a}$. </p>
</li>
<li><p>The maximum value of the lengths of its subintervals is called the <strong>mesh</strong> of $\mathbf{a}$-this value is denoted by $\mbox{mesh}(\mathbf{a})$.</p>
</li>
<li><p>The collection of all dissections of $[a, b]$ is denoted as $\mathcal{D}[a, b]$.</p>
</li>
</ul>
<p><strong>Finer than</strong>:</p>
<ul>
<li>For any dissections $\mathbf{a} = [a_0,\cdots,a_m]$ and $\mathbf{b} = [b_0,\cdots, b_k]$ of $[a, b]$, we write</li>
</ul>
<p>$$<br>\mathbf{a}\Cup\mathbf{b}<br>$$</p>
<ul>
<li>for the dissection $[c_0,\cdots, c_l]$ in $\mathcal{D}[a, b]$ where ${c_0,\cdots, c_l} = {a_0,\cdots, a_m}\cup{b_0,\cdots, b_k}$. </li>
<li>Moreover, we say that $\mathbf{b}$ is <strong>finer than</strong> $\mathbf{a}$ if ${a_0,\cdots, a_m} \subseteq {b_0,\cdots, b_k}$. </li>
<li>Evidently, $\mathbf{a}\Cup \mathbf{b} = \mathbf{b}$ iff $\mathbf{b}$ is finer than $\mathbf{a}$. </li>
<li>We also have $\mbox{mesh}(\mathbf{b})\leq \mbox{mesh}(\mathbf{a})$ if $\mathbf{b}$ is finer than $\mathbf{a}$, but not conversely.</li>
</ul>
<p><strong>Darboux Sum</strong>: </p>
<ul>
<li>Take any bounded real functions $\varphi$ and $F$ on $[a, b]$ with $F$ being increasing. </li>
<li>For any dissection $\mathbf{a} = [a_0,\cdots, a_m]$ of $[a, b]$, we define</li>
</ul>
<p>$$<br>\check{\varphi}<em>{\mathbf{a}}(i):=\sup{\varphi(t):a</em>{i-1}\leq t\leq a_i}<br>$$</p>
<ul>
<li><p>and<br>$$<br>\hat{\varphi}<em>{\mathbf{a}}(i):=\inf{\varphi(t):a</em>{i-1}\leq t\leq a_i}<br>$$</p>
</li>
<li><p>for each $i\in [m]$. (Since $\varphi$ is bounded, every one of these numbers is real.) </p>
</li>
<li><p>By a <strong>Darboux sum</strong> of $\varphi$ with respect to $F$ and $\mathbf{a}$, we mean a number like<br>$$<br>\sum_{i\in[m]}\alpha_i(F(a_i)-F(a_{i-1}))<br>$$</p>
</li>
<li><p>where $\hat{\varphi}_{\mathbf{a}} (i)\leq \alpha_i\leq \check{\varphi}_{\mathbf{a}}(i)$ for each $i$.</p>
</li>
<li><p>In particular, the <strong>$\mathbf{a}$-upper Darboux sum</strong> of $\varphi$ with respect to $F$ is defined as the number<br>$$<br>\mathbf{S}<em>{F,\mathbf{a}}(\varphi):=\sum</em>{i\in[m]}\check{\varphi}_{\mathbf{a}}(i)(F(a_i)-F(a_{i-1}))<br>$$</p>
</li>
<li><p>and the <strong>$\mathbf{a}$-lower Darboux sum</strong> of $\varphi$ with respect to $F$ is defined dually as<br>$$<br>\mathbf{s}<em>{F,\mathbf{a}}(\varphi):=\sum</em>{i\in[m]}\hat{\varphi}_{\mathbf{a}}(i)(F(a_i)-F(a_{i-1}))<br>$$</p>
</li>
<li><p>Clearly, $\mathbf{S}<em>{F,\mathbf{a}}(\varphi)$ decreases, and $\mathbf{s}</em>{F,\mathbf{a}}(\varphi)$ increases, as a becomes finer. </p>
</li>
<li><p>Evidently, we always have $\mathbf{S}<em>{F,\mathbf{a}}(\varphi)\geq\mathbf{s}</em>{F,\mathbf{a}}(\varphi) $. </p>
</li>
<li><p>Furthermore, and this is important, we have<br>$$<br>\inf{\mathbf{S}<em>{F,\mathbf{a}}(\varphi):\mathbf{a}\in\mathcal{D}[a,b]}\geq\sup{\mathbf{s}</em>{F,\mathbf{a}}(\varphi):\mathbf{a}\in\mathcal{D}[a,b]}<br>$$</p>
</li>
<li><p>The number on the left-hand side of this inequality is called the <strong>upper Stieltjes integral of $\varphi$ with respect to</strong> $F$, and is denoted by $\mathcal{S}_F (\varphi)$. </p>
</li>
<li><p>Similarly, the number on the right-hand side is called the <strong>lower Stieltjes integrals of $\varphi$ with respect to $F$</strong>, and is denoted by $\mathbf{s}_F (\varphi)$. </p>
</li>
<li><p>Thus, our inequality reads:<br>$$<br>\mathcal{S}_F (\varphi)\geq\mathbf{s}_F (\varphi)<br>$$</p>
</li>
</ul>
<p><strong>Stiltjes Integrals, Stieltjes Integrable, $F$-integrable</strong>: </p>
<ul>
<li>Let $\varphi,F\in \mathbf{B}[a, b]$ and assume that $F$ is increasing. </li>
<li>Suppose that there exists a real number $\theta $ such that, for any $\varepsilon &gt; 0$, there exists a dissection $\mathbf{a}$ of $[a, b]$ with</li>
</ul>
<p>$$<br>|\mathbf{S}<em>{F,\mathbf{a}}(\varphi)-\theta|&lt;\varepsilon\mbox{ and }|\mathbf{s}</em>{F,\mathbf{a}}(\varphi)-\theta|&lt; \varepsilon<br>$$</p>
<ul>
<li><p>Then this number $\theta$ is unique, and  it is denoted by<br>$$<br>\int^b_a\varphi dF\mbox{ or }\int^b_a\varphi(t)dF(t)<br>$$</p>
</li>
<li><p>When it exists, we refer to this number as the <strong>Stieltjes integral of $\varphi$ with respect to $F$,</strong> and in that case, we say that $\varphi$ is <strong>Stieltjes integrable with respect to $F$</strong>, or simply, <strong>$F$-integrable</strong>. </p>
</li>
<li><p>If , $\varphi$ is $F$-integrable, we also define<br>$$<br>\int^a_b\varphi dF=-\int^b_a\varphi dF<br>$$</p>
</li>
<li><p>Finally, where $a\leq c\leq  d\leq  b$, we say that $\varphi$ is $F$-integrable on $[c,d]$, if $\varphi|<em>{[c,d]}$ is $F|</em>{[c,d]}$-integrable.</p>
</li>
<li><p>$\varphi$ is $F$-integrable iff the upper and lower Stieltjes integrals of $\varphi$ with respect to $F$ are equal, that is,<br>$$<br>\inf{\mathbf{S}<em>{F,\mathbf{a}}(\varphi) :\mathbf{a}\in\mathcal{D}[a, b]} = \sup{\mathbf{s}</em>{F,\mathbf{a}}(\varphi) : \mathbf{a} \in \mathcal{D}[a, b]}<br>$$</p>
</li>
</ul>
<blockquote>
<p>The following statements are equivalent:</p>
<ul>
<li><p>$\varphi$ is $F$-integrable,</p>
</li>
<li><p>$\mathbf{S}_F(\varphi) = \mathbf{s}_F(\varphi)$,</p>
</li>
<li><p>For every $\varepsilon &gt; 0$, there is a dissection $\mathbf{a}\in \mathcal{D}[a, b]$ such that<br>$$<br>\mathbf{S}<em>{F,\mathbf{a}}(\varphi) -\mathbf{s}</em>{F,\mathbf{a}}(\varphi)&lt;\varepsilon<br>$$</p>
</li>
</ul>
</blockquote>
<p><strong>Riemman Integrable, Riemann Intergral</strong>: </p>
<ul>
<li>The Riemann integral is a special case of the Stieltjes integral. </li>
<li>Formally, we say that $\varphi\in \mathbb{R}^{[a,b]}$ is <strong>Riemann integrable</strong> if it is $\mbox{id}_{[a,b]}$-integrable, and for any such $\varphi$, we define the <strong>Riemann integral</strong> of $\varphi$, denoted by</li>
</ul>
<p>$$<br>\int^b_a\varphi(t)dt<br>$$</p>
<ul>
<li>as the Stieltjes integral of $\varphi$ with respect to the identity function on $[a, b]$.</li>
</ul>
<p><strong>Continuous almost everywhere</strong>: </p>
<ul>
<li>Let us agree to call a real map $\varphi$ on $[a,b]$ continuous almost everywhere on $[a, b]$ if</li>
</ul>
<p>$$<br>d(\varphi):={t\in[a,b]:\varphi\mbox{ is not continuous at }t}<br>$$</p>
<ul>
<li>is a “small” set in the following sense: For every $\varepsilon &gt; 0$ there is an open subset $O$ of $[a,b]$ such that $ d(\varphi)\subseteq O$ and $\ell(O) &lt;\varepsilon$.</li>
</ul>
<blockquote>
<p>The totality of discontinuity points $\varphi$ fits within an open set of arbitrarily Lebesgue measure.</p>
</blockquote>
<p><strong>$F$-integrable, Improper Stieltjes Integral, Riemann Integrable</strong>: </p>
<ul>
<li><p>Let $\varphi$ and $F$ be any two self-maps on $\mathbb{R} $, and assume that $F$ is increasing. </p>
</li>
<li><p>Given any real numbers $a$ and $b$ with $a \leq b$, we say that $\varphi$ is <strong>$F$-integrable on $[a,b]$</strong>, if $\varphi$ is bounded on $[a,b]$ and $\int^b_a\varphi dF$ exists. (Note. If $\varphi$ is $F$-integrable on $[a,b]$, then $\int^b_a\varphi dF$ is a real number.) </p>
</li>
<li><p>In turn, we say that $\varphi$ is <strong>$F$-integrable on $[a,\infty)$</strong> if $\varphi$ is $F$-integrable on $[a,b]$ for each $b\geq a$, and $\lim_{b\to \infty}\int^b_a\varphi dF\in \overline{\mathbb{R} }$</p>
</li>
<li><p>In this case we define the <strong>(improper) Stieltjes integral</strong> of $\varphi$ with respect to $F$ on this unbounded interval as<br>$$<br>\int^\infty_a\varphi dF:=\lim_{c\to \infty}\int^c_a\varphi dF.<br>$$</p>
</li>
<li><p>The $F $-integrability of $\varphi$ on $(-\infty,a]$ and the extended real number $\int^a_{-\infty}\varphi dF$ is analogously defined. </p>
</li>
<li><p>Finally, we say that $\varphi$ is <strong>$F$-integrable</strong>, if there exists a real number a such that $\varphi$ is $F$-integrable on both $(-\infty,a]$ and $[a,\infty)$, and the expression $\int^a_\infty \varphi dF+\int^\infty_a\varphi dF$ is not of the indeterminate form $\infty-\infty$. </p>
</li>
<li><p>In this case, we define<br>$$<br>\int^\infty_{-\infty}\varphi dF:=\int^a_{-\infty}\varphi dF+\int^\infty_{a}\varphi dF,<br>$$</p>
</li>
<li><p>and note that this definition allows $\int^\infty_{-\infty}\varphi dF$ to equal $-\infty$ or $\infty$. </p>
</li>
<li><p>If $\varphi$ is $id_{\mathbb{R} }$-integrable, then we say that it is <strong>Riemann integrable</strong>.</p>
</li>
</ul>
<blockquote>
<p>The $F $-integrability of $\varphi$ implies that, on every compact interval, $\varphi$ is $F $-integrable. In particular, such a $\varphi$ is bounded on every compact interval. </p>
<p>A self-map on $\mathbb{R} $ with this property is said to be locally bounded.</p>
</blockquote>
<p><strong>Continous Almost Everywhere</strong>: </p>
<ul>
<li>The notion of almost everywhere continuity is extended to self-maps on $\mathbb{R} $ in the obvious way. </li>
<li>Put precisely, we say that $\varphi$ is <strong>continuous almost everywhere</strong> (or almost everywhere continuous) if, for every $\varepsilon &gt; 0$, there is an open subset $O$ of $\mathbb{R} $ such that $\ell(O) &lt; \varepsilon$ and every point of discontinuity of $\varphi$ is contained in $O$. </li>
</ul>
<blockquote>
<p>Obviously, if $\varphi$ is continuous almost everywhere, then it is continuous almost everywhere on any compact interval.</p>
</blockquote>
<p><strong>Density, Density Function</strong>: </p>
<ul>
<li>We say that a distribution function $F$ has <strong>density</strong>, if there exists an almost everywhere continuous map $f : \mathbb{R}\to \mathbb{R}_+$ such that $f$ is locally bounded and</li>
</ul>
<p>$$<br>F(s)=\int^s_{-\infty}f(t)dt<br>$$</p>
<ul>
<li>for every real number $s$. </li>
<li>In this case, we say that $f$ is a <strong>density function</strong> for $F$.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Lemma 1.1</strong>: </p>
<ul>
<li>Let $\varphi,\ F\in \mathbf{B}[a,b]$ be such that $F$ is increasing. If $\varphi$ is $F$-integrable, then,</li>
</ul>
<p>$$<br>\int^b_a\varphi dF=\int^c_a\varphi dF+\int^b_c\varphi dF,\ \ \ \  a\leq c\leq b<br>$$</p>
<ul>
<li>The same conclusion also holds if $\varphi$ is $F $-integrable on both $[a, c]$ and $[c, b]$.</li>
</ul>
<blockquote>
<p>The Stieltjes integral is additive with respect to the interval of integration. </p>
</blockquote>
<p><strong>Lemma 1.2</strong>: </p>
<ul>
<li>Assume $a &lt; b$, and take any $\varphi, F\in \mathbf{B}[a,b]$ with $F$ being increasing. </li>
<li>If $F$ is continuous and $\varphi$ is $F $-integrable. Then, </li>
</ul>
<p>$$<br>\int^{\frac{b-1}{m}}_a\varphi dF\to\int^b_a\varphi dF\ \ \ and\  \ \ \ \int^b_{\frac{a+1}{m}}\varphi dF\to\int^b_a\varphi dF<br>$$</p>
<blockquote>
<p>When the integrator is a continuous function, altering the value of an integrable function at a single point does not alter the value of its integral</p>
</blockquote>
<p><strong>Lemma 1.3</strong>: </p>
<ul>
<li>Let $\varphi,\ F\in \mathbf{B}[a, b]$ be such that $F$ is increasing and $\varphi$ is $F $-integrable. </li>
<li>If $\varphi\geq 0$ almost everywhere on $[a, b]$, then</li>
</ul>
<p>$$<br>\int^b_a\varphi dF\geq0<br>$$</p>
<p><strong>Corollary 1.4</strong>: </p>
<ul>
<li>Let $\varphi, F \in \mathbf{B}[a, b]$ be such that $F$ is increasing and $\varphi$ is $F $-integrable. If $\varphi$ vanishes almost everywhere on $[a,b]$, then</li>
</ul>
<p>$$<br>\int^b_a\varphi  dF=0<br>$$</p>
<p><strong>Lemma 1.5</strong>: </p>
<ul>
<li>Let $\varphi, F \in \mathbf{B}[a, b]$ be such that $F$ is increasing, and both $\varphi$ and $\phi$ are $F$-integrable. Then, for any real number $\alpha$, we have</li>
</ul>
<p>$$<br>\int^b_a(\alpha\varphi+\phi)dF=\alpha\int^b_a\varphi dF+\int^b_a\phi dF<br>$$</p>
<blockquote>
<p>The Stieltjes integral is linear with respect to its integrand. </p>
</blockquote>
<p><strong>Proposition 1.6</strong>: (Stieltjes) </p>
<ul>
<li>Take any $\varphi, F\in \mathbf{B}[a, b]$ with $F$ being increasing. </li>
<li>If $\varphi$ is continuous, then it is $F$-integrable.</li>
</ul>
<p><strong>Proposition 1.7</strong>: </p>
<ul>
<li>Let $\varphi, F \in \mathbf{B}[a, b]$ and assume that $F$ is increasing and Lipschitz continuous. </li>
<li>If $\varphi$ is continuous almost everywhere on $[a,b]$, then it is $F $-integrable.</li>
</ul>
<p><strong>The Lebesgue Criterion</strong>: </p>
<ul>
<li>If $\varphi \in \mathbf{B}[a,b]$ is continuous almost everywhere on $[a,b] $, then it is Riemann integrable.</li>
</ul>
<blockquote>
<p>Riemann integrability = Continuity almost everywhere.</p>
</blockquote>
<p><strong>Corollary 1.8</strong>: </p>
<ul>
<li>Let $\varphi\in \mathbf{B}[a, b]$ be continuous almost everywhere on $[a, b]$. If $\varphi$ vanishes almost everywhere on $[a, b]$, then</li>
</ul>
<p>$$<br>\int^b_a\varphi(t)dt=0<br>$$</p>
<p><strong>The Fundamental Theorem of Calculus 1</strong>: </p>
<ul>
<li>Let $f \in \mathbf{B}[a, b]$ be continuous almost everywhere and $F\in \mathbb{R}^{[a,b]}$ satisfy</li>
</ul>
<p>$$<br>F(s)=F(a)+\int^s_af(r)dr,\ \ \ \ a\leq s\leq b<br>$$</p>
<ul>
<li>Then, $F$ is Lipschitz continuous on $[a,b]$, and for every $s\in [a,b]$ at which $f$ is continuous, $F’(s)$ exists and equals $f(s)$.</li>
</ul>
<p><strong>The Fundamental Theorem of Calculus 2</strong>: </p>
<ul>
<li><p>Take any $F\in \mathbf{C}^1[a,b]$, and let $f \in \mathbf{B}[a,b]$ is a Riemann integrable function such that $F’ = f$ almost everywhere on $[a, b]$. </p>
</li>
<li><p>Then we have<br>$$<br>F(s)=F(a)+\int^s_af(r)dr,\ \ \ \ a\leq s\leq b<br>$$</p>
</li>
</ul>
<blockquote>
<p>Roughly speaking, The Fundamental Theorem of Calculus says that we can think of dif ferentiation and Riemann integration as inverse operations.</p>
</blockquote>
<p><strong>Theorem 1.10</strong>: </p>
<ul>
<li>Let $\varphi\in \mathbf{B}[a, b]$ be continuous almost everywhere on $[a, b]$. </li>
<li>Then, for any increasing $F\in \mathbf{C}^1[a,b]$, we have</li>
</ul>
<p>$$<br>\int^b_a\varphi dF=\int^b_a\varphi(t)F’(t)dt<br>$$</p>
<p><strong>Corollary 1.11</strong>: </p>
<ul>
<li><p>Let $f,\varphi\in \mathbf{B}[a,b]$ be continuous almost everywhere on $[a,b]$ and assume $f \geq 0$. </p>
</li>
<li><p>Then, for any $F\in \mathbf{C}^1[a, b]$ such that<br>$$<br>F(s)=F(a)+\int^s_af(r)dr,\ \ \ \ a\leq s\leq b<br>$$</p>
</li>
<li><p>holds, we have</p>
</li>
</ul>
<p>$$<br>\int^b_a\varphi dF=\int^b_a\varphi(t)f(t)dt<br>$$</p>
<blockquote>
<p>Theorem 1.10 and Corollary 1.11 help to reduce a Stieltjes Integral to a Riemann Integral, gaining computational power.</p>
</blockquote>
<p><strong>Proposition 1.12</strong>: </p>
<ul>
<li>Let $\varphi$ and $F$ be two self-maps on $\mathbb{R} $, and assume that $F$ is increasing and continuously differentiable. </li>
<li>If $\varphi$ is nonnegative, locally bounded, and continuous almost everywhere, then it is $F $-integrable.</li>
</ul>
<p><strong>Theorem 1.13</strong>: </p>
<ul>
<li>Let $F\in \mathbf{C}^1(\mathbb{R} )$ be a distribution function, and $\varphi$ a locally bounded self-map on $\mathbb{R} $ that is continuous almost everywhere. </li>
<li>If $f$ is a density function for $F $, then</li>
</ul>
<p>$$<br>\int^\infty_{-\infty}\varphi dF=\int^\infty_{-\infty}\varphi(t)f(t)dt<br>$$</p>
<ul>
<li>in the sense that if one side exists so does the other.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T13:55:59.000Z" title="12/15/2020, 9:55:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/10%20Expectation%20via%20the%20Lebesgue%20Integral/Spaces-of-Integrable-Random-Variables/">Spaces of Integrable Random Variables</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>$p$-Integrable</strong></p>
<ul>
<li><p>Let $(X,\Sigma, \mathbf{p})$ be a measure space. </p>
</li>
<li><p>For any real number $p\geq 1$, we define $\mathcal{L}^p(X,\Sigma,\mu)$ as the set of all random variables $f$ on $(X,\Sigma,\mu)$ such that $|f|^p$ is integrable. </p>
</li>
<li><p>In other words,<br>$$<br>\mathcal{L}^p(X,\Sigma,\mu):=\left{f\in\mathcal{L}^0(X,\Sigma):\int_X|f|^pd\mu&lt;\infty\right}<br>$$</p>
</li>
<li><p>Any one member of $\mathcal{L}^p(X,\Sigma,\mu)$ is said to be $p$-<strong>integrable</strong>.</p>
</li>
</ul>
<blockquote>
<p>There is a natural way of making $\mathcal{L}^p(X,\Sigma,\mu)$ a seminormed linear space.</p>
<p>We define the real map $|\cdot|_p$ on $\mathcal{L}^p(X,\Sigma,\mu)$ by<br>$$<br>|f|_p:=\left(\int_X|f|^pd\mu\right)^\frac{1}{p}<br>$$<br>It is not a normed linear space proper, because $|\cdot|_p$ identifies any two random variables that are distinct from each other only on a negligible set with respect to $\mu$. </p>
<p>In other words, the map $(f, g) \mapsto |f- g|_p$ is a semimetric, but it is not a metric, for it fails to separate points in $\mathcal{L}^p(X,\Sigma,\mu)$. </p>
<p>The problem is that $|f|<em>p =0$ does not yield $f =\mathbf{0}$, it implies only that $f =</em>{a.s.} \mathbf{0}$.</p>
</blockquote>
<p><strong>$\mathcal{L}^p$-bounded</strong>:</p>
<ul>
<li>For any given real number $p\geq1$, we say that a set $\mathcal{X}$ of random variables on a given probability space is <strong>$\mathcal{L}^p$-bounded</strong> if either it is empty or<br>$$<br>\sup{\mathbb{E}(|x|^p):x\in\mathcal{X}}&lt;\infty<br>$$</li>
</ul>
<p><strong>$\mathcal{L}^p$-convergence</strong>:</p>
<ul>
<li><p>For any given real number $p\geq1$, let $x, x_1,x_2,\dots$ be random variables on $\mathcal{L}^p(X,\Sigma,\mu)$</p>
</li>
<li><p>We say that $x_m$ is <strong>$\mathcal{L}^p$-convergent</strong> if<br>$$<br>\lim_{m\to\infty}|x_m-x|<em>p=\lim</em>{m\to\infty}\left(\int_X|x_m-x|^pd\mu\right)^\frac{1}{p}=\lim_{m\to\infty}\mathbb{E}(|x_m-x|^p)=0<br>$$</p>
</li>
<li><p>$\mathcal{L}^1$-convergence is <strong>called convergence in the mean</strong>.</p>
</li>
</ul>
<blockquote>
<p>$\mathcal{L}^p$-convergence is a natural notion of convergence for sequences in this semimetric space.</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 5.4</strong>: </p>
<ul>
<li>Let $X$ be a nonempty set of random variables on a probability space $(X,\Sigma,\mathbf{p})$. </li>
<li>If $X$ is uniformly integrable, then it is $\mathcal{L}^1$-bounded.</li>
</ul>
<p><strong>Proposition 5.5</strong>:</p>
<ul>
<li>Let $X$ be a set of random variables on a probability space $(X,\Sigma,\mathbf{p})$.  </li>
<li>If $X$ is $\mathcal{L}^p$-bounded for some $p &gt; 1$, then it is uniformly integrable.</li>
</ul>
<blockquote>
<p>$$<br>\mathcal{L}^p\mbox{-boundeness}\ \ (p&gt;1)\Longrightarrow\mbox{uniform integrability}\Longrightarrow\mathcal{L}^1\mbox{-boundeness}<br>$$</p>
</blockquote>
<p><strong>Proposition 5.6</strong>:</p>
<ul>
<li>Let $x, x_1,x_2,\dots$ be integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$ such that $\mathbb{E}(|x_m -x|) \to 0$. </li>
<li>Then, $(x_m)$ is uniformly integrable.</li>
</ul>
<blockquote>
<p>$$<br>\mathcal{L}^1\mbox{-convergence}\Longrightarrow\mbox{uniform integrability}\Longrightarrow\mathcal{L}^1\mbox{-boundeness}<br>$$</p>
</blockquote>
<p><strong>Proposition 5.7</strong>: </p>
<ul>
<li>Let $(x_m)$ be a uniformly integrable sequence of random variables on a probability space $(X,\Sigma, \mathbf{p})$ such that $x_m\to_{a.s.} x$ for some $x \in \mathcal{L}^0(X,\Sigma)$</li>
<li>Then, $x$ is integrable and $\mathbb{E}(|x_m-x |) \to 0$.</li>
</ul>
<blockquote>
<p>Almost sure convergence does imply $\mathcal{L}^1$-convergence for uniformly integrable sequences.</p>
</blockquote>
<p><strong>Corollary 5.8</strong>: </p>
<ul>
<li>Let $x, x_1, x_2,\dots$ be integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$ such that $x_m\to_{a.s.} x$. </li>
<li>Then, $\mathbb{E}(|x_m-x|) \to 0$ iff, $(x_m)$ is uniformly integrable.</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example 5.1</strong>: </p>
<ul>
<li>Almost sure convergence does not imply $\mathcal{L}^1$-convergence. </li>
<li>Consider the sequence $(x_m)$ of random variables on the probability space  $([0,1],\mathcal{B}[0,1],\ell)$ where $x_m$ equals $m$ on $[0,\frac{1}{m}]$ and $0$ elsewhere on $[0,1]$. </li>
<li>Then, $x_m(\omega) \to0$ for each $\omega\in (0,1]$, and hence, $x_m \to_{a.s.} 0$. </li>
<li>But $|x_m|_1 = 1$ for each $m$.</li>
</ul>
<p><strong>Example 5.2</strong>: </p>
<ul>
<li>$\mathcal{L}^1$-convergence does not imply almost sure convergence. </li>
<li>Consider $(x_m) := (\mathbf{1}<em>{[0,1)},\mathbf{1}</em>{[0,\frac{1}{2})}, \mathbf{1}<em>{[\frac{1}{2},1)},\mathbf{1}</em>{[0,\frac{1}{3})}, \mathbf{1}<em>{[\frac{1}{3},\frac{2}{3})},\mathbf{1}</em>{[\frac{2}{3},1)},\dots),$ which is a sequence in $\mathcal{L}^1([0,1),\mathcal{B}[0,1),\ell)$. </li>
<li>Clearly, we have $\mathbb{E}(x_m)\to 0$, that is, $|x_m|_1\to 0$. </li>
<li>But $ (x_m(\omega))$ does not converge to a real number for any $\omega$ in $[0, 1)$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T13:54:59.000Z" title="12/15/2020, 9:54:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/10%20Expectation%20via%20the%20Lebesgue%20Integral/Elementary-Probability-Inequalities/">Elementary Probability Inequalities</a></h1><div class="content"><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Jensen’s Inequality</strong>:</p>
<ul>
<li><p>Let $x$ be an integrable random variable on a probability space $(X,\Sigma, \mathbf{p})$, $I$ an open interval that contains $x(X)$, and $\varphi : I \to \mathbb{R}$ a concave function. </p>
</li>
<li><p>Then, $\varphi \circ x \in \mathcal{L}^0(X ,\Sigma)$ and<br>$$<br>\mathbb{E}(\varphi\circ x) \leq \varphi(\mathbb{E}(x))<br>$$</p>
</li>
<li><p>If $\varphi$ is convex, then the inequality goes the other direction.</p>
</li>
</ul>
<blockquote>
<p>Under fairly general conditions, the expectation of a concave transformation of a random variable $x$ is always less than the same transformation of the expectation of $x$</p>
</blockquote>
<p><strong>Lemma 4.1</strong>:</p>
<ul>
<li><p>Let $Y$ be a metric space, and $x$ a $Y$-valued random variable on a probability space $(X,\Sigma, \mathbf{p})$. </p>
</li>
<li><p>Then, for any continuous $\varphi: Y \to \mathbb{R}_+$ and real number $\lambda &gt; 0$, we have<br>$$<br>\mathbf{p}{\varphi\circ x\geq \lambda}\leq\frac{1}{\lambda}\mathbb{E}(\varphi\circ x).<br>$$</p>
</li>
</ul>
<p><strong>Markov’s Inequality</strong>:</p>
<ul>
<li><p>Let $Y$ be a normed metric space, and $x$ a $Y$-valued random variable on a probability space $(X,\Sigma, \mathbf{p})$. </p>
</li>
<li><p>Then, for any real number $\lambda &gt; 0$, we have<br>$$<br>\mathbf{p}{| x|_Y\geq \lambda}\leq\frac{1}{\lambda}\mathbb{E}(| x|_Y).<br>$$</p>
</li>
<li><p>In particular, for any  random variable $x$ on $(x,\Sigma,\mathbf{p})$<br>$$<br>\mathbf{p}{| x|\geq \lambda}\leq\frac{1}{\lambda}\mathbb{E}(| x|).<br>$$</p>
</li>
</ul>
<p><strong>The Chebyshev-Bienaymé Inequality</strong>:</p>
<ul>
<li>For any random variable $x$ defined on a probability space $(X,\Sigma,\mathbf{p})$, and any real number $\lambda&gt; 0$, we have<br>$$<br>\mathbf{p}{| x|\geq \lambda}\leq\frac{1}{\lambda^2}\mathbb{E}(x^2).<br>$$</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T13:53:59.000Z" title="12/15/2020, 9:53:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/10%20Expectation%20via%20the%20Lebesgue%20Integral/The-Expectation-Functional-of-Arbitrary-Random-Variables/">The Expectation Functional of Arbitrary Random Variables</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Expectation of Arbitrary Random Variables</strong>:</p>
<ul>
<li>Let $x$ be an $\overline{\mathbb{R}}$-valued random variable on a probability space $(X,\Sigma, \mathbf{p})$. </li>
<li>Note that $x^+ := \max{x, 0}$ and $x^- := \max{-x,0}$ are $[0, \infty]$-valued random variables on $(X,\Sigma,\mathbf{p})$ with $x = x^++  x^-$ </li>
<li>Define the <strong>expectation</strong> of $x$ as the (extended real) number $\mathbb{E}(x) := \mathbb{E}(x^+) -\mathbb{E}(x^-)$, provided that $\mathbb{E}(x^+)$ and $\mathbb{E}(x^-)$ are not both infinite.<ul>
<li>If $\mathbb{E} (x^+) = \infty = \mathbb{E}(x^-)$, we say that the expectation of $x$ does not exist.</li>
</ul>
</li>
</ul>
<blockquote>
<p>This definition generalizes the one we gave for the expectation of $[0,\infty]$-valued random variables.</p>
</blockquote>
<blockquote>
<p>In real analysis, $\mathbb{E}(x)$ is called the Lebesgue integral of $x$ (with respect to $\mathbf{p}$), and is denoted by $\int_Xxd\mathbf{p}$.</p>
</blockquote>
<p><strong>Variance of Arbitrary Random Variables</strong>:</p>
<ul>
<li><p>In turn, the <strong>variance</strong> of an arbitrary random variable $x$ on $(X,\Sigma,\mathbf{p})$ is the number<br>$$<br>\mathbb{V}(x) := \mathbb{E}((x - \mathbb{E}(x))^2)<br>$$</p>
</li>
<li><p>where $(x-\mathbb{E}(x))^2$ is the arbitrary random variable $\omega\mapsto (x(\omega) -\mathbb{E}(x))^2$ on $(X,\Sigma, \mathbf{p})$.</p>
</li>
</ul>
<blockquote>
<p>However, it is often more convenient to use the alternate formula<br>$$<br>\mathbb{V}(x) := \mathbb{E}(x^2) - \mathbb{E}(x)^2<br>$$</p>
</blockquote>
<p><strong>Integrable</strong>:</p>
<ul>
<li>$x$ is <strong>integrable</strong> (with respect to $\mathbf{p}$) if $\mathbb{E}(|x|)=\mathbb{E}(x^+)+\mathbb{E}(x^-)&lt;\infty$ .</li>
</ul>
<blockquote>
<p>$\mathbb{E}(x)$ exists iff $\min{\mathbb{E}(x^+), \mathbb{E}(x^-)} &lt; \infty$, iff either $\mathbb{E}(x^+)$ or $\mathbb{E}(x^-)$ is finite</p>
<p>$x$ is integrable iff $\max{\mathbb{E}(x^+), \mathbb{E}(x^-)} &lt; \infty$, iff both $\mathbb{E}(x^+)$ and $\mathbb{E}(x^-)$ is finite</p>
<p>$x$ is integrable iff $\mathbb{E}(x)$ exists and it is finite.</p>
</blockquote>
<blockquote>
<p>Integrability of a random variable means simply that the expectation of this random variable is a real number.</p>
</blockquote>
<p><strong>Lebesgue Integration of Aribitrary Maps</strong>:</p>
<ul>
<li><p>Let $(X,\Sigma,\mu)$ be a measure space. </p>
</li>
<li><p>$f : X\to \overline{\mathbb{R}}$ a -measurable map. The <strong>Lebesgue integral of $f$ (with respect to $\mu$</strong>) is defined as the (extended real) number<br>$$<br>\int_Xfdu:=\int_Xf^+d\mu-\int_Xf^-d\mu<br>$$</p>
</li>
<li><p>provided that the right-hand side of this expression is not of the $\infty-\infty$ form. </p>
<ul>
<li>If the latter condition is not met, we say that the Lebesgue integral of $f$ (with respect to $\mu$) does not exist.</li>
</ul>
</li>
<li><p>For any $S\in\Sigma$, we also define<br>$$<br>\int_Sfd\mu:=\int_X\mathbf{1}_Sd\mu,<br>$$</p>
</li>
<li><p>provided that the right-hand side of this expression exists. </p>
</li>
</ul>
<p><strong>Integrable</strong>:</p>
<ul>
<li>$f$ is <strong>integrable</strong> (with respect to $\mu$) if $\int_X |f|d\mu &lt; \infty$.</li>
</ul>
<p><strong>Uniform Integrable</strong>:</p>
<ul>
<li><p>A collection $\mathcal{X}$ of random variables on a probability space $(X,\Sigma,\mathbf{p})$ is said to be <strong>uniformly integrable</strong> (with respect to $\mathbf{p}$) if<br>$$<br>\lim_{ a\to\infty}\sup\left{ \int_{ { \vert x\vert&gt;a} }\vert x\vert d\mathbf{p}:x\in\mathcal{X} \right }=0<br>$$</p>
</li>
<li><p>that is, for every $\varepsilon &gt; 0$, there is a real number $a &gt; 0$ such that<br>$$<br>\int_{ {|x|&gt;a } }|x|d\mathbf{p}&lt;\varepsilon \ \ \ \ \ \mbox{ for each }x\in\mathcal{X}<br>$$</p>
</li>
<li><p>In turn, a sequence $(x_m)$ of random variables on $(X,\Sigma,\mathbf{p})$ is called uniformly integrable if ${x_1, x_2,\dots}$ is <strong>uniformly integrable</strong></p>
</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 2.1</strong>: </p>
<p>For any $\mathbb{R}$-valued random variable $x$ on a probability space $(X,\Sigma,\mathbf{p})$ such that $\mathbb{E}(x)$ exists, we have $|\mathbb{E}(x)| \leq \mathbb{E}(|x|)$</p>
<p><strong>Proposition 2.2</strong>:</p>
<ul>
<li>Let $x$ and $y$ be $\overline{\mathbb{R}}$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$.</li>
<li>For any real number $a$, we have $\mathbb{E}(ax) = a\mathbb{E}(x)$, provided that $\mathbb{E}(x)$ exists.</li>
<li>Furthermore, $\mathbb{E}(x + y) = \mathbb{E}(x) + \mathbb{E}(y)$, provided that $x + y\in \overline{\mathbb{R}}^X$, $\mathbb{E}(x)$ exists, and $y$ is integrable.</li>
</ul>
<blockquote>
<p>The set of all integrable random variables on a probability space $(X,\Sigma,\mathbf{p})$ is denoted by $\mathcal{L}^1(X,\Sigma,\mathbf{p})$, that is,<br>$$<br>\mathcal{L}^1(X,\Sigma,\mathbf{p}):=\left{x\in\mathcal{L}^0(X,\Sigma,\mathbf{p}):\int_X|x|d\mathbf{p}\right}<br>$$<br>$\mathcal{L}^1(X,\Sigma,\mathbf{p})$ is a linear subspace of $\mathcal{L}^0(X,\Sigma)$.</p>
<p>The map $\mathbb{E}$ acts as a linear functional on this linear space.</p>
</blockquote>
<p><strong>Proposition 2.3</strong>:</p>
<ul>
<li><p>Let $x$ and $y$ be $\overline{\mathbb{R}}$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$.</p>
</li>
<li><p>If both $\mathbb{E}(x)$ and $\mathbb{E}(y)$ exist, then<br>$$<br>x\geq_{a.s.}y\ \ \ \ \mbox{ implies }\ \ \ \ \mathbb{E}(x) \geq \mathbb{E}(y)<br>$$</p>
</li>
</ul>
<p><strong>Corollary 2.4</strong>:</p>
<ul>
<li><p>Let $x$ and $y$ be $\overline{\mathbb{R}}$-valued random variables on a probability space $(X,\Sigma,\mathbf{p})$.</p>
</li>
<li><p>If both $\mathbb{E}(x)$ and $\mathbb{E}(y)$ exist, then<br>$$<br>x=_{a.s.}y\ \ \ \ \mbox{ implies }\ \ \ \ \mathbb{E}(x) = \mathbb{E}(y)<br>$$</p>
</li>
</ul>
<p><strong>The Monotone Convergence Theorem 2</strong>:</p>
<ul>
<li>Let $x, x_1,x_2,\dots$ be $\overline{\mathbb{R}}$-valued random variables on a probability space $(X,\Sigma, \mathbf{p})$. Then,<br>$$<br>\mathbb{E}(x_1)&gt;-\infty,\ \ \ x_m \uparrow_{a.s.} x\ \ \ \ \ \mbox{ implies }\ \ \ \ \ \ \mathbb{E}(x_m)\uparrow \mathbb{E}(x)<br>$$</li>
</ul>
<p><strong>The Change of Variables Formula</strong>:</p>
<ul>
<li><p>Let $Y$ be a metric space, $x$ a $Y$-valued random variable on a probability space $(X,\Sigma,\mathbf{p})$, and $\varphi$ an $\overline{\mathbb{R}}$-valued random variable on $(Y,\mathcal{B}(Y ),\mathbf{p}_x)$. </p>
</li>
<li><p>If either $\mathbb{E}_{\mathbf{p}_x} (\varphi)$ or $\mathbb{E}_\mathbf{p}(\varphi\circ x)$ exists, then<br>$$<br>\int_Y\varphi d\mathbf{p}_x=\int_X(\varphi\circ x)d\mathbf{p}<br>$$</p>
</li>
</ul>
<blockquote>
<p>The Change of Variables Formula remains valid in the context of any measure space. </p>
</blockquote>
<blockquote>
<p>Note that when $Y = \mathbb{R} $, the distribution function induced by $\mathbf {p}_x $is the distribution function of $x$, and denoted by $F_x$.</p>
<p>And the distribution induced by $F_x$ is the Lebesgue-Stieltjes probability measure actually, that is $\mathbf{p}_x=\ell_x$</p>
</blockquote>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example 2.5</strong>:</p>
<ul>
<li><p>Let $x$ be a nonnegative random variable on a probability space $\mathbb(X,\Sigma,\mathbf{p})$ such that $x(X)$ is countable.</p>
</li>
<li><p>The Change of Variables Formula says that the expectation of $x$ equals the Lebesgue integral of the identity function on $x(X)$ with respect to the probability measure $\mathbf{p}_x$.</p>
</li>
<li><p>Therefore,<br>$$<br>\mathbb{E}(x)=\sum_{a\in x(X)}a\mathbf{p}{x=a}<br>$$</p>
</li>
</ul>
<p><strong>Example 2.6, Geometric distribution</strong>:</p>
<ul>
<li><p>Take any $p\in (0,1)$ and let $x$ be an $\mathbb{N}$-valued random variable on a probability space $(X,\Sigma, \mathbf{p})$ such that $\mathbf{p}{x = i} = p(1-p)^{i-1}$ for each $i\in\mathbb{N}$. </p>
</li>
<li><p>Such a random variable is said to have a <strong>geometric distribution</strong> with parameter $p$.<br>$$<br>\mathbb{E}(x)=p+2p(1-p)+3p(1-p)^2 +\cdots=\frac{p}{1-p}\sum^\infty_{i=1}i(1-p)^i=\frac{p}{1-p}\frac{1-p}{1-(1-p)^2}=\frac{1}p<br>$$</p>
</li>
</ul>
<p><strong>Example 2.7, Poisson distribution</strong>:</p>
<ul>
<li><p>Given a positive real number  $\lambda&gt; 0$, a $\mathbb{Z}_+$-valued random variable $x$ on a probability space $(X,\Sigma, \mathbf{p})$ such that<br>$$<br>\mathbf{p}{x=i}=\frac{e^{-\lambda}\lambda^i}{i!},\ \ \ i=0,1,\dots<br>$$</p>
</li>
<li><p>is said to have a Poisson distribution with parameter $\lambda$.</p>
</li>
<li><p>Since $e^\lambda=1+\lambda+\frac{\lambda^2}{2}+\frac{\lambda^3}{3!}+\cdots$</p>
</li>
<li><p>We have $\mathbb{E}(x)=\lambda$, $\mathbb{E}(x^2)=\lambda+\lambda^2$ and $\mathbb{V}(x)=\lambda$.</p>
</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li><p>Consider the experiment of throwing a single die, that is, the probability space $(X, 2^X,\mathbf{p})$ where $X := [6]$ and $\mathbf{p}(S) := \frac{|S|}{6} $ for all $S\in 2^X$.</p>
</li>
<li><p>Consider the random variable $x\in[2]^X$ which is defined as<br>$$<br>x(\omega):=\left{\begin{matrix}1,&amp;\mbox{ if }\omega\mbox{ is even}\2, &amp;\mbox{ if }\omega\mbox{ is odd}\end{matrix}\right.<br>$$</p>
</li>
<li><p>In addition, define $\varphi : [2] \to \mathbb{R}$ by $\varphi(t) := t^2$, and observe that $\varphi$ is a ${1,4}$-valued random variable on $([2], 2^{[2]},\mathbf{p}_x)$.</p>
</li>
<li><p>Clearly, we have $\mathbb{E}_{\mathbf{p}_x} (\varphi) = (1) \frac{1}{2} + (4) \frac{1}{2} = \frac{5}{2}$.</p>
</li>
<li><p>On the other hand, $\varphi\circ x$ is a ${1,4}$-valued random variable on $(X,2^X,\mathbf{p})$ that takes value $1$ if the outcome of the experiment is even and $4$ if the outcome is odd. </p>
</li>
<li><p>Thus, we have $\mathbb{E}_{\mathbf{p}} (\varphi\circ x) = (1) \frac{1}{2} + (4) \frac{1}{2} = \frac{5}{2}$.</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T13:52:59.000Z" title="12/15/2020, 9:52:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/10%20Expectation%20via%20the%20Lebesgue%20Integral/The-Expectation-Functional-of-Simple-Random-Variables/">The Expectation Functional of Simple Random Variables</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Expectation of Simple Random Variables</strong>:</p>
<ul>
<li><p>Let $(X,\Sigma, \mathbf{p})$ be a probability space. </p>
</li>
<li><p>The set of all simple random variables on $(X,\Sigma, \mathbf{p})$ is a linear space relative to the pointwise defined addition and scalar multiplication operations.</p>
</li>
<li><p>By definition, for any simple random variable $x$ on $(X,\Sigma, \mathbf{p})$, we have $|x(X)| &lt; \infty$ and<br>$$<br>x=\sum_{a\in x(X)}a\mathbf{1}_{x=a}<br>$$</p>
</li>
<li><p>where $\mathbf{1}_{ {x=a} }$ stands for the indicator function of the event ${x = a}\in\Sigma$ on $X$.</p>
</li>
<li><p>We define the <strong>expectation</strong> of any such $x$ as the real number<br>$$<br>\mathbb{E}(x):=\sum_{a\in  x(X)}a\mathbf{p}{x=a}<br>$$</p>
</li>
<li><p>Thus, the expected value of $x$ is the weighted average of its values, where the weight of $a$ is $\mathbf{p}<em>x{a}$ for each $a\in x(X)$. That is, $\mathbf{E}(x) =\sum</em>{a\in x(X)}\mathbf{p}_x{a}$.</p>
</li>
</ul>
<blockquote>
<p>Linearity of $\mathbb{E}$:<br>$$<br>\mathbb{E}(x+y)=\mathbb{E}(x)+\mathbb{E}(y)<br>$$<br>Monotonicity of $\mathbb{E}$:<br>$$<br>\mathbb{E}(x) \geq \mathbb{E}(y)\mbox{ whenever }x\geq_{a.s.}y<br>$$</p>
<p>$$<br>\mathbb{E}(x)= \mathbb{E}(y)\mbox{ whenever }x=_{a.s.}y<br>$$</p>
</blockquote>
<p><strong>Variance of Simple Random Variables</strong>:</p>
<ul>
<li><p>The <strong>variance</strong> of a simple random variable $x$ on $(X,\Sigma,\mathbf{p})$ is the number<br>$$<br>\mathbb{V}(x) := \mathbb{E}((x - \mathbb{E}(x))^2)<br>$$</p>
</li>
<li><p>where $(x-\mathbb{E}(x))^2$ is the simple random variable $\omega\mapsto (x(\omega) -\mathbb{E}(x))^2$ on $(X,\Sigma, \mathbf{p})$.</p>
</li>
</ul>
<blockquote>
<p>However, it is often more convenient to use the alternate formula<br>$$<br>\mathbb{V}(x) := \mathbb{E}(x^2) - \mathbb{E}(x)^2<br>$$</p>
</blockquote>
<p><strong>Expectation of Nonnegative Random Variables</strong></p>
<ul>
<li><p>Let $x$ be a $[0, \infty]$-valued random variable on a probability space $(X,\Sigma,\mathbf{p})$. </p>
</li>
<li><p>We define the <strong>expectation</strong> of $x$ as the (extended real) number<br>$$<br>\mathbb{E}(x) := \sup{\mathbb{E}(z) : z \in \mathfrak{L}(x)}<br>$$</p>
</li>
<li><p>where $\mathfrak{L}(x)$ stands for the set of all simple random variables $z$ such that $z\leq x$.</p>
</li>
<li><p>In turn, the <strong>variance</strong> of $x$ is defined as the (extended real) number<br>$$<br>\mathbb{V}(x) := \mathbb{E}((x - \mathbb{E}(x))^2),<br>$$</p>
</li>
<li><p>provided that $\mathbb{E}(x) &lt; \infty$. </p>
</li>
<li><p>where $(x-\mathbb{E}(x))^2$ is the simple random variable $\omega\mapsto (x(\omega) -\mathbb{E}(x))^2$ on $(X,\Sigma, \mathbf{p})$.</p>
</li>
</ul>
<blockquote>
<p>These definitions agree with those in the case of simple random variables on $(X,\Sigma, \mathbf{p})$. Moreover, they extend those definitions to the case of $[0,\infty]$-valued simple random variables.</p>
</blockquote>
<blockquote>
<p>In words, the (extended real) number $\mathbb{E}(x)$ is defined as the supremum of the weighted averages of all those simple random variables that are “smaller than” $x$ everywhere. </p>
<p>So, in this sense, the idea behind the definition of $\mathbb{E}(x)$ is reminiscent of that of the computation of the area under a given curve in $\mathbb{R}\times \mathbb{R}_+$ by approximating this area with the sum of the areas of the rectangles that lie under the curve and above the horizontal axis.</p>
<p>Put this way, you should see that $\mathbb{E}(x)$ can be thought of as some sort of an integral of $x$ – it is called the <strong>Lebesgue integral</strong> of $x$ with respect to $\mathbf{p}$ –where the sets in the domain of $x$ (analogous to the bases of the rectangles under the curve) are “measured” according to the underlying probability measure.</p>
</blockquote>
<blockquote>
<p>The commonly used notation for the Lebesgue integral of $x$ on $X$ with respect to $\mathbf{p}$ is $\int_Xxdp$, that is<br>$$<br>\int_X xd\mathbf{p}\ \mbox{ and }\ \mathbb{E}(x)<br>$$<br>denote the same (extended real) number. </p>
</blockquote>
<blockquote>
<p>Adopting the widely used conventions of integration theory, we also set<br>$$<br>\int_Sxd\mathbf{p}:=\mathbb{E}(x\mathbf{1}_S)\mbox{ for any } S\in \Sigma,<br>$$<br>where $\mathbf{1}_S$ is the indicator function of $S$ on $X$. Therefore, recalling the definition of the expectation of a simple random variable, we see that $\int_S d\mathbf{p}$, that is, the Lebesgue integral of the constant function $1$ with respect to $\mathbf{p}$ on $S$, and $\mathbb{E}(\mathbf{1}_S )$ denote the same number, namely, $\mathbf{p}(S)$, for any $S\in\Sigma$.</p>
</blockquote>
<blockquote>
<p>Many authors write<br>$$<br>\int_X x(\omega)\mathbf{p}(dw)<br>$$<br>instead of $\int_Xxd\mathbf{p}$. The Lebesgue integral of the map $\omega\mapsto\omega^2$ on the probability space $([0,1],\mathcal{B}[0,1],\ell)$ is, for instance, written as $\int_{[0,1]}\omega^2\ell(d\omega)$. </p>
</blockquote>
<p><strong>Lebesgue Integration of Simple Maps</strong>:</p>
<ul>
<li><p>Let $(X,\Sigma,\mu )$ be a measure space. </p>
</li>
<li><p>By <strong>a nonnegative simple $\Sigma$-measurable map</strong> on $X$, we mean a real function $f\in \mathcal{L}^0(X,\Sigma)$ such that $f\geq0$ and $f(X)$ is finite. </p>
</li>
<li><p>The <strong>Lebesgue integral</strong> of any such map with respect to $\mu$ is defined as the number<br>$$<br>\int_Xfd\mu:=\sum_{a\in f(X)}a\mu{f=a},<br>$$</p>
</li>
<li><p>where we adopt the convention that $0\cdot\infty$ equals $0$.</p>
</li>
</ul>
<p><strong>Lebesgue Integration of Nonnegative Maps</strong>:</p>
<ul>
<li><p>Let $(X,\Sigma,\mu)$ be a measure space. </p>
</li>
<li><p>The <strong>Lebesgue integral of any $\Sigma$-measurable</strong> $f : X \to [0, \infty]$ with respect to $\mu$ is defined as the (extended) real number<br>$$<br>\int_X fd\mu:=\sup{\int_Xhd\mu:h\in\mathfrak{M}(f)}<br>$$</p>
</li>
<li><p>where $\mathfrak{M}(f)$ stands for the set of all nonnegative simple $\Sigma$-measurable maps $h$ on $X$ with $h\leq f$. </p>
</li>
<li><p>In turn, we define<br>$$<br>\int_S fd\mu:=\int_X f\mathbf{1}_S d\mu\ \ \ \ \mbox{ for any }S\in\Sigma<br>$$</p>
</li>
<li><p>where $\mathbf{1}_S$ is the indicator function of $S$ on $X$.</p>
</li>
</ul>
<p><strong>$\sigma$-finite Measure</strong>:</p>
<ul>
<li>Let $(X,\Sigma,\mu)$ be a measure space. </li>
<li>We say that this space (or $\mu$ itself) is <strong>$\sigma$-finite</strong> if there is a countable partition $\mathcal{S}$ of $X$ such that $\mathcal{S}\subseteq \Sigma$ and $\mu(S) &lt; \infty$ for each $S \in\mathcal{S}$</li>
</ul>
<p><strong>Absolutely Continuous, Density Function, Density</strong>:</p>
<ul>
<li><p>Let $F$ be a distribution function. </p>
</li>
<li><p>We say that $F$ is <strong>absolutely continuous</strong> (with respect to the Lebesgue measure) if there is a Borel measurable map $f : \mathbb{R} \to \mathbb{R}<em>+$ such that<br>$$<br>F(t)=\int</em>{(-\infty,t]}fd\ell<br>$$</p>
</li>
<li><p>for every real number $t$.</p>
</li>
<li><p>When this is the case, we say that $F$ is induced by the <strong>density function</strong> $f$, and refer to $f$ as a <strong>density</strong> for $F$.</p>
</li>
</ul>
<blockquote>
<p>When $f$ is a density for $F$, $F(t)-F(s)=\int_{(s,t]}fd\ell$ for any real numbers $s$ and $t$ with $t &gt; s$.</p>
</blockquote>
<blockquote>
<p>Some distribution functions do not have closed form decriptions, but they are rather defined through integrating a density function.</p>
<p>The most important example of such a function is the so-called <strong>normal distribution</strong> function (with parameters $\mu $ and $\sigma$). For any given real numbers $\mu$ and $\sigma&gt;0$, this function $F$ is defined by<br>$$<br>f(t)=\frac{1}{\sqrt{2\pi}}e^{\frac{(t-\mu)^2}{2\sigma^2}}<br>$$<br>for any real number $t$, it is thus trivially absolutely continuous.</p>
<p>If  $\mu= 0$ and $\mu = 1$ here, this function is called the <strong>standard normal distribution</strong> <strong>function</strong>.</p>
</blockquote>
<blockquote>
<p>Not all distribution function arises from a density funtion.</p>
</blockquote>
<p><strong>Absolutely Continuous Measures</strong>:</p>
<ul>
<li>Let $(X,\Sigma)$ be a measurable space, and $\mu$ and $\nu$  two measures on $\Sigma$. </li>
<li>We say that $\mu$ is <strong>absolutely continuous with respect to $\nu$</strong>, this is denoted by writing    $\mu\ll\nu$ </li>
<li>if $\mu(S) = 0$ for every $S\in\Sigma$  with $\nu(S) = 0$.</li>
</ul>
<blockquote>
<p>The zero measure on $\Sigma$ is absolutely continuous with respect to any measure on $\Sigma$</p>
<p>While the counting measure on $\Sigma$ is not absolutely continuous with respect to any measure $\nu$ on $\Sigma$  such that $\nu(S) = 0$ for some nonempty $S\in\Sigma$ .</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1.1</strong>: </p>
<ul>
<li><p>For any $[0,\infty]$-valued random variables $x$ and $y$ (on a given probability space),<br>$$<br>x\geq_{a.s.}y\ \ \ \ \mbox{ implies }\ \ \ \ \mathbb{E}(x) \geq \mathbb{E}(y)<br>$$</p>
<p>$$<br>x=_{a.s.}y\ \ \ \ \mbox{ implies }\ \ \ \ \mathbb{E}(x) = \mathbb{E}(y)<br>$$</p>
</li>
</ul>
<p><strong>The Monotone Convergence Theorem 1</strong>:</p>
<ul>
<li>Let $x, x_1,x_2,\dots$ be $[0,\infty]$-valued random variables on a probability space $(X,\Sigma, \mathbf{p})$. Then,<br>$$<br>x_m \uparrow_{a.s.} x\ \ \ \ \ \mbox{ implies }\ \ \ \ \ \ \mathbb{E}(x_m)\uparrow \mathbb{E}(x)<br>$$</li>
</ul>
<p><strong>Proposition 1.2</strong>: </p>
<ul>
<li>Let $x$ and $y$ be two $[0,\infty]$-valued random variables on a probability space $(X,\Sigma, \mathbf{p})$. Then, for any $a\geq 0$,<br>$$<br>\mathbb{E}(ax+y)=a\mathbb{E}(x)+\mathbb{E}(y)<br>$$</li>
</ul>
<p><strong>Proposition 1.3</strong>: </p>
<ul>
<li><p>Let $(X,\Sigma,\mu )$ be a finite measure space. Then, there is a real number  $\lambda\geq 0$ and a probability measure $\mathbf{p}$ on $\Sigma$ such that<br>$$<br>\int_Xfd\mu=\lambda\int_Xfd\mathbf{p}<br>$$</p>
</li>
<li><p>for every $\Sigma$-measurable $f : X \to [0,\infty]$.</p>
</li>
</ul>
<p><strong>Proposition 1.5</strong>:</p>
<ul>
<li><p>Given any measure space $(X,\Sigma,\mu )$,<br>$$<br>\int_x(af+g)d\mu=a\int_Xfd\mu+\int_Xgd\mu<br>$$</p>
</li>
<li><p>for every $a\geq0$ and $\Sigma$-measurable $f : X \to [0,\infty]$.</p>
</li>
</ul>
<p><strong>Proposition 1.6</strong>: </p>
<ul>
<li>Every absolutely continuous distribution function is uniformly continuous.</li>
</ul>
<blockquote>
<p>If a distribution function is not continuous even at a single point, then it cannot possibly possess a density.</p>
<p>The converse of Proposition 1.6 is false. Indeed, even a continuous distribution function need not possess a density.</p>
</blockquote>
<p><strong>Proposition 1.7</strong>:</p>
<ul>
<li><p>Let $(X,\Sigma,\nu )$ be a measure space and take any $f\in \mathcal{L}^0_+(X,\Sigma )$. </p>
</li>
<li><p>Then, the map $\mu:\Sigma \to\mathbb{R}$ defined by<br>$$<br>\mu(S):=\int_S fd\nu,<br>$$</p>
</li>
<li><p>is a measure on $\Sigma$ with  $\mu\ll\nu$ . </p>
</li>
<li><p>If $\nu$ is $\sigma$-finite, so is $\mu$.</p>
</li>
</ul>
<p><strong>The Radon-Nikodym Theorem</strong>:</p>
<ul>
<li>Let $(X,\Sigma)$ be a measurable space, and $\mu$ and $\nu$ two $\sigma$-finite measures on $\Sigma$ with $\mu\ll\nu$. Then, there is an $f\in \mathcal{L}^0_+ ( X ,\Sigma)$ such that<br>$$<br>\mu(S)=\int_Sfd\nu\mbox{ for every }S\in\Sigma<br>$$</li>
</ul>
<blockquote>
<p>Any map $f\in\mathcal{L}^0_+(X,\Sigma)$ such that above equation holds is commonly referred to as either the <strong>density of $\mu$ with respect to $\nu$</strong> or as the <strong>Radon-Nikodym derivative of $\mu$ with respect to $\nu$</strong>. </p>
<p>It is quite standard to denote any such map as $\frac{d\mu}{d\nu}$.</p>
</blockquote>
<p><strong>Proposition 1.8</strong>:</p>
<ul>
<li>Let $F$ be a distribution function and $\mathbf{p}_F$ the Lebesgue-Stieltjes probability measure on $\mathbb{R}$ induced by $F$. </li>
<li>Then, $F$ is absolutely continuous iff, $\mathbf{p}_F$ is absolutely continuous with respect to $\ell$.</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example 1.1, Binomial Distribution</strong>: </p>
<ul>
<li><p>Let $n$ be a positive integer and $p$ a number in $[0, 1]$.</p>
</li>
<li><p>A ${0,\dots, n}$-valued random variable $x$ on a probability space $(X,\Sigma,\mathbf{p})$ such that<br>$$<br>\mathbf{p}{x=i}=\left(\begin{matrix}n\ i\end{matrix}\right)p^i(1-p)^{n-i},\ \ \ \ i=0,\dots,n<br>$$</p>
</li>
<li><p>is said to have a binomial distribution with parameters $n$ and $p$</p>
</li>
<li><p>If $n = 1$ here, we say that $x$ has a Bernoulli distribution with parameter $p$</p>
</li>
<li><p>When $n = 1$, we have $\mathbb{E}(x) = p(1) + (1-p)(0) = p$, that is, the expected value of any random variable that has a Bernoulli distribution with parameter $p$ is $p$.</p>
</li>
<li><p>In this case, we also find that $\mathbb{E}(x^2) = p$, so $\mathbb{V}(x) = p -p^2$</p>
</li>
<li><p>More generally, for any random variable that has a binomial distribution with parameters $n$ and $p$, we have<br>$$<br>\mathbb{E}(x)=\sum^n_{i=0}\left(\begin{matrix}n\ i \end{matrix}\right)p^i(1-p)^{n-i}i=np\sum^{n-1}_{i=0}\left(\begin{matrix}n-1\ i \end{matrix}\right)p^i(1-p)^{(n-1)-i}<br>$$</p>
</li>
<li><p>so, by the Binomial Theorem, $\mathbb{E}(x) = np(p + (1 -p))^{n-1} = np$. By using a similar</p>
<p>method, we can also show that $\mathbb{E}(x^2) = n^2 p^2 -np^2 + np$, so $\mathbb{V}(x) = np(1-p)$.</p>
</li>
</ul>
<p><strong>Example 1.3</strong>:</p>
<ul>
<li><p>Let $(X,2^X,\mathbf{p})$ be a probability space with $X$ being a countable set. </p>
</li>
<li><p>We wish to find an expression for the expectation of an arbitrary nonnegative random variable $x$ on $(X,2^X,\mathbf{p})$.</p>
</li>
<li><p>Assume that $X$ is countably infinite, and enumerate it as $X := {\omega_1,\omega_2,\dots}$. </p>
</li>
<li><p>For every positive integer $m$, let us define the simple random variable<br>$$<br>x_m(\omega)=\left{\begin{matrix}x(\omega),&amp;\mbox{ if }\omega\in{\omega_1,\dots,\omega_m}\0,&amp;\mbox{otherwise}\end{matrix}\right.<br>$$</p>
</li>
<li><p>By definition of $\mathbb{E}$ for simple random variables, we have $\mathbb{E}(x_m) =\sum_{i\in[m]} x(\omega_i)\mathbf{p}{\omega_i}$ for each $m$.</p>
</li>
<li><p>But, $x_m \uparrow x$, so we have $\mathbb{E}(x_m)\uparrow \mathbb{E}(x)$ by the Monotone Convergence Theorem 1. Consequently,<br>$$<br>\mathbb{E}(x)=\lim\mathbb{E}(x_m)=\lim\sum_{i\in[m]}x(\omega_i)\mathbf{p}{\omega_i}=\sum^\infty_{i=1}x(\omega_i)\mathbf{p}{\omega_i}=\sum_{x\in X}x(\omega)\mathbf{p}{\omega}<br>$$</p>
</li>
<li><p>for any nonnegative random variable $x$ on $(X,2^X,\mathbf{p})$.</p>
</li>
</ul>
<p><strong>Example 1.5</strong>: </p>
<ul>
<li><p>For any real numbers $a$ and $b$ with $a &lt; b$, the uniform distribution $F$ on $[a,b]$ is absolutely continuous.</p>
</li>
<li><p>The map $f:\mathbb{R}\to\mathbb{R}+$ where<br>$$<br>f(t)=\left{\begin{matrix}\frac{1}{a-b},&amp;0\leq t\leq b\0,&amp;\mbox{otherwise}\end{matrix}\right.<br>$$</p>
</li>
<li><p>is a density for that distribution function. </p>
</li>
<li><p>Similarly, for any $\lambda &gt; 0$, the exponential distribution with parameter $\lambda$ ,is absolutely continuous.</p>
</li>
<li><p>The map $f:\mathbb{R}\to\mathbb{R}+$ where<br>$$<br>f(t)=\left{\begin{matrix}\lambda e^{-\lambda t},&amp; t\geq 0\0,&amp;\mbox{otherwise}\end{matrix}\right.<br>$$</p>
</li>
<li><p>is a density for that distribution function. </p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T13:51:59.000Z" title="12/15/2020, 9:51:59 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-E-Probability-via-Measure-Theory/">1.1.E Probability via Measure Theory</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/9%20Random%20Variables/The-Distribution-of-a-Random-Variable/">The Distribution of a Random Variable</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Image Measure of $\mu$ by $f$</strong>: </p>
<ul>
<li>Let $(X_1,\Sigma_1)$ and $(X_2,\Sigma_2)$ be two measurable spaces, and $f : X_1\to X_2$ a $\Sigma_1\backslash\Sigma_2$-measurable function. </li>
<li>For any measure $\mu$ on $\Sigma_1$, the map $\mu_f : \Sigma_2 \to [0, \infty]$, defined by</li>
</ul>
<p>$$<br>\mu_f(S):=\mu(f^{-1}(S))<br>$$</p>
<ul>
<li>is called <strong>the image measure of $\mu$ by $f$.</strong></li>
</ul>
<p><strong>Distribution</strong>: </p>
<ul>
<li>Let $Y$ be a metric space. </li>
<li>For any $Y $-valued random variable $x$ on a probability space $(X,\Sigma,\mathbf{p})$, the image measure of $\mu$ by $x$ (where we think of $x$ as $\Sigma\backslash \mathcal{B}(Y )$-measurable) is called the <strong>distribution</strong> of $x$</li>
<li>that is, $\mathbf {p}_x$ is the Borel measure on $Y$ such that</li>
</ul>
<p>$$<br>\mathbf{p}_x(S):=\mathbf{p}(x^{-1}(S))\ \ \mbox{ for each }\ S\in\mathcal{B}(Y)<br>$$</p>
<blockquote>
<p>For the single measure on the probability space $(X,\Sigma,\mathbf{p})$, if we have different random variables, we will have different distributions.</p>
</blockquote>
<blockquote>
<p>For any metric space $Y$, the distribution $\mathbf{p}_x$ of a $Y$-valued random variable $x$ on a probability space $(X,\Sigma,\mathbf{p})$ describes the probabilities for every event that may involve $x$. </p>
</blockquote>
<p><strong>Distribution Function</strong>: </p>
<ul>
<li>When $x$ is a random variable that is, $Y = \mathbb{R} $, the distribution function induced by $\mathbf {p}_x $is called the <strong>distribution function</strong> of $x$. </li>
<li>We denote this distribution function by $F_x$, and say that $x$ is <strong>continuous</strong> if $F_x$ is a continuous function. </li>
<li>On the other hand, we say that $x$ is discrete if $F_x$ is a <strong>discrete</strong> distribution function.</li>
</ul>
<blockquote>
<p>We often describe the probabilistic behavior of a random variable by specifying its distribution function directly. So, the statement<br>$$<br>x \mbox{ is a random variable on }X \mbox{ with the distribution function }F<br>$$<br>means that $x$ is a random variable on some probability space $(X,\Sigma,\mathbf{p})$ such that the distribution of $x$ is the Lebesgue-Stieltjes probability measure induced by $F$, that is, $F = F_x$.</p>
</blockquote>
<blockquote>
<p>For the single measure on the probability space $(X,\Sigma,\mathbf{p})$, if we have different random variables, we will also have different distribution functions.</p>
</blockquote>
<p><strong>Equal $\mu$-almost everywhere, Equal almost surely</strong>: </p>
<ul>
<li>Let $Y$ be a separable metric space, and $x$ and $y$ two $Y $-valued random variables on a measure space $(X,\Sigma,\mu)$. </li>
<li>If $\mu{x\neq y}=0$, we say that $x$ and $y$ are <strong>equal $\mu$-almost everywhere</strong>, and write $x=_{\mu-a.e.} y$. </li>
<li>But if $\mu$ is a probability measure, and $x$ and $y$ are equal $\mu$-almost everywhere; that is, when $\mu{x = y} = 1$, we say that $x $ and $y$ are <strong>equal almost surely</strong> (assuming that $\mu$ is understood from the context), and write $x =_{a.s.} y$.</li>
</ul>
<p><strong>$x$ large than $y$ $\mu$-almost everywhere, larger than $y$ almost surely</strong>: </p>
<ul>
<li>Let $x$ and $y$ be two $\overline{\mathbb{R}}$-valued random variables on a measure space $(X,\Sigma,\mu)$. If ${x &lt; y} = 0$, we say that $x$ is <strong>larger than $y$ $\mu$-almost everywhere</strong>, and write $x\geq_{\mu-a.e.} y$. </li>
<li>But if $\mu$ is a probability measure, and $x$ is larger than $y$ $\mu$-almost everywhere, that is, when ${x \geq y} = 1$, we say that $x$ is <strong>larger than $y$ almost surely</strong> (assuming that $\mu$ is understood from the context), and write $x\geq_{a.s.} y$. </li>
<li>The expressions “$x\leq_{\mu-a.e.}y$” and “$x\leq_{a.s.} y$” are understood similarly.</li>
</ul>
<p><strong>Converge Almost Surely</strong>: </p>
<ul>
<li>Let $Y$ be a separable metric space, and let $x,x_1,x_2,\cdots$ be a $Y$-valued random variables on a probability space $(X,\Sigma, \mathbf{p})$. </li>
<li>We say that $(x_m)$ <strong>converges almost surely to $x$</strong> (or that $x$ is an <strong>almost sure limit</strong> of $(x_m)$) whenever $\mathbf{p}{x_m\to x}=1$, that is,</li>
</ul>
<p>$$<br>\mathbf{p}{\omega\in X : d_Y (x_m(\omega),x(\omega)) \to 0} = 1.<br>$$</p>
<ul>
<li>In this case, we write $x_m\to_{a.s.} x$, but if $x$ is a constant map, say, $x =\nu $ for some $\nu\in Y$, we abuse notation and write $x_m\to_{a.s.}\nu $.</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Uniform Distribution</strong>: </p>
<ul>
<li>Let $a $ and $b$ be two real numbers with $a &lt; b$. </li>
<li>Consider the distribution function $F$ where $F |<em>{(-\infty,a)} = 0,\ F|</em>{(b,\infty) }= 1$, and</li>
</ul>
<p>$$<br>F(t)=\frac{t-a}{b-a},\ \ \ a\leq t\leq b.<br>$$</p>
<ul>
<li>This distribution function, as well as the Lebesgue-Stieltjes probability measure induced by it, is called the uniform distribution on $[a, b]$.</li>
<li>In turn, a random variable with the uniform distribution on $[a,b]$ is often referred to as a “random variable which is uniformly distributed on $[a, b]$”.</li>
</ul>
<p><strong>Exponential Distribution</strong>: </p>
<ul>
<li>Let $\lambda$ be a positive real number. </li>
<li>Consider the distribution function $F$ where $F|_{(-\infty;0)} = 0$ and</li>
</ul>
<p>$$<br>F(t)=1-e^{-\lambda t}, \ \ \ \ t\geq0.<br>$$</p>
<ul>
<li>This distribution function, as well as the Lebesgue-Stieltjes probability measure induced by it, is called the exponential distribution with parameter $\lambda$. </li>
<li>In turn, a random variable with the exponential distribution with parameter  is often referred to as a “random variable which is exponentially distributed with parameter $\lambda$”.</li>
</ul>
<p><strong>Remark 2.1</strong>:</p>
<ul>
<li><p>There is a one-to-one correspondence between any two of the following three concepts:</p>
<ol>
<li><p>Borel probability measures on $\mathbb{R} $</p>
</li>
<li><p>Distribution functions</p>
</li>
<li><p>Random variables on $ ((0, 1), \mathcal{B}(0, 1),\ell)$, (with different distributions $\ell_x$, actually)</p>
<ul>
<li><p>Every random variable transfers a measure space to a Borel probability space. The measure space is the same, but different random variables transfer it into different Borel probability spaces.</p>
</li>
<li><p>The Lebesgue measure on the measure space equals the probability measure on the Borel measure space.</p>
<ul>
<li><p>If $\mathbf{p}$ is induced by a strictly increasing distribution function, say $F$. </p>
</li>
<li><p>Since $F$ is then a bijection from $\mathbb{R}$ onto $(0,1)$, it is invertible, and we may define in this case $x := F^{-1}$</p>
</li>
<li><p>Since $x$ is increasing, we have $x\in \mathcal{L}^0((0,1), \mathcal{B}(0,1))$. </p>
</li>
<li><p>Moreover,<br>$$<br>\ell_x(a, b] = \ell(x^{-1}(a), x^{-1}(b)] = F (b)-  F (a) = \mathbf{p}(a, b]<br>$$<br>for any $-\infty \leq a \leq b &lt; 1$<br>$$<br>\ell_x(a,\infty) = 1 - F (a) = \mathbf{p}(a,\infty)<br>$$<br>for any $-\infty\leq  a$.</p>
</li>
<li><p>Thus $\ell_x$ and $\mathbf{p}$ agree on the semialgebra of all right-semiclosed intervals. </p>
</li>
<li><p>If $F$ is not invertible, need to define</p>
</li>
<li><p><strong>Pseudo-Inverse</strong>: Define the pseudo-inverse of a distribution function $F$ as the map $F^{-1} :(0,1)\to \mathbb{R} $ with<br>$$<br>F^{-1}(\omega) := \inf {t\in \mathbb{R} : F(t) \geq\omega}.<br>$$</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ul>
<p><strong>Example 2.3</strong>:</p>
<ul>
<li>Consider the probability space $(\mathbb{N},2^\mathbb{N},\mathbf{p})$ where $\mathbf{p}{i} = 2^{-i}$ for each $i\in \mathbb{N}$. </li>
<li>Any two ($Y$-valued) random variables $x$ and $y$ on this space is almost surely equal to each other is $x = y$.</li>
</ul>
<p><strong>Example 2.3</strong>:</p>
<ul>
<li>Consider the probability space $([0,1], \mathcal{B}[0, 1], \ell)$ and take any two random variables $x$ and $y$ on this space. If $x|<em>{[0,1]\setminus\mathbb{Q} }= y|</em>{[0,1]\setminus\mathbb{Q}}$, then $x =_{a.s.} y$. </li>
<li>Of course, we can replace $\mathbb{Q}$ with any countable subset of $[0, 1]$ in this observation.</li>
</ul>
<p><strong>Example 2.4</strong>: </p>
<ul>
<li>Consider the probability space $([0,1], \mathcal{B}[0, 1], \ell)$.</li>
<li>Consider the sequence $(x_m) $ in $\mathbb{R}^{[0,1]}$ where $x_m(\omega) := 1 + \omega^m$. Then, $x_m\to 1$ is false, because $x_m(1) = 2$ for each $m$, and yet ${x_m\to 1} = [0, 1)$, and hence $x_m \to_{a.s.} 1$.</li>
</ul>
<p><strong>Example 2.4</strong>: </p>
<ul>
<li>Consider the probability space $([0,1], \mathcal{B}[0, 1], \ell)$.</li>
<li>Consider the sequence $(y_m)$ in $\mathbb{R}^{[0,1]}$ where $y_m$ is the indicator function of $[0, \frac{m+1}{2m}]$ on $[0,1]$. Then, $y_m\to_{a.s}. \mathbf{1}_{[0,\frac{1}{2})}$, but $y_m(\frac{1}{2}) = 1$ for each $m$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T13:51:55.000Z" title="12/15/2020, 9:51:55 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-E-Probability-via-Measure-Theory/">1.1.E Probability via Measure Theory</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/9%20Random%20Variables/Random-Variables/">Random Variables</a></h1><div class="content"><p>One is often interested in a particular characteristic of the (uncertain) outcomes of an experiment. To deal with such situations we may want to transform the given probability space (that models the underlying experiment) into another probability space the sample space of which contains the set of all values of the characteristic that we are interested in. </p>
<p>This is achieved by means of a measurable function, which is more commonly referred to as a random variable in probability theory. In the most general sense of the term, such a function maps one measurable space into another. </p>
<p>However, most measurable functions that are encountered in practice map a measurable space into a metric space, leading to the notion of Borel measurability. </p>
<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Measurable</strong>: </p>
<ul>
<li>Let $(X_1,\Sigma_1)$ and $(X_2,\Sigma_2)$ be two measurable spaces. </li>
<li>We say that a function $f : X_1\rightarrow X _2$ is **$\Sigma_1\backslash\Sigma_2$-measurable** if $f^{-1}(S) \in \Sigma_1$ for every $S \in \Sigma_2 $.</li>
</ul>
<p><strong>Random Variable, $\Sigma$-Measurable Real Function, Borel Measurable</strong>: </p>
<ul>
<li><p>Let $(X,\Sigma)$ be a measurable space and $Y$ a metric space. </p>
</li>
<li><p>A function $x : X\rightarrow Y$ is called a <strong>$Y$-valued random variable</strong> on $(X,\Sigma)$ if it is $\Sigma\backslash \mathcal{B}(Y)$-measurable, that is, if $x^{-1}(S)\in\Sigma$  for every $S\in \mathcal{B}(Y )$. </p>
</li>
<li><p>An $\mathbb{R} $-valued random variable is simply called a <strong>random variable</strong> on $(X,\Sigma)$.</p>
<ul>
<li>The set of all random variables on a measurable space $(X,\Sigma )$ is denoted by $\mathcal {L}^0(X,\Sigma)$. Moreover, we define<br>$$<br>L^0_+(X,\Sigma) := {x \in \mathcal{L}^0(X,\Sigma) : x\geq 0}.<br>$$</li>
</ul>
</li>
<li><p>In real analysis what we call here a random variable on $(X,\Sigma )$ is called a <strong>$\Sigma$-measurable real function</strong> on $X$. </p>
</li>
<li><p>Furthermore, a ($Y$-valued) random variable on a Borel probability space is said to be <strong>Borel measurable</strong>. </p>
</li>
</ul>
<blockquote>
<p>Let $(X,\Sigma)$ be a measurable space and $Y$ a metric space. In principle, to verify that a map $x:X\to Y$ is a $Y$-valued random variable, we need to show that $x^{-1}(B)\in\Sigma$ for every Borel subset $B$ of $Y$ .</p>
<p>Fortunately, there is a nice short-cut. If we can find a collection $\mathcal{A}$ of subsets of $Y$ that generates $\mathcal{B}(Y)$ and if we manage to verify that $x^{-1}(\mathcal{A})$  for every $A\in\mathcal{A}$ we may then conclude that $x$ is a $Y$-valued random variable.</p>
<p>For instance, if<br>$$<br>x^{-1}(O)\in\Sigma\mbox{ for every open subset }O \mbox{ of }Y,<br>$$<br>or<br>$$<br>x^{-1}(C)\in\Sigma\mbox{ for every closed subset }C \mbox{ of }Y,<br>$$<br>then $x$ is sure to be a $Y$-valued random variable.</p>
</blockquote>
<blockquote>
<p>The fact that the set of all intervals of the form $(-\infty,a]$, or of the form $(1,a)$, generates $\mathcal{B}(\mathbb{R})$ is used frequently, so it is probably a good idea to state it separately.</p>
<p>Given any measurable space $(X,\Sigma)$ and a map $x : X \rightarrow \mathbb{R} $, we have<br>$$<br>x\in\mathcal{L}^0(X,\Sigma)\mbox{ iff }{\omega\in X:x(\omega)\leq a}\in\Sigma\  \mbox{ for every } a\in\mathbb{R}<br>$$</p>
<p>and<br>$$<br>x\in\mathcal{L}^0(X,\Sigma)\mbox{ iff }{\omega\in X:x(\omega)&lt; a}\in\Sigma\  \mbox{ for every } a\in\mathbb{R}<br>$$</p>
<p>In measure theory and probability, we simply write ${x \leq a}$ for the set ${ \omega\in X : x(\omega) \leq a}$. While it may take a bit getting used to at first, this type of shorthand notation simplifies complex expressions considerably. For instance, with this notational convention, they we have<br>$$<br>x\in \mathcal{L}^0(X,\Sigma)\mbox{ iff }{x\leq a}\in\Sigma\ \mbox{ for every } a\in\mathbb{R}<br>$$<br>In what follows, we will work with these sorts of expressions routinely. All you need to do is to remember that, given a probability (or measure) space $(X,\Sigma, \mathbf{p})$, the set<br>$$<br>{\mbox{a statement about random variables defined on this space}}<br>$$<br>equals, by convention,<br>$$<br>{\omega\in X :\mbox{ this statement is true}}<br>$$<br>For any $x,y \in \mathcal{L}^0(X )$, the event that the absolute value of the difference between $x$ and $y$ is strictly larger than $3$ is written as ${|x-y|&gt; 3} $ instead of the mouthful ${\omega\in X : |x(\omega) -y(\omega)| &gt; 3}$. </p>
<p>In turn, the probability of this event is written simply as $\mathbf{p}{|x- y| &gt; 3}$:</p>
</blockquote>
<p><strong>Random $n$-vector, Random Real Sequence, Random Bounded Map, Random Element</strong>: </p>
<ul>
<li>An $\mathbb{R}^n$-valued random variable as a <strong>random $n$-vector</strong> if $n \geq 2$. </li>
<li>Analogously, an $\mathbb{R}^{\infty}$-valued random variable may be called a <strong>random real sequence</strong></li>
<li>A $\mathbf{B}[0,1]$-valued random variable is a <strong>random bounded map</strong> on $[0,1]$ and so on. </li>
<li>More generally, a $Y$-valued random variable (for any metric space $Y$) is sometimes referred to as a <strong>random element</strong> of $Y$.</li>
</ul>
<p><strong>Simple</strong>: </p>
<ul>
<li>Let $Y$ be a metric space. A $Y$-valued random variable $x$ is called <strong>simple</strong> if it takes finitely many values, that is, when the range of $x$ is a finite set.</li>
</ul>
<p><strong>Postive Parts, Negative Parts</strong>: </p>
<ul>
<li>Let $X$ be a nonempty set and $x$ an $\overline{\mathbb{R} }$-valued function on $X$: We define the positive and negative parts of $x$ as</li>
</ul>
<p>$$<br>x^+:= \max {x, 0}\mbox{ and }x^-:= \max {-x,0}<br>$$</p>
<ul>
<li><p>respectively. </p>
</li>
<li><p>Clearly, both $x^+$ and $x^-$ are $[0, \infty]$-valued functions on $X$, and we have<br>$$<br>x=x^+ -x^-<br>$$</p>
</li>
<li><p>The absolute value of $x$ is also decomposed into these two functions as follows:<br>$$<br>|x|=x^+ +x^-<br>$$</p>
</li>
</ul>
<blockquote>
<p>It is plain that the positive and negative parts of an $\overline{\mathbb{R}}$-valued function inherits the measurability of that function. </p>
<p>In other words, if $x$ is an $\overline{\mathbb{R}}$-valued random variable on a probability space $(X,\Sigma,\mathbf{p})$, then both $x^+$ and $x^-$ are $[0,\infty]$-valued random variables on $(X,\Sigma, \mathbf{p})$. </p>
<p>Conversely, if $x^+$ and $x^-$ are $[0,\infty]$-valued random variables on $(X,\Sigma,\mathbf{p})$, then $x$ must itself be an $\overline{\mathbb{R}}$-valued random variable on $(X,\Sigma,\mathbf{p})$.</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1.1</strong>: </p>
<ul>
<li>Let $X$ and $Y$ be two metric spaces, and $x: X \rightarrow Y$ a map that is continuous at all but countably many points. Then, $x$ is a $Y $-valued random variable on $(X,\mathcal{B}(X))$.</li>
</ul>
<blockquote>
<p>Continuity at everywhere but countably many points is suffcient for Borel measurability. The converse is false. Even a map that is discontinuous everywhere may be Borel measurable.</p>
</blockquote>
<p><strong>Proposition 1.2</strong>: </p>
<ul>
<li>Let $(X_i,\Sigma_ i)$ be a measurable space for each $i \in [3]$. </li>
<li>If $f : X_1 \rightarrow X_2$ is a $\Sigma_1\backslash\Sigma_2$-measurable map and $g : X_2 \rightarrow X_3$ is a $\Sigma_2\backslash\Sigma_3$-measurable map, then $g \circ f$ is a $\Sigma_1\backslash\Sigma_3$-measurable map from $X_1$ into $X_3$.</li>
</ul>
<p><strong>Proposition 1.5</strong>: </p>
<ul>
<li>Let $Y$ be a metric space and $(x_m)$ a sequence of $Y $-valued random variables on a measurable space $(X,\Sigma )$ such that $x_m\rightarrow x $ for some function $x: X \rightarrow Y$ . Then, $x$ is a $Y $-valued random variable on $(X,\Sigma )$.</li>
</ul>
<blockquote>
<p>If $(x_m)$ is a sequence of metric space-valued random variables on some measurable space, $\lim x_m$ is well-defined as a random variable on that space</p>
<p>This is unlike what is the case with continuous functions.</p>
</blockquote>
<p><strong>Corollary 1.6</strong>: </p>
<ul>
<li>Let $(x_m)$ be an increasing sequence of $\overline{\mathbb{R} }$-valued random variables on a measurable space $(X,\Sigma )$. Then, $\lim x_m$ is an $\overline{\mathbb{R}}$-valued random variable on $(X,\Sigma )$.</li>
</ul>
<p><strong>Theorem 1.7</strong>: </p>
<ul>
<li>Let $Y$ be a compact metric space and $x$ a $Y $-valued random variable on a measurable space $(X,\Sigma)$. </li>
<li>Then, there is a sequence $(x_m)$ of simple $Y$-valued random variables on $(X,\Sigma )$ such that $x_m\rightarrow x$ uniformly.</li>
</ul>
<blockquote>
<p>For a random variable with compact range, we can approximate a given random variable on a measurable space by means of simple random variables on that space. </p>
</blockquote>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example 1.1</strong>: </p>
<ul>
<li>If $X$ is a nonempty set and $Y$ a metric space, then any map from $X$ into $Y$ is a $Y$-valued random variable on $(X,2^X )$. </li>
<li>In particular, $\mathbb{R}^X = \mathcal{L}^0(X, 2^X )$.</li>
<li>If $X$ is finite, then any such function is a simple random variable on $(X,2^X )$. </li>
</ul>
<p><strong>Example 1.3</strong>:</p>
<ul>
<li>Let $X$ and $Y$ be two metric spaces. If $x : X\to Y$ is continuous, then $x$ is a $Y $-valued random variable on $(X,\mathcal{B}(X))$.</li>
<li>Indeed, continuity of $x$ implies that the inverse image of every open set in $Y$ under $x$ is open in $X$. </li>
<li>So this is enough to conclude that $x$ is a $Y$-valued random variable. </li>
<li>In particular, $\mathbf{C}(X) \subseteq \mathcal{L}^0(X,\mathcal{B}(X))$</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>$x\in \mathcal{L}^0(X,\Sigma)$ implies $ax\in \mathcal{L}^0(X,\Sigma )$ for any real number $a$. </li>
<li>$x + y \in \mathcal{L}^0(X,\Sigma )$ for any $x,y\in \mathcal{L}^0(X,\Sigma)$. </li>
<li>Thus, $\mathcal{L}^0(X,\Sigma)$ is a linear space under the usual (pointwise) addition and scalar multiplication operations. <ul>
<li>The set of all simple random variables on $(X,\Sigma) $ constitutes a linear subspace of this linear space, which is itself a linear $X$ subspace of $\mathbb{R}^X $ .  </li>
<li>If $X$ is a metric space, then $\mathbf{C}(X)$ is also a linear subspace of $\mathcal{L}^0(X,\mathcal{B}(X))$.</li>
</ul>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T13:11:55.000Z" title="12/15/2020, 9:11:55 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-E-Probability-via-Measure-Theory/">1.1.E Probability via Measure Theory</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/8%20Probability%20via%20Measure%20Theory/Constructing-Probability-Spaces/">Constructing Probability Spaces</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Distribution Function</strong>: </p>
<ul>
<li>A map $F\in \mathbb{R} \rightarrow [0, 1]$ is said to be a <strong>distribution function</strong> if it is increasing, right-continuous, and $F(-\infty) = 0 = 1-F(\infty)$.</li>
</ul>
<blockquote>
<p>Every Borel probability measure on $\mathbb{R}$ induces a distribution function in a natural manner. Indeed, it is easy to see that the map $t \mapsto\mathbf{p}(-\infty, t]$ on $\mathbb{R}$ is a distribution function for any $\mathbf{p}\in \Delta(\mathbb{R})$</p>
</blockquote>
<p><strong>Distribution function induced by</strong> $\mathbf{p}$:</p>
<ul>
<li><p>For any $\mathbf{p}\in \Delta(\mathbb{R})$, the map $F_\mathbf{p} :\mathbb{R}\to [0, 1]$ defined by $F (t) := \mathbf{p}(-\infty, t]$, is a distribution function. </p>
</li>
<li><p>We refer to $F_\mathbf{p}$ <strong>the distribution function induced by $\mathbf{p}$.</strong></p>
</li>
</ul>
<p><strong>Lebesgue-Stieltjes Probability Measure induced by $F$ on $\mathbb{R} $</strong>: </p>
<ul>
<li>Let $\mathcal{S}$ be the semialgebra of all right-semiclosed intervals. </li>
<li>Taking any distribution function $F$, define the map $\mathbf{q}\in[0, 1]^{\mathcal {S}}$ as</li>
</ul>
<p>$$<br>\mathbf{q}(a,b]:=F(b)-F(a).\ \ \ -\infty\leq a\leq b&lt;\infty<br>$$</p>
<ul>
<li><p>and<br>$$<br>\mathbf{q}(a,\infty):=1-F(a),\ \ \ \ -\infty\leq a.<br>$$</p>
</li>
<li><p>The probability measure on $\sigma(\mathcal{S})=\mathcal{B}(\mathbb{R} )$ which accords with $\mathbf{q}$, and denoted by $\mathbf{p}_F$ is called the <strong>Lebesgue-Stieltjes probability measure induced by $F$ on $\mathbb{R} $.</strong></p>
<ul>
<li><p>Carathéodory’s Extension Theorem would then say that there is a unique extension of $\mathbf{q}$ to a probability measure on $\mathcal{B}(\mathbb{R} )$.</p>
</li>
<li><p>To prove $\mathbf{q}$ is $\sigma$-additive on $\mathcal{S}$, let us take any countably infinite set $\mathcal{T}$ of pairwise disjoint right-semiclosed intervals, and assume that $\bigsqcup{T}$ is itself a right-semiclosed interval, say, $(a,b]$. </p>
</li>
<li><p>As things are trivial when $a = b$, we assume in what follows that $a &lt; b$ </p>
</li>
<li><p>We will assume here that $\bigsqcup \mathcal{T}$ is bounded from above, that is, it is an interval of the form $(a,b]$,  where $-\infty\leq a &lt; b &lt;\infty$</p>
</li>
<li><p>Let us then enumerate $T$ as ${f(a_1, b_1], (a_2, b_2],\cdots}$. </p>
</li>
<li><p>Then we can show that<br>$$<br>\mathbf{q}(a,b]=\sum^\infty_{i=1}\mathbf{q}(a_i,b_i].<br>$$</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>A Borel probability measure on $\mathbb{R} $ can actually be identified with a distribution function.</p>
</blockquote>
<p><strong>Lebesgue-Stieltjes probability measure induced by $F$ on $(a, b]$ / Lebesgue-Stieltjes measure induced by $F$ on $(a, b]$</strong>: </p>
<ul>
<li>If $X := (a, b]$ with $-\infty&lt; a &lt; b &lt; \infty$, and $F \in [0,1]^X$ is an increasing and right-continuous function with $F(a+) = 0$ and $F(b) = 1$, then we can define <strong>the Lebesgue-Stieltjes probability measure induced by $F$ on $(a, b] $</strong>, analogously.</li>
<li>Drop the conditions $F(a+) = 0$ and $F(b) = 1$, and allow $F \in \mathbb{R}^X$ then we can define <strong>the Lebesgue-Stieltjes measure induced by $F$ on $(a, b] $</strong>, analogously.</li>
</ul>
<p><strong>Lebesque Measure on $\mathbb{R} $</strong>: </p>
<ul>
<li><p>Take any integer $i$, let $X_i := (i,i+1]$, and define $F_i : X_i \rightarrow [0,1]$ by $F_i(t) := t-i$.</p>
</li>
<li><p>Let $\ell_i$ denote the Lebesgue-Stieltjes measure induced by $F_i$ on $X_i$ for each $i$</p>
</li>
<li><p>We define the <strong>Lebesgue measure on $\mathbb{R} $</strong> as the $[0,\infty]$-valued map $\ell$ on $\mathcal{B}(\mathbb{R} )$ with<br>$$<br>\ell(S):=\sum_{i\in\mathbb{Z}}\ell_i(S\cap X_i)<br>$$</p>
</li>
</ul>
<blockquote>
<p>As such, $\ell$ is both regular and tight.</p>
<p>Furthermore, it assigns to any interval its length as its measure. </p>
<p>In particular, it is readily verified that $\ell(a, b] = b$  a for any real numbers $a$ and $b$ with $a &lt; b$, while $\ell(I) = \infty$ for any unbounded interval $I$</p>
</blockquote>
<blockquote>
<p>Lebesque Measure on $\mathbb{R} $ is, geometrically speaking, the natural measure on the real line.</p>
<p>This is indeed a $\sigma$-finite measure on $\mathcal{B}(\mathbb{R} )$.</p>
<p>The restriction of $\ell$ to any Borel subset $X$ of $\mathbb{R}$ is a Borel measure on $X$; that is, $(X,\mathcal{B}(X),\ell|_{\mathcal{B}(X)})$ is a Borel measure space for any $X\in\mathcal{B}(\mathbb{R} )$. </p>
<p>For brevity, we denote this measure space simply as $(X,\mathcal{B}(X),\ell)$ in what follows. For instance, $([0,1],\mathcal{B}([0,1]),\ell)$ is a Borel probability space.</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Carathéodory’s Extension Theorem</strong>: </p>
<ul>
<li><p>Let $\mathcal{S}$ be a semialgebra on a nonempty set $X$ and take any $\mathbf{q}: \mathcal{S}\rightarrow [0,\infty]$. </p>
</li>
<li><p>If $\mathbf{q}$ is $\sigma$-additive, then there exists a measure $\mathbf{p}$ on $\sigma(\mathcal{S})$ such that $\mathbf{p}(S) = \mathbf{q}(S)$ for each $S \in \mathcal{S}$. </p>
</li>
<li><p>Moreover, if $\mathbf{q}(X) &lt; \infty$, then $\mathbf{p}$ is unique.</p>
</li>
</ul>
<blockquote>
<p>We can always extend a $\sigma$-additive $[0,\infty]$-valued map on an algebra $\mathcal{A}$ to a $\sigma$-additive $[0,\infty]$-valued map on $\sigma(\mathcal{A})$ </p>
<p>Moreover, this extension is unique, provided that our map is real-valued.</p>
</blockquote>
<blockquote>
<p>We can always obtain a probability measure, uniquely, on a $\sigma$-algebra by specifying the behavior of the measure only on a semialgebra that generates this $\sigma$-algebra. Since semialgebras and algebras are often easier than $\sigma$-algebras to work with, this fact is quite useful in constructing probability measures.</p>
</blockquote>
<p><strong>Proposition 3.1</strong>: </p>
<ul>
<li>For any real number a and Borel subset $S$ of $\mathbb{R} $, the set $S + a$ is Borel, and $\ell (S + a) =\ell (S)$.</li>
</ul>
<p><strong>Proposition 3.4</strong>: </p>
<ul>
<li><p>Let $f$ be an additive self-map on $\mathbb{R} $.</p>
</li>
<li><p>If $f$ is bounded on some Borel subset $S$ of $\mathbb{R} $ with $\ell(S) &gt; 0$, then it is linear.</p>
</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example</strong>:</p>
<ul>
<li>Any singleton set in $\mathbb{R}$ has Lebesgue measure zero.</li>
<li>Since the Lebesgue measure of any singleton is zero, so must be the Lebesgue measure of any countable subset of $\mathbb{R}$, because the $\sigma$-additivity of $\ell$ implies that $\ell{a_1, a_2\dots, } = \sum^\infty \ell{a_i}=0$ for any real sequence $(a_m)$. </li>
<li>For instance: $\ell(\mathbb{Q}) = 0$</li>
</ul>
<p><strong>Example 3.5</strong>:</p>
<ul>
<li><p>There are in fact uncountable sets in $\mathbb{R}$ which have Lebesgue measure zero.</p>
</li>
<li><p>For example, the Contor Set.</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T12:51:55.000Z" title="12/15/2020, 8:51:55 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-E-Probability-via-Measure-Theory/">1.1.E Probability via Measure Theory</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/8%20Probability%20via%20Measure%20Theory/Elements-of-Probability-Theory/">Elements of Probability Theory</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Finitely Additive, $\sigma$-additive</strong>: </p>
<ul>
<li>Let $X$ be a nonempty set and $\mathcal{A}$ a collection of subsets of $X$ which constains $\empty$. </li>
<li>A map $\mathbf{p}:\mathcal{A}\mapsto [0,+\infty]$ is said to be <strong>finitely additive</strong> (**$\sigma$-additive**, responsive.) if $\mathbf{p}(\empty)=0$ and</li>
</ul>
<p>$$<br>\mathbf{p}(\bigsqcup\mathcal{S})=\sum_{S\in\mathcal{S}}\mathbf{p}(S)<br>$$</p>
<ul>
<li>for any nonempty finite (countable, resp.) set $\mathcal{S}$ of pariwise disjoint elements of $\mathcal{A}$ such that $\bigsqcup \mathcal{S}\in \mathcal{A}$.</li>
</ul>
<blockquote>
<p>$\sigma$-additivity implies the likelihood of the union of countably many pairwise disjoint events is simply the sum of the individual probabilities of each of these events.</p>
</blockquote>
<blockquote>
<p>The additivity property is the heart of the measure theory, entails several other useful properties for measures.</p>
</blockquote>
<p><strong>Measure, Measure Space</strong>: </p>
<ul>
<li>Let $(X,\Sigma)$ be a measurable space. </li>
<li>Any $\sigma$-additive function $\mu: \Sigma\mapsto[0,\infty]$ is called a <strong>measure</strong> on $\Sigma$, and in that case the ordered triplet $(X,\Sigma , \mu)$ is said to be a <strong>measure space</strong>. </li>
<li>If $\mu(X) &lt;\infty $; then $\mu$ is called a <strong>finite measure</strong>, and $(X,\Sigma ,\mu )$ is referred to as a <strong>finite measure space</strong>. </li>
<li>If there is a countable subset $\mathcal{T}$ of $\Sigma$ such that $X =\bigsqcup \mathcal{T}$ and $\mu(T) &lt; \infty$ for each $T\in \mathcal{T }$, we say that $\mu$ is a **$\sigma$-finite measure**, and refer to $(X,\Sigma,\mu)$ as a **$\sigma$-finite measure space**.</li>
</ul>
<blockquote>
<p>A useful interpretation is to think of a measure on $\Sigma$ as a function that tells us the “size” of each set in $\Sigma$ in some geometric sense.</p>
<p>Depending on the context, for instance, this may correspond to, say, area or volume or mass, etc., of the members of $\Sigma$.</p>
</blockquote>
<p><strong>Probability Measure, Probability Space</strong>: </p>
<ul>
<li>Let $(X,\Sigma,\mu)$ be a measure space. </li>
<li>If $\mu(X) = 1$, then $\mu$ is said to be a <strong>probability measure</strong>, and in this case, $(X,\Sigma,\mu)$ is called a <strong>probability space</strong>.</li>
</ul>
<blockquote>
<p>Roughly speaking, a probability measure tells us the likelihood of observing any conceivable event in an experiment the outcome of which is uncertain.</p>
</blockquote>
<p><strong>Borel Measure, Borel Space, Borel Probability Space</strong>: </p>
<ul>
<li>Given a metric space $X$, any measure $\mathbf{p}$ on $\mathcal{B}(X)$ is called a <strong>Borel measure</strong> on $X$, and in this case $(X,\mathcal{B}(X),\mathbf{p})$ is referred to as a <strong>Borel space</strong>. </li>
<li>If, in addition, $\mathbf{p}$ is a probability measure, then $(X,\mathcal{B}(X),\mathbf{p})$ is called a <strong>Borel probability space</strong>.<ul>
<li>We denote the set of all Borel probability measures on a metric space $X$ by $\Delta(X)$.</li>
</ul>
</li>
</ul>
<p><strong>Regular</strong>: </p>
<ul>
<li><p>Let $X$ be a metric space and $\mathbf{p}\in\Delta(X)$.</p>
</li>
<li><p>Let $\mathcal{O}_X$ and $\mathcal{C}_X$ denote the class of all open and closed subsets of $X$, respectively. </p>
</li>
<li><p>A Borel measure $\mathbf{p}$ on a metric space $X$ is said to be <strong>regular</strong> if<br>$$<br>\inf{\mathbf{p}(O): S\subseteq O\in \mathcal{O}_X}=\mathbf{p}(S) = \sup \mathbf{p}(C) : {S\supseteq  C\in \mathcal{C}_X}<br>$$</p>
</li>
<li><p>holds for any $S\in \mathcal{B}(X)$.</p>
</li>
</ul>
<blockquote>
<p>Much of the diffculty with working with probability measures stems from the fact that the contents of the $\sigma$-algebra of a probability space are often obscure. The situation reads markedly better in the case of Borel probability spaces, for the $\sigma$-algebra of such a space is generated by sets that are somewhat concrete (such as open (or closed) subsets of the sample space). </p>
<p>Moreover, in the context of any Borel probability space, it is possible to approximate the probability of any event by using only the open and closed sets. </p>
<p>Put more formally, if $X$ is a metric space and $\mathbf{p}\in\Delta(X)$, then, for any Borel subset $S$ of $X$ and $\varepsilon&gt; 0$, we can find an open subset $O$ and a closed subset $C$ of $X$ such that $C\subseteq S\subseteq O$ and $\mathbf{p}(O\setminus C) &lt;\varepsilon$.</p>
</blockquote>
<blockquote>
<p>Every Borel probability measure is regular.</p>
<p>Every finite Borel measure on a metric space is regular.</p>
</blockquote>
<p><strong>Tight</strong>: </p>
<ul>
<li>A finite Borel measure $\mathbf{p}$ on a metric space $X$ is said to be <strong>tight</strong> if for each $\varepsilon &gt; 0$, there is a compact subset $K$ of $X$ such that $p(X\backslash K) &lt;\varepsilon$.</li>
</ul>
<blockquote>
<p>Every Borel probability measure on a complete and separable metric space is tight.</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Lemma 2.1</strong>: </p>
<ul>
<li>Let $\mathcal{S}$ be a semialgebra on a nonempty set $X$, and $\mathbf{q} : \mathcal{S}\mapsto [0,+\infty]$ a finitely additive map. Then, there is a unique finitely additive map $\mathbf{p} : \alpha(\mathcal{S})\mapsto[0,+\infty]$ such that $\mathbf{p}(S) = \mathbf{q}(S)$ for each $S \in \mathcal{S}$.</li>
</ul>
<p><strong>Lemma 2.2</strong>: </p>
<ul>
<li>Let $\mathcal{S}$ be a semialgebra on a nonempty set $X$, and $\mathbf{q}: \mathcal{S} \mapsto [0,+\infty]$ a $\sigma$-additive map. Then, there is a unique $\sigma$-additive map $\mathcal{p}:\sigma(\mathcal{S})\mapsto [0,+\infty]$ such that $\mathbf{p}(S) = \mathbf{q}(S)$ for each $S\in \mathcal{S}$.</li>
</ul>
<blockquote>
<p>Every finitely additive (finitely additive and $\sigma$-subadditive) $[0,\infty]$-valued map on a semialgebra $\mathcal{S}$ can be extended, uniquely, to a finitely additive ($\sigma$-additive) $[0,\infty]$-valued map on the algebra generated by $\mathcal{S}$.</p>
</blockquote>
<p><strong>Lemma 2.3</strong>: </p>
<ul>
<li>Let $X$ be a nonempty set, $\mathcal{A}$ an algebra on $X$, and $\mu:\mathcal{A}\mapsto[0,\infty]$ a $\sigma$-additive map on $\mathcal{A}$. Then, $\mu$ is finitely additive. Consequently,</li>
</ul>
<p>$$<br>\mu(B\backslash A)=\mu(B)-\mu(A)\mbox{ for any} A,B\in \mathcal{A}\ \mbox{with}\ A\subseteq B,<br>$$</p>
<ul>
<li><p>which shows that $\mu$ is $\supseteq$-increasing (that is, $\mu(A)\leq\mu(B)$ for any $A,B\in \mathcal{A}$ with</p>
<p>$A\subseteq B$). Moreover,<br>$$<br>\mu(\bigcup\mathcal{S})\leq\sum_{S\in\mathcal{S}}\mu(S)<br>$$</p>
</li>
<li><p>for any countable $\mathcal{S}\subseteq \mathcal{A}$ with $\bigcup\mathcal{S}\in\mathcal{A}$.</p>
</li>
</ul>
<p><strong>Boole’s Inequality</strong>: </p>
<ul>
<li>Let $(X,\Sigma,\mathbf{p})$ be a probability space. Then,</li>
</ul>
<p>$$<br>\mathbf{p}(\bigcup\mathcal{S})\leq\sum_{S\in\mathcal{S}}\mathbf{p}(S)\ \mbox{for any nonempty countable}\ \mathcal{S}\subseteq\Sigma<br>$$</p>
<p><strong>Proposition 2.4</strong>: </p>
<ul>
<li>Let $(X,\Sigma, \mu)$ be a finite measure space, and let $(A_m)\in\Sigma^\infty$. If $A_1\subseteq A_2\subseteq\cdots$  (in which case we say that $(A_m)$ is an increasing sequence), then</li>
</ul>
<p>$$<br>\lim\mu(A_m)=\mu\left(\bigcup^\infty_{i=1}A_i\right).<br>$$</p>
<ul>
<li>On the other hand, if $A_1\supseteq A_2\supseteq \cdots$  (in which case we say that $(A_m)$ is a decreasing sequence), then<br>$$<br>\lim\mu(A_m)=\mu \left(\bigcap^\infty_{i=1}A_i\right) .<br>$$</li>
</ul>
<blockquote>
<p>These two properties of a finite measure measure are often referred to as the properties of continuity (from below and above, resp.) of a finite measure.</p>
</blockquote>
<p><strong>Proposition 2.5</strong>: </p>
<ul>
<li>Let $X$ be a nonempty set, $\mathcal{A}$ be an algebra on $X$, and  $\mu:A\rightarrow [0,\infty]$ a finitely additive function such that $\mu(C_m)\downarrow 0$ for any $(C_m)\in A^\infty$ with $C_m\downarrow\empty$. Then, $\mu$ is $\sigma$-additive.</li>
</ul>
<blockquote>
<p>Finite additivity and continuity of a set function jointly imply its $\sigma$-additivity. A finitely additive $[0,\infty]$-valued function on an algebra that is continuous from above at $\emptyset$ is $\sigma$-additive.</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-15T12:41:55.000Z" title="12/15/2020, 8:41:55 PM">2020-12-15</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-E-Probability-via-Measure-Theory/">1.1.E Probability via Measure Theory</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/15/Mathematics/Analysis/8%20Probability%20via%20Measure%20Theory/Measurable-Spaces/">Measurable Spaces</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Algebra</strong></p>
<ul>
<li>Let $X$ be a nonempty set. </li>
<li>A nonempty collection $\mathcal{A} $ of subsets of $X$ is called an <strong>algebra</strong> on $X$ if<ol>
<li>$X\backslash A\in \mathcal{A}$ for all $A \in \mathcal{A}$; and</li>
<li>$A\cup B\in \mathcal{A}$ for all $A, B\in \mathcal{A}$.</li>
</ol>
</li>
<li>We say that $\mathcal{A}$ is a <strong>finite algebra</strong> on $X$ if it is an algebra on $X$ such that $| \mathcal{A}| &lt; \infty$.</li>
</ul>
<blockquote>
<p>In words, an algebra on a nonempty set $X$ is a nonempty collection of subsets of $X$ that is closed under complementation and taking pairwise (and thus finite) unions, and a finite algebra on $X$ is one that contains finitely many elements. (Obviously, any algebra on a finite set is a finite algebra on that set.)</p>
</blockquote>
<blockquote>
<p>Both $\emptyset$ and $X$ belong to any algebra $\mathcal{A}$ on $X$</p>
<p>By the de Morgan Law, an algebra $\mathcal{A}$ on $X$ is closed under taking finite intersections. That is, if $\mathcal{S}$ is a finite subset of $\mathcal{A}$, then $\bigcap\mathcal{S}\in\mathcal{A}$</p>
</blockquote>
<p><strong>$\sigma$-algebra, $\Sigma$-measureable set, Measureable Space</strong>: </p>
<ul>
<li>Let $X$ be a nonempty set. A nonempty collection of subsets of $X$ is called <strong>$\sigma$-algebra</strong> on $X$ if<ol>
<li> $X\backslash A\in\Sigma$ for all $A \in \Sigma$; and</li>
<li> $\bigcup^\infty A_i\in\Sigma$ whenever $A_i\in\Sigma$ for each $i=1,2…$</li>
</ol>
</li>
<li>Any element of $\Sigma$ is called a **$\Sigma$-measurable set** in $X$. </li>
<li>If $\Sigma$ is a $\sigma$-algebra on $X$, we refer to the pair $(X,\Sigma)$ as a <strong>measurable space</strong>.</li>
</ul>
<blockquote>
<p>A $\sigma$-algebra on $X$ is a nonempty collection of subsets of $X$ that is closed under complementation and taking countably infinite unions. </p>
<p>In particular, there is no difference between an algebra and a $\sigma$-algebra when the ground set $X$ under consideration is finite. </p>
<p>More generally, every finite algebra on $X$ is a $\sigma$-algebra on $X$</p>
</blockquote>
<blockquote>
<p>By the de Morgan Law, an $\sigma$-algebra on $X$ is closed under taking finite intersections. That is, if $\mathcal{C}$ is a countable subset of a $\sigma$-algebra $\Sigma$ on $X$, then $\bigcap\mathcal{C}\in\Sigma$</p>
</blockquote>
<p><strong>Sample Space</strong>:</p>
<ul>
<li>Given a nonempty set $X$ and a $\sigma$-algebra $\Sigma$ on $X$, we think of $X$ as the set of all possible outcomes that may result in an experiment, the so-called <strong>sample space</strong>, and view any one member of $\Sigma$ (and only such a subset of $X$) as an “event” that may take place in the experiment.</li>
</ul>
<p><strong>Event, Event Space</strong>:</p>
<ul>
<li>Given a $\sigma$-algebra $\Sigma$ on $X$, the intuitive concept of an “<strong>event</strong>“ is formalized as any $\Sigma$-measurable set in $X$. </li>
<li>That is, we say that $A$ is an event if and only if $A\in\Sigma$ , and for this reason a $\sigma$-algebra on $X$ is often referred to as an <strong>event space</strong> on $X$. </li>
</ul>
<blockquote>
<p>One may define many different event spaces on a given sample space, so what an “event” really is depends on the model one chooses to work with.</p>
</blockquote>
<p><strong>Semialgebra</strong>: </p>
<ul>
<li><p>Let $X$ be a nonempty set and $\mathcal{S}$ a collection of subsets of $X$. </p>
</li>
<li><p>We say that $\mathcal{S}$ is a <strong>semialgebra</strong> on $X$ if </p>
<ol>
<li>both $\empty$ and $X$ belong to $\mathcal{S}$; </li>
<li>$\mathcal{S}$ is closed under taking finite intersections, and </li>
<li>for any $S \in \mathcal{S}$, the set $X\backslash S$ can be written as the union of a collection of finitely many pairwise disjoint elements of $\mathcal{S}$.</li>
</ol>
</li>
</ul>
<p><strong>Generated Algebra</strong>: </p>
<ul>
<li><p>Let $X$ be a nonempty set and $\mathcal{S}$ a subset of $2^X$. </p>
</li>
<li><p>The smallest algebra on $X$ that contains $\mathcal{S}$ (in the sense that this algebra is included in any other algebra that contains $\mathcal{S}$) is called the <strong>algebra generated by $\mathcal{S}$</strong>; and is denoted as $\alpha(\mathcal{S})$.</p>
<ul>
<li>we have $\mathcal{A} =\alpha(\mathcal{A})$ for any algebra $\mathcal{A}$ on any nonempty set.</li>
</ul>
</li>
</ul>
<p><strong>Generated $\sigma$-algebra</strong>: </p>
<ul>
<li><p>Let $X$ be a nonempty set and $\mathcal{S}$ a nonempty subset of $2^X $. </p>
</li>
<li><p>The smallest $\sigma$-algebra on $X$ that contains $\mathcal{S}$ (in the sense that this $\sigma$-algebra is included in any other $\sigma$-algebra that contains $\mathcal{S}$) is called the $\sigma$-algebra generated by $\mathcal{S}$; and is denoted as $\sigma (\mathcal{S})$.<br>$$<br>\sigma(\mathcal{S})=\bigcap{\Sigma:\Sigma\mbox{ is a }\sigma\mbox{-algebra on }X\mbox{ such that }\mathcal{S}\subseteq\Sigma}<br>$$</p>
</li>
</ul>
<p><strong>Borel $\sigma$-algerbra</strong>: </p>
<ul>
<li>Let $X$ be a metric space, and, let $\mathcal{O}_X$ stand for the collection of all open, and $\mathcal{C}_X$ for that of all closed, subsets of $X$. <ul>
<li>The members of $\mathcal{O}_X$, or of $\mathcal{C}_X$, are of obvious importance, but unfortunately neither of these collections is an algebra in general. </li>
</ul>
</li>
<li>In metric spaces, then, it is natural to consider the $\sigma$-algebra generated by $\mathcal{O}_X$, called the <strong>Borel $\sigma$-algebra</strong> on $X$; and its members are referred to as <strong>Borel sets</strong> (or in probabilistic jargon, <strong>Borel events</strong>). </li>
<li>We denote the Borel $\sigma$-algebra on a metric space $X$ by $\mathcal{B}_X$. </li>
<li>By denifition, therefore, we have $\mathcal{B}(X):=\sigma(\mathcal{O}_X)$.<ul>
<li>We usually write $\mathcal{B}[a, b]$ for $\mathcal{B}([a, b])$, and $\mathcal{B}(a, b]$ for $\mathcal{B}((a, b])$, where $-\infty &lt; a &lt; b &lt; \infty$.</li>
</ul>
</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example 1.3</strong></p>
<ol>
<li>The collection of all intervals is a semialgebra on $\mathbb{R}$, but it is not an algebra.</li>
<li>The collection of all open intervals is not a semialgebra on $\mathbb{R}$.</li>
<li>The collection of all right-semiclosed intervals is a semialgebra on $\mathbb{R}$; but it is not an algebra. </li>
<li>The smallest algebra on $\mathbb{R}$ that contains all right-semiclosed intervals is the collection of all finite disjoint unions of right-semiclosed intervals. This algebra is not a $\sigma$-algebra. </li>
</ol>
<p><strong>Example</strong>:</p>
<ul>
<li>if $X := {a,b,c}$,then <ul>
<li>$\alpha(\emptyset) = \alpha({\emptyset}) = \alpha({X}) = {\emptyset,X}$</li>
<li>$\alpha({\emptyset, X, {a}}) = {\emptyset,X,{a},{b,c}}$</li>
<li>$\alpha({\emptyset,X,{a},{b}}) = 2^X$</li>
</ul>
</li>
</ul>
<p><strong>Example 1.5</strong>:</p>
<ul>
<li>By definition, $\mathcal{B}(\mathbb{R}) = (\mathcal{O}_\mathbb{R})$; but one does not actually need all open sets in $\mathbb{R}$ for generating $\mathcal{B}(\mathbb{R})$<ul>
<li>For instance, what if we used instead the class of all open and bounded intervals, call it $\mathcal{A}_1$, then $\sigma(\mathcal{A}_1)=\mathcal{B}(\mathbb{R})$.</li>
<li>$\mathcal{A}_2:=$ the set of all closed and bounded intervals</li>
<li>$\mathcal{A}_3:=$ the set of all closed sets in $\mathbb{R}$</li>
<li>$\mathcal{A}_4:=$ the set of all bounded intervals of the form $(a, b]$</li>
<li>$\mathcal{A}_5:=$ the set of all intervals of the form $(-\infty,a]$</li>
<li>$\mathcal{A}_5:=$ the set of all intervals of the form $(-\infty,a)$</li>
<li>All of these collections generate the same $\sigma$-algebra, namely, $\mathcal{B}(\mathbb{R})$<ul>
<li>For any metric space $X$, $\mathcal{B}(X):=\sigma(\mathcal{O}_X)=\sigma(\mathcal{C}_X)$.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, “Measure and Probability Theory with Economic Applications”;</li>
<li>Lecture notes, ECON 30400 2, Autumn 2020, Introduction: Mathematical Methods In Economics;</li>
<li>Walter Rudin, 1987, “Real and Complex Analysis”; </li>
<li>Walter Rudin, 1976, “Principle of Mathematical Analysis”; </li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-06T12:42:55.000Z" title="12/6/2020, 8:42:55 PM">2020-12-06</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-D-Metric-Linear-Spaces-and-Normed-Linear-Spaces/">1.1.D Metric Linear Spaces and Normed Linear Spaces</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/06/Mathematics/Analysis/7%20Normed%20Linear%20Spaces/Banach-Spaces/">Banach Spaces</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Banach Space</strong>: </p>
<ul>
<li>A normed linear space $X$ is called a <strong>Banach space</strong> if it is complete (that is, if $(X, d_{\|·\|})$ is a complete metric space).</li>
</ul>
<p><strong>Infinite Series, Convergent, Absolutely Convergent</strong>: </p>
<ul>
<li><p>By an <strong>infinite series</strong> in a normed linear space $X$, we mean a sequence in $X$ of the form $(\sum^m x^i)$ for some $(x^m) ∈ X^∞$.</p>
</li>
<li><p>We say that this series is <strong>convergent</strong> (in $X$) if $\lim \sum^m x^i ∈ X$ (that is, when there exists an $x ∈ X$ with $\|\sum^m x^i − x\| → 0 $) . </p>
<ul>
<li>In this case the vector $\lim \sum^m x_i$ is denoted as $\sum^\infty x^i $. </li>
</ul>
</li>
<li><p>We say that the infinite series $(\sum^mx^i )$ in $X$ is <strong>absolutely convergent</strong> if $\sum^∞ \|x^i\| &lt; ∞$.</p>
</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 3</strong>: (Banach) </p>
<ul>
<li><p>Let $X$ be a normed linear space. </p>
</li>
<li><p>Then, $X$ is Banach iff, every absolutely convergent series in $X$ is convergent.</p>
</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example</strong>:</p>
<ul>
<li>Every closed subspace of a Banach space is itself a Banach Space</li>
<li>$\mathbf{CB}^1(I)$ is a Banach space  for any interval $I$</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-06T12:41:55.000Z" title="12/6/2020, 8:41:55 PM">2020-12-06</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-D-Metric-Linear-Spaces-and-Normed-Linear-Spaces/">1.1.D Metric Linear Spaces and Normed Linear Spaces</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/06/Mathematics/Analysis/7%20Normed%20Linear%20Spaces/Elements-of-Normed-Linear-Spaces/">Elements of Normed Linear Spaces</a></h1><div class="content"><p>Any metrix linear space $X$ that has the absolute homogeneity can be considered as a normed liner space under the norm $\|\cdot\|<em>d\in\mathbb{R}^X</em>+$ with $\|x\|_d:=d(x,\mathbf{0})$</p>
<p>In fact, not only that the metric of any such space arises from a norm, but it is also true that there is an obvious way of “making” a normed linear space $X$ into a metric linear space that has the absolute homogeneity.</p>
<p>Indeed, any given norm $\|\cdot\|$ on $X$ readily induces a distance function $d_{\|\cdot\|} $ on $X$: $d_{\|\cdot\|}:=\|x-y\|$ $\mbox{for all}\ x,y\in X$.</p>
<p>Therefore, we say that a normed linear space is a metric linear space, but not conversely.</p>
<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=F27200>Norm</font>, <font color=941100>Normed Linear Space</font></strong>:</p>
<ul>
<li><p>Let $X$ be a linear space. </p>
</li>
<li><p>A function $\|·\| : X → \mathbb{R}_+$ that satisfies the following properties is called a <strong><font color=F27200>norm</font></strong> on $X$: </p>
</li>
<li><p>For all $\mathbf{x}, \mathbf{y} ∈ X$,</p>
<ol>
<li>(Positive Definite)$\|\mathbf{x}|=0$ iff $\mathbf{x}=\mathbf{0}$,</li>
<li>(Absolute Homogeneity) $\|λ\mathbf{x}\| = |λ| \|\mathbf{x}\|$ for all $λ ∈ \mathbb{R} $,</li>
<li>(Subadditivity) $\|\mathbf{x} + \mathbf{y}\| ≤ \|\mathbf{x}\| + \|\mathbf{y}\|$ .</li>
</ol>
</li>
<li><p>If $\|·\|$ is a norm on $X$, then we say that $(X, \|·\|)$ is a <strong><font color=941100>normed linear space</font></strong>. </p>
</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1</strong>: </p>
<ul>
<li>The norm of any normed linear space $X$ is a nonexpansive map on $X$.</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>$p$-norm, $|\cdot|_p$</strong>:</p>
<ul>
<li>For any $1 ≤ p ≤ ∞$, the $p$-norm $\|·\|_p$ on $\mathbb{R}^n$ is defined as</li>
</ul>
<p>$$<br>\|x\|<em>p:=\left(\sum^n</em>{i=1}|x_i|^p\right)^\frac1{p}<br>$$</p>
<ul>
<li>if $p$ is finite, and</li>
</ul>
<p>$$<br>  \|x\|_p := \max\{|x_i| : i = 1,…,n\}<br>$$</p>
<ul>
<li> if $p = ∞$. </li>
</ul>
<p>$\ell^p$: </p>
<ul>
<li>Let $1 ≤ p &lt; ∞$. The space $\ell^p$ becomes a normed linear space when endowed with the norm $\|·\|<em>p : \ell^p → \mathbb{R}</em>+$ defined by</li>
</ul>
<p>$$<br>|(x_m)|<em>p:=\left(\sum^n</em>{i=1}|x_i|^p\right)^\frac1{p}=d_p((x_m),\mathbf{0})<br>$$</p>
<ul>
<li><p>Similarly, we make $\ell^∞$ a normed linear space by endowing it with the norm $\|·\|<em>∞ : \ell^∞ → \mathbb{R}</em>+$ defined by<br>$$<br>|(x_m)|_∞ := \sup{|x_m| : m ∈ \mathbb{N}} = d_∞ ((x_m),\mathbf{0}).<br>$$</p>
</li>
<li><p>It is easily checked that $\|·\|_∞$ is indeed a norm on $\ell^∞$.</p>
</li>
</ul>
<p><strong>$\mathbf{B}(T)$, sup-norm</strong>: </p>
<ul>
<li>For any nonempty set $T$, the linear space $\mathbf{B}(T)$ of all bounded real functions on $T$ is normed by $|·|<em>∞ : \mathbf{B}(T) → \mathbb{R}</em>+$, where</li>
</ul>
<p>$$<br>|f|_∞ := \sup{|f(t)| : t ∈ T} = d_∞ (f,\mathbf{0}).<br>$$</p>
<ul>
<li>For obvious reasons, $|·|_∞$ is called the sup-norm. </li>
<li>Of course, $\mathbf{B}(\mathbb{N})$ and $\ell^∞$ are the same normed linear spaces.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-25T11:41:54.000Z" title="11/25/2020, 7:41:54 PM">2020-11-25</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-D-Metric-Linear-Spaces-and-Normed-Linear-Spaces/">1.1.D Metric Linear Spaces and Normed Linear Spaces</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/25/Mathematics/Analysis/6%20Metric%20Linear%20Spaces/Elements-of-Metric-Linear-Spaces/">Elements of Metric Linear Spaces</a></h1><div class="content"><h1 id="Elements-of-Metric-Linear-Spaces"><a href="#Elements-of-Metric-Linear-Spaces" class="headerlink" title="Elements of Metric Linear Spaces"></a>Elements of Metric Linear Spaces</h1><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=F27200>Translation Invariant</font></strong>:  </p>
<ul>
<li><p>Let $X$ be a linear space. </p>
</li>
<li><p>A metric $d ∈ \mathbb{R}^{X×X}_+$ is called <strong><font color=F27200>translation invariant</font></strong> if</p>
</li>
</ul>
<p>$$<br>d(x+z,y+z)=d(x,y)\ \ \ \mbox{for all}\ \ \ \ x,y,z∈X.<br>$$</p>
<p>**<font color=941100>Metric Linear Space</font> **: </p>
<ul>
<li><p>Let $X$ be a linear space which is also a metric space. </p>
</li>
<li><p>If the metric $d$ of $X$ is translation invariant, and, for all convergent $(λ_m) ∈ \mathbb{R} ^∞$ and $(x_m) ∈ X^∞$, we have</p>
</li>
</ul>
<p>$$<br>\limλ_mx_m =(\limλ_m)(\lim x_m),<br>$$</p>
<ul>
<li>then $X$ is called a <strong><font color=941100>metric linear space</font></strong>. </li>
<li>If, in addition, $(X, d)$ is complete, then, we say that $X$ is a <strong>Fréchet space</strong>. </li>
<li>A metric linear space $X$ is said to be <strong>nontrivial</strong> if $X \neq {\mathbf{0}}$, <strong>finite dimensional</strong> if $dim(X) &lt; ∞$, and <strong>infinite dimensional</strong> if $dim(X) = ∞$.<ul>
<li>Put succinctly, a metric linear space is a linear space endowed with a translation invariant distance function that renders the scalar multiplication operation continuous</li>
<li>So, it follows from Proposition 1 that $X$ is a metric linear space iff it is both a linear and a metric space such that<ol>
<li>The distance between any two points are preserved under the identical translations of these points,</li>
<li>The scalar multiplication map $(λ, x) \mapsto λx$ is a continuous function from $\mathbb{R} ×X$ into $X$, and</li>
<li>The vector addition map $(x, y) \mapsto x + y$ is a continuous function from $X × X$ into $X$.</li>
</ol>
</li>
</ul>
</li>
</ul>
<p><strong>Metric Linear Subspace</strong>: </p>
<ul>
<li>A <strong>metric linear subspace</strong> of a metric linear space $X$ is defined as a subset of $X$ which is both a linear and a metric subspace of $X$. </li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1</strong>: </p>
<ul>
<li><p>Let $X$ be a linear space which is also a metric space. </p>
</li>
<li><p>If the metric $d$ of $X$ is translation invariant, then the map $(x, y) \mapsto x + y$ is a continuous function from $X × X$ into $X$.</p>
</li>
</ul>
<p><strong>Proposition 2</strong>: </p>
<ul>
<li>For any subsets $A$ and $B$ of a metric linear space $X$, the following are true:<ol>
<li>$cl_X(A + x) = cl_X(A) + x$ for all $x ∈ X$;</li>
<li>If $A$ is open, then so is $A+B$;</li>
<li>If $A$ is compact and $B$ is closed, then $A + B$ is closed; </li>
<li>If both $A$ and $B$ are compact, so is $A+B$.</li>
</ol>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-10-26T10:51:53.000Z" title="10/26/2020, 6:51:53 PM">2020-10-26</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-C-Linear-Spaces-and-Convexity/">1.1.C Linear Spaces and Convexity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/10/26/Mathematics/Analysis/5%20Convexity/Convex-Sets/">Convex Sets</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=531B93>Convex Set</font></strong>: </p>
<ul>
<li>For any $0 &lt; λ &lt; 1$, a subset $S$ of a linear space $X$ is said to be <strong>$λ$-convex</strong> if</li>
</ul>
<p>$$<br>λx+(1−λ)y∈S\ \ \ \mbox{for any}\ \ \ x,y∈S<br>$$</p>
<ul>
<li><p>or equivalently, if<br>$$<br>λS+(1−λ)S=S<br>$$</p>
</li>
<li><p>If $S$ is $λ$-convex for all $0 &lt; λ &lt; 1$, that is, if<br>$$<br>λS+(1−λ)S= S\ \ \ \mbox{for all} \ \ \ \ 0≤λ≤1,<br>$$</p>
</li>
<li><p>then it is said to be a <strong><font color=531B93>convex set</font></strong>.</p>
</li>
</ul>
<blockquote>
<p>For any two distinct vectors $x$ and $y$ in the linear space $X$, we think of the set ${λx+(1−λ)y ∈ X : λ ∈ \mathbb{R} }$ as the line through $x$ and $y$. </p>
<p>The set ${λx+(1−λ)y ∈ X :0 ≤ λ ≤ 1}$ corresponds to the line segment between $x$ and $y$.</p>
<p>A subset $S$ of $X$ is is convex iff it contains the entire line segment between any two of its elements.</p>
</blockquote>
<blockquote>
<p>The affine hull of a convex set can in general be expressed much more easily than that of a nonconvex set. Indeed, for any convex subset $S$ of a linear space, we simply have<br>$$<br>aff (S) = {λx + (1 − λ)y : λ ∈ \mathbb{R} \mbox{ and } x, y ∈ S}.<br>$$</p>
</blockquote>
<p><strong><font color=941751>Convex Hull</font></strong>: </p>
<ul>
<li><p>Let $S$ be any set in a linear space $X$, and let $\mathcal{S}$ be the class of all convex subsets of $X$ that contain $S$.</p>
</li>
<li><p>We have $  \mathcal{S}\neq∅$ — after all, $X∈\mathcal{S}$. </p>
</li>
<li><p>$\bigcap \mathcal {S}$ is a convex set in $X$ which contains $S$. </p>
</li>
<li><p>This set is the smallest (that is, $⊇$-minimum) subset of $X$ that contains $S$ — it is called the <strong><font color=941751>convex hull</font></strong> of $S$, and denoted by $co(S)$. </p>
<ul>
<li>The closed convex hull is the intersection of all closed convex set containing $S$</li>
</ul>
</li>
<li><p>It is worth noting that the definition of $co(S)$ uses vectors that lie outside $S$ (because the members of $S$ may well contain such vectors). For this reason, this definition can be viewed as an external one. </p>
<ul>
<li>We may also characterize $co(S)$ internally, that is, by using only the vectors in $S$. </li>
<li>$co(S)$ is  the set of all convex combinations of finitely many members of $S$. That is,</li>
</ul>
<p>$$<br>co(S)=\left\{\sum^m_{i=1}\lambda_ix^i:m\in\mathbb{N},\ x^1,\dots,x^m\in S\ \mbox{and}\ (\lambda_1,\cdots,\lambda_m)\in[0,1]^m \mbox{ and } \sum^m_{i=1}\lambda_i=1 \right\}<br>$$</p>
</li>
</ul>
<blockquote>
<p>$S = co(S)$ iff $ S$ is convex.</p>
<p>Moreover, for any $x, y ∈ X$, we see that $co({x, y})$ — which is denoted simply as $co{x,y}$ henceforth — is nothing but the line segment between $x$ and $y$, and we have<br>$$<br>co(S) = \bigcup{co(T ) : T ∈ \mathcal{P}(S)},<br>$$<br>where $\mathcal{P}(S)$ is the class of all nonempty finite subsets of $S$.</p>
</blockquote>
<p><strong>Concave Function, Convex Function</strong>: </p>
<ul>
<li><p>Let $X$ be a linear space and $T$ a nonempty convex subset of $X$. </p>
</li>
<li><p>A real map $φ$ on $T$ is called <strong>concave</strong> if</p>
</li>
</ul>
<p>$$<br>φ(λx+(1−λ)y)≥λφ(x)+(1−λ)φ(y)\ \ \ \ \mbox{for all}\ \ \ \ x,y∈T\ \ \ \mbox{and} \ \ \ \ 0≤λ≤1,<br>$$</p>
<ul>
<li>while it is called <strong>convex</strong> if $−φ$ is concave. </li>
</ul>
<blockquote>
<p>If both $φ$ and $−φ$ are concave, then $φ$ is an affine map</p>
</blockquote>
<p><strong><font color=F27200>Cone</font>, <font color=F27200>Convex Cone</font></strong>: </p>
<ul>
<li><p>A nonempty subset $C$ of a linear space $X$ is said to be a **<font color=F27200>cone </font>**if it is closed under nonnegative scalar multiplication, that is, $λC ⊆ C$ for all $λ ≥ 0$, i.e.<br>$$<br>λx∈C\ \ \ \mbox{for any}\ \ \ x∈C\ \ \ \mbox{and}\ \ \ λ≥0.<br>$$</p>
</li>
<li><p>If $C$ is, in addition, closed under addition, that is, $C + C ⊆ C$, i.e.<br>$$<br>x+y∈C\ \ \ \ \mbox{for any}\ \ \ \ \ x,y∈C,<br>$$</p>
</li>
<li><p>then it is called a <strong><font color=F27200>convex cone</font></strong>. </p>
<ul>
<li>We say that a cone $C$ in $X$ is pointed if $C ∩ −C ={\mathbf{0} }$, generating if $\mbox{span}(C) = X$, and nontrivial if $C \neq {\mathbf{0}}$.</li>
</ul>
</li>
</ul>
<blockquote>
<p>Geometrically speaking, a cone is a set that contains all rays that start from the origin and pass through another member of the set. In turn, a convex cone is none other than a cone which is also convex set. </p>
<p>In a manner of speaking, the concept of convex cone lies in between that of a convex set and that of a linear subspace.</p>
<p>Any linear subspace $C$ of a linear space $X$ is a convex cone. </p>
<p>The smallest (i.e. $⊇$-minimum) convex cone in a linear space $X$ is the trivial linear subspace ${\mathbf{0}}$ while the largest (i.e. $⊇$-maximum) convex cone is $X$ itself. Moreover, if $C$ is a convex cone in this space, so is $−C$.</p>
<p>If $X$ and $Y$ are two linear spaces, and $L∈\mathcal{L}(X,Y)$, then ${x∈X:L(x)≥\mathbf{0}}$ is a (pointed) convex cone in $X$. All of these cones are infinite dimensional and generating.</p>
</blockquote>
<p><strong><font color=941751>Conical Hull</font></strong>: </p>
<ul>
<li><p>Let $S$ be any nonempty set in a linear space $X$. </p>
</li>
<li><p>It is easily checked that $S$ is a convex cone in $X$ iff $\sum_{x∈T} λ(x)x ∈ S$ for any nonempty finite subset $T$ of $S$ and $λ ∈ \mathbb{R}^T_+$. </p>
</li>
<li><p>It follows that the smallest convex cone that contains $S$ — the **<font color=941751>conical hull </font>**of $S$ — exists and equals the set of all positive linear combinations of finitely many members of $S$.  </p>
</li>
<li><p>Denoting this convex cone by $cone(S)$, therefore, we may write</p>
</li>
</ul>
<p>$$<br>cone(S)=\left\{\sum_{x\in T}λ(x)x:T ∈\mathcal{P}(S)\ \ \mbox{and}\ \ \ λ∈\mathbb{R}^T_+\right\}<br>$$</p>
<ul>
<li>where, as usual, $\mathcal{P}(S)$ stands for the class of all nonempty finite subsets of $S$. </li>
</ul>
<blockquote>
<p>By convention, we let $cone(∅) = {\mathbf{0} }$</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Examples</strong>:</p>
<ul>
<li>Note that $\mathbb{Q} $ is a midpoint convex subset of $\mathbb{R} $ which is not convex. </li>
<li>Indeed, a nonempty subset of $\mathbb{R}$ is convex iff it is an interval. </li>
<li>In any linear space $X$, all singleton sets, line segments and lines, along with $∅$, are convex sets. </li>
<li>Any linear subspace, affine manifold, hyperplane or half space in $X$ is also convex. </li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li>Both ${f∈\mathbf{C}^1[0,1]:f≥0}$ and $C:={f∈\mathbf{C}^1[0,1]:f’ ≥0}$ are convex cones in $\mathbf{C}^1[0, 1]$. The former one is pointed, but the latter is not — all constant functions belong to $C ∩ −C$.</li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li>For any $n ∈ \mathbb{N} $, $cone(\mathbb{R}^n_{++}) = \mathbb{R}^n_{++}∪ {\mathbf{0}}$ and $cone({\mathbf{e}^1, …, \mathbf{e}^n}) = \mathbb{R}^n_+$, where ${\mathbf{e}^1, …, \mathbf{e}^n}$ is the standard basis for $\mathbb{R}^n$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-10-26T09:41:54.000Z" title="10/26/2020, 5:41:54 PM">2020-10-26</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-C-Linear-Spaces-and-Convexity/">1.1.C Linear Spaces and Convexity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/10/26/Mathematics/Analysis/4%20Linear%20Spaces/Linear-Algebra/">Linear Algebra</a></h1><div class="content"><h1 id="Linear-Algebra"><a href="#Linear-Algebra" class="headerlink" title="Linear Algebra"></a>Linear Algebra</h1><p>A point in $\mathbb{R}^n$ is represented by an $n$-tuple of elements of $\mathbb{R}$, written $\mathbf{p} = (p_1, . . . , p_n)$, with each $p_i ∈ \mathbb{R}$. </p>
<p>Geometrically, these are thought of as points in space, where the $p_i$’s give the coordinates of the point $\mathbf{p}$. At the same time, we may consider $n$-tuples of real numbers as $n$-dimensional vectors giving the data of a direction and a magnitude, without specifying a base point from which this vector emanates. </p>
<p>Thinking this way, we see that such vectors are elements of a vector space, $\mathbb{E}^n$, where elements can be written as $\mathbf{v} = (v_1,…,v_n)$, with each $v_i ∈ \mathbb{R}$. We will consistently distinguish between the “points” of $\mathbb{R}^n$, and “vectors” in $\mathbb{E}^n$, since geometrically they are quite different. </p>
<p>Vectors are free to wander around in space, and points have to stay where they are.</p>
<p>vector + vector = vector, </p>
<p>point + vector = point,</p>
<p>point − point = vector</p>
<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=005493>Algebra</font></strong>:</p>
<ul>
<li>Let $F$ be a field. </li>
<li>An <strong><font color=005493>algebra</font></strong> over $F$ is a set $A$ such that $A$ is a vector space over $F$ and $A$ has an internal law of composition $◦$ satisfying the associative law, and left and right distributivity. </li>
<li>That is, for $a,b,c ∈ A$, we have<ol>
<li>(Associativity) $a ◦ (b ◦ c) = (a ◦ b) ◦ c$, </li>
<li>(Left Distributivity) $a ◦ (b + c) = (a ◦ b) + (a ◦ c)$ and </li>
<li>(Right Distributivity) $(a + b) ◦ c = (a ◦ c) + (b ◦ c)$.</li>
</ol>
</li>
<li>For scalar multiplication we have for $α ∈ F$ , <ol>
<li>$(α · a) ◦ b = α · (a ◦ b) = a ◦ (α · b)$.</li>
</ol>
</li>
<li>An algebra $A$ is an <strong>algebra with identity</strong> if there is an element $\mathbf{1} ∈ A$ so that $a◦\mathbf{1} = \mathbf{1}◦a = a$ for all $a ∈ A$. </li>
<li>The algebra $A$ is a <strong>commutative algebra</strong> if $a ◦ b = b ◦ a$ for all $a, b ∈ A$.</li>
</ul>
<p><strong>Line</strong>:</p>
<ul>
<li>Let $\mathbf{p}_0$ be a point in $\mathbb{R}^n$ and $\mathbf{v}$ be a direction in $\mathbb{E}^n$. </li>
<li>The line $\ell$ through $\mathbf{p}_0$ in the direction $\mathbf{v}$ is given by $\ell=\{\mathbf{p}∈\mathbb{R}^n |\mathbf{p}=\mathbf{p}_0 +t\mathbf{v}, t∈\mathbb{R}\}$.</li>
</ul>
<p><strong>Plane</strong>:</p>
<ul>
<li>Suppose that $\mathbf{p}_0\in\mathbb{R}^n$ and $\mathbf{v}, \mathbf{w}$ are linearly independent vectors in $\mathbb{E}^n$. </li>
<li>The <strong>plane</strong> through $\mathbf{p}_0$ spanned by $\mathbf{v}$ and $\mathbf{w}$ is $\mathcal{P} = {\mathbf{p} ∈ \mathbb{R}^n | \mathbf{p} = \mathbf{p}_0 + t\mathbf{v} + s\mathbf{w}, t, s ∈ \mathbb{R}}$. </li>
</ul>
<p><strong>Affine Subspace, Hyperplane</strong>:</p>
<ul>
<li>If $\mathbf{v}_1, . . . , \mathbf{v}_k$ are linearly independent vectors in $\mathbb{E}^n$, then we define the $k$-dimensional <strong>affine subspace</strong> through $\mathbf{p}_0\in\mathbb{R}^n$ spanned by $\mathbf{v}_1, . . . , \mathbf{v}_k$ as</li>
</ul>
<p>$\mathbf{H}=\{\mathbf{p}\in\mathbb{R}^n |\mathbf{p}=\mathbf{p}_0+t_1\mathbf{v}_1+…+t_k\mathbf{v}_k,\ \mbox{where}\ t_j ∈\mathbb{R},1≤j≤k\}$.</p>
<ul>
<li>The collection of vectors $\{t_1\mathbf{v}_1 + … + t_k\mathbf{v}_k,\ t_j ∈ \mathbb{R}\}$ is actually a subspace of $\mathbb{E}^n$. Thus, a $k$-dimensional affine subspace is constructed by taking a $k$-dimensional subspace of  $\mathbb{E}^n$ and adding it to a point of  $\mathbb{R}^n$. </li>
<li>When $k=n−1$, $\mathbf{H}$ is called a <strong>hyperplane</strong> in $\mathbb{R}^n$.</li>
</ul>
<p><strong>Parallelogram</strong>:</p>
<ul>
<li>If $\mathbf{v}_1, . . . , \mathbf{v}_k$ are linearly independent vectors in $\mathbb{E}^n$, and $\mathbf{p}_0\in\mathbb{R}^n$, we define the $k$-dimensional <strong>parallelepiped</strong> with vertex $\mathbf{p}_0$ spanned by $\mathbf{v}_1, . . . , \mathbf{v}_k$ as $\mathbf{P} = {\mathbf{p} ∈ \mathbb{R}^n | \mathbf{p} = \mathbf{p}_0 + t_1\mathbf{v}_1 + . . . + t_k\mathbf{v}_k,\ \mbox{with}\ 0 ≤ t_j ≤ 1}$.</li>
<li>Note that if $k = n = 2$ then $\mathbf{P}$ is just a standard <strong>parallelogram</strong> in $\mathbb{R}^2$.</li>
</ul>
<p><strong>Bilinear Form</strong>:</p>
<ul>
<li><p>Let $V$ be a vector space over a field $F$ . A <strong>bilinear form</strong> $⟨·, ·⟩$ on $V$ is a map $⟨·, ·⟩ : V × V → F$ which satisfies linearity in both variables.</p>
</li>
<li><p>That is, for all $\mathbf{v},\ \mathbf{v}_1,\ \mathbf{v}_2,\ \mathbf{w},\ \mathbf{w}_1, \mathbf{w}_2 ∈V$, and all $α∈F$,</p>
<ol>
<li>$⟨\mathbf{v}_1 +\mathbf{v}_2,\mathbf{w}⟩=⟨\mathbf{v}_1,\mathbf{w}⟩+⟨\mathbf{v}_2,\mathbf{w}⟩ $</li>
<li>$⟨α\mathbf{v}, \mathbf{w}⟩ = α⟨\mathbf{v}, \mathbf{w}⟩$</li>
<li>$⟨\mathbf{v},\mathbf{w}_1 +\mathbf{w}_2⟩=⟨\mathbf{v},\mathbf{w}_1⟩+⟨\mathbf{v},\mathbf{w}_2⟩ $</li>
<li>$⟨\mathbf{v}, α\mathbf{w}⟩ = α⟨\mathbf{v}, \mathbf{w}⟩$.</li>
</ol>
</li>
<li><p>The form $⟨·, ·⟩$ is said to be symmetric if $⟨\mathbf{v}, \mathbf{w}⟩ = ⟨\mathbf{w}, \mathbf{v}⟩$ for all $\mathbf{v}, \mathbf{w} ∈ V$ .</p>
</li>
</ul>
<p><strong>Positive Definite</strong>:</p>
<ul>
<li>Let $V$ be a vector space over $\mathbb{R}$. </li>
<li>The bilinear form $⟨·, ·⟩$ is said to be <strong>positive definite</strong> if $⟨\mathbf{v}, \mathbf{v}⟩ ≥ 0$ for all $\mathbf{v} ∈ V$ , and $⟨\mathbf{v}, \mathbf{v}⟩ = 0$ iff $\mathbf{v} = 0$.</li>
</ul>
<p><strong>Scalar Product, Dot Product</strong>:</p>
<ul>
<li>Suppose that $\mathbf{v} = (\mathbf{v}_1,…,\mathbf{v}_n)$ and $\mathbf{w} = (\mathbf{w}_1,…,\mathbf{w}_n)$ are vectors in $\mathbb{E}^n$. The <strong>scalar product</strong> of $\mathbf{v}$ and $\mathbf{w}$ is $⟨\mathbf{v}, \mathbf{w}⟩ = \mathbf{v}_1\mathbf{w}_1 + . . . + \mathbf{v}_n\mathbf{w}_n$. </li>
<li>The scalar product is sometimes called the <strong>dot product</strong> and is denoted by $\mathbf{v}· \mathbf{w}$. </li>
</ul>
<p><strong>Length, Norm</strong>:</p>
<ul>
<li>If $\mathbf{v} = (\mathbf{v}_1,…,\mathbf{v}_n) ∈ \mathbb{E}^n$, then the <strong>length</strong> or <strong>norm</strong> of $\mathbf{v}$ is defined by $\|\mathbf{v}\|=\sqrt{⟨\mathbf{v}, \mathbf{v}⟩}=(v_1^2+\dots+v_n^2)^\frac{1}{2}$</li>
</ul>
<p><strong>Orthogonal, Mutually Orthogonal</strong>:</p>
<ul>
<li>Let $\mathbf{v}, \mathbf{w} ∈ \mathbb{E}^n$. Then $\mathbf{v}$ and $\mathbf{w}$ are said to be <strong>orthogonal</strong> if $⟨\mathbf{v}, \mathbf{w}⟩ = 0$. </li>
<li>A set $\{\mathbf{v}_1,…,\mathbf{v}_n\}$ of vectors in $\mathbb{E}^n$ is said to be <strong>mutually orthogonal</strong> or <strong>pairwise orthogonal</strong> if $⟨\mathbf{v}_i, \mathbf{v}_j ⟩ = 0$ for all pairs $i,j$ with $i\neq j$.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Theorem 2.5.12</strong>: (Cauchy-Schwarz Inequality) </p>
<ul>
<li>Let $\mathbf{v}, \mathbf{w} ∈ \mathbb{E}^n$ . Then $|⟨\mathbf{v}, \mathbf{w}⟩| ≤ \|\mathbf{v}\|\|\mathbf{w}\|$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-10-26T09:41:53.000Z" title="10/26/2020, 5:41:53 PM">2020-10-26</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-C-Linear-Spaces-and-Convexity/">1.1.C Linear Spaces and Convexity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/10/26/Mathematics/Analysis/4%20Linear%20Spaces/Linear-Operators-and-Functionals/">Linear Operators and Functionals</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=F27200>Linear Operator, Linear Transformation</font></strong>: </p>
<ul>
<li>Let $X$ and $Y$ be two linear spaces. </li>
<li>A function $L : X → Y$ is called a <strong><font color=F27200>linear operator</font></strong> (or a <strong><font color=F27200>linear transformation</font></strong>) if</li>
</ul>
<p>$$<br>L(α\mathbf{x}+\mathbf{x}’)=αL(\mathbf{x})+L(\mathbf{x}’)\ \ \ \mbox{for all}\ \ \ \ \mathbf{x},\mathbf{x}’ ∈X\ \ \ \mbox{and}\ \ \ \ α∈\mathbb{R}<br>$$</p>
<ul>
<li>or equivalently,<br>$$<br>L\left(\sum^m_{i=1}\alpha_i\mathbf{x}^i\right)=\sum^m_{i=1} α_iL(\mathbf{x}^i)\ \ \ \ \ \mbox{for all}\ \ \ \ m ∈ \mathbb{N}\ \ \ \ \ \mbox{and}\ \ \ \ (\mathbf{x}^i, α_i) ∈ X × \mathbb{R},\ \ \ i = 1, …, m.<br>$$</li>
</ul>
<blockquote>
<p>As is customary, we don’t adopt a notation that makes this explicit</p>
<p>The $+$ operation on the left of the equation in<br>$$<br>L(α\mathbf{x}+\mathbf{x}’)=αL(\mathbf{x})+L(\mathbf{x}’)\ \ \ \mbox{for all}\ \ \ \ \mathbf{x},\mathbf{x}’ ∈X\ \ \ \mbox{and}\ \ \ \ α∈\mathbb{R}<br>$$<br>is not the same as the $+$ operation on the right of this equation. Indeed, the former one is the addition operation on $X$ while the latter is that on $Y$. </p>
<p>The same comment applies to the $\cdot $ operation as well — we use the scalar multiplication operation on $X$ when writing $αx$, and that on $Y$ when writing $αL(x)$.</p>
</blockquote>
<p><strong>Null Space, Kernel</strong>: </p>
<ul>
<li>The set $L^{−1}(0)$ is called the null space (or the kernel) of $L$, and is denoted by $null(L)$, that is,</li>
</ul>
<p>$$<br>null(L):={x∈X :L(x)=0}.<br>$$</p>
<p><strong><font color=941751>Linear Funtional</font></strong>: </p>
<ul>
<li>A real-valued linear operator is referred to as a **<font color=941751>linear functional </font>**on $X$.</li>
</ul>
<blockquote>
<p>The set of all linear operators from a linear space $X$ into a linear space $Y$ is denoted as $\mathcal{L}(X,Y)$. So, $L$ is a linear functional on $X$ iff $L ∈ \mathcal{L}(X,\mathbb{R} )$.</p>
</blockquote>
<p><strong>Linear Correspondence</strong>: </p>
<ul>
<li>Let $X$ and $Y$ be two linear spaces, and $Γ : X \rightrightarrows  Y$ a correspondence. We say that $Γ$ is a linear correspondence if</li>
</ul>
<p>$$<br>αΓ(x) ⊆ Γ(αx)\ \ \ \ \mbox{and}\ \ \ \ Γ(x) + Γ(x’) ⊆ Γ(x + x’)<br>$$</p>
<ul>
<li>for all $x, x’ ∈ X$ and $α ∈ \mathbb{R} $.</li>
</ul>
<p><strong><font color=941751>Linear Function</font></strong>:  </p>
<ul>
<li>Let $S$ be a nonempty subset of a linear space $X$, and denote by $\mathcal{P}(S)$ the class of all nonempty finite subsets of $S$. </li>
<li>A real map $φ ∈ \mathbb{R}^S$ is linear if</li>
</ul>
<p>$$<br>φ\left(\sum_{\mathbf{x}\in A}λ(\mathbf{x})\mathbf{x}\right) = \sum_{\mathbf{x}\in A}λ(\mathbf{x})φ(\mathbf{x})<br>$$</p>
<ul>
<li>for any $A∈\mathcal{P}(S)$ and $λ∈\mathbb{R}^A $ such that $\sum_{\mathbf{x}∈A}λ(\mathbf{x})\mathbf{x}∈S$.</li>
</ul>
<p><strong>Affine Function</strong>: </p>
<ul>
<li>Let $S$ be a nonempty subset of a linear space $X$, and denote by $\mathcal{P}(S)$ the class of all nonempty finite subsets of $S$. </li>
<li>A real map $φ ∈ \mathbb{R}^S$ is called <strong>affine</strong> if</li>
</ul>
<p>$$<br>φ\left(\sum_{x\in A}λ(\mathbf{x})\mathbf{x}\right) = \sum_{\mathbf{x}\in A}λ(\mathbf{x})φ(\mathbf{x})<br>$$</p>
<ul>
<li>for any $A∈\mathcal{P}(S)$ and $λ∈\mathbb{R}^A$ such that $\sum_{\mathbf{x}∈A}λ(\mathbf{x})=1$ and $\sum_{\mathbf{x}∈A}λ(\mathbf{x})\mathbf{x}∈S$.</li>
</ul>
<p><strong>Linear Isomorphism</strong>: </p>
<ul>
<li>Let $X$ and $Y$ be two linear spaces and $L ∈ \mathcal{L}(X,Y)$. </li>
<li>If $L$ is a bijection, then it is called a <strong>linear isomorphism</strong> between $X$ and $Y$, and we say that $X$ and $Y$ are <strong>isomorphic</strong>.</li>
</ul>
<blockquote>
<p>Just like two isometric metric spaces are indistinguishable from each other insofar as their metric properties are concerned, the linear algebraic structures of two isomorphic linear spaces coincide. </p>
<p>Put differently, from the perspective of linear algebra, one can regard two isomorphic linear spaces as differing from each other only in the labelling of their constituent vectors.</p>
<p>It is quite intuitive that a finite dimensional linear space can never be isomorphic to an infinite dimensional linear space. But there is more to the story. Given the interpretation of the notion of isomorphism, you might expect that the dimensions of any two isomorphic linear spaces must in fact be identical. </p>
</blockquote>
<p><strong>Closed Halfspace, Open Halfspace</strong>: </p>
<ul>
<li><p>Given Corollary 4, we may identify a hyperplane $H$ in a linear space $X$ with a real number and a nonzero linear functional $L$ on $X$. </p>
</li>
<li><p>This allows us to give an analytic form to the intuition that a hyperplane “divides” the entire space into two parts. </p>
</li>
<li><p>In keeping with this, we refer to either one of the sets<br>$$<br>{x∈X :L(x)≥α}\ \ \ \mbox{and}\ \ \  {x∈X :L(x)≤α}<br>$$</p>
</li>
<li><p>as a <strong>closed halfspace</strong> induced by $H$, and to either one of the sets<br>$$<br>{x∈X :L(x)&gt;α}\ \ \ \ \mbox{and}\ \ \ \ {x∈X :L(x)&lt;α}<br>$$</p>
</li>
<li><p>as an <strong>open halfspace</strong> induced by $H$.</p>
</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Lemma 1</strong>: </p>
<ul>
<li>Let $T$ be a subset of a linear space with $\mathbf{0} ∈T$. Then $φ∈\mathbb{R}^T $ is affine iff, $φ − φ(\mathbf{0} )$ is a linear real map on $T$ .</li>
</ul>
<blockquote>
<p>It is important to note that this fact remains true even if the domain of the affine map is not a linear space.</p>
</blockquote>
<p><strong>Proposition 4</strong>: </p>
<ul>
<li>Let $n∈\mathbb{N} $ and $∅\neq S⊆\mathbb{R}^n$. Then $φ∈\mathbb{R}^S$ is affine iff, there exist real numbers $α_1, …, α_n, \beta$, such that $φ(x) =\sum^n α_ix_i + β$ for all $x ∈ S$.</li>
</ul>
<p><strong>Proposition 5</strong>: </p>
<ul>
<li>Two finite dimensional linear spaces are isomorphic iff, they have the same dimension.</li>
</ul>
<p><strong>Corollary 3</strong>: </p>
<ul>
<li>Every nontrivial finite dimensional linear space is isomorphic to $\mathbb{R}^n$, for some $n ∈ \mathbb{N} $.</li>
</ul>
<p><strong>Proposition 6</strong>: </p>
<ul>
<li><p>Let $Y$ be a subset of a linear space $X$. </p>
</li>
<li><p>Then, $Y$ is a $⊇$-maximal proper linear subspace of $X$ iff,<br>$$<br>Y = null(L)<br>$$</p>
</li>
<li><p>for some nonzero linear functional $L$ on $X$.</p>
</li>
</ul>
<p><strong>Corollary 4</strong>: </p>
<ul>
<li>A subset $H$ of a linear space $X$ is a hyperplane in $X$ iff,</li>
</ul>
<p>$$<br>H ={x∈X :L(x)=α}<br>$$</p>
<ul>
<li>for some $α ∈ \mathbb{R} $ and nonzero linear functional $L$ on $X$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-10-26T09:41:52.000Z" title="10/26/2020, 5:41:52 PM">2020-10-26</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-C-Linear-Spaces-and-Convexity/">1.1.C Linear Spaces and Convexity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/10/26/Mathematics/Analysis/4%20Linear%20Spaces/Elements-of-Linear-Spaces/">Elements of Linear Spaces</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=005493>Group, Abelian Group</font></strong>: </p>
<ul>
<li><p>Let $X$ be any nonempty set, and $+$ a binary operation on $X$. </p>
</li>
<li><p>The doubleton $(X, +)$ is called a **<font color=005493>group </font>**if the properties 1-4 are satisfied.</p>
<ol>
<li>(Closure) $\mathbf{x}+\mathbf{y}\in X$ for all $\mathbf{x},\mathbf{y}∈X$;</li>
<li>(Associativity) $(\mathbf{x}+\mathbf{y})+\mathbf{z}=\mathbf{x}+(\mathbf{y}+\mathbf{z})$ for all $\mathbf{x},\mathbf{y},\mathbf{z}∈X$;</li>
<li>(Existence of an identity element) There exists an element $\mathbf{0}  ∈ X$ such that $\mathbf{0}  + \mathbf{x} = \mathbf{x} = \mathbf{x} + \mathbf{0} $ for all $\mathbf{x} ∈ X$;</li>
<li>(Existence of inverse elements) For each $\mathbf{x} ∈ X$, there exists an element $−\mathbf{x} ∈ X$ such that $\mathbf{x} + −\mathbf{x} = 0 = −\mathbf{x} + \mathbf{x}$.</li>
<li>(Commutativity) $\mathbf{x}+\mathbf{y}=\mathbf{y}+\mathbf{x}$ for all $\mathbf{x},\mathbf{y}∈X$,</li>
</ol>
</li>
<li><p>$(X, +)$ is said to be an <strong><font color=005493>Abelian (or commutative) group</font></strong> if all the properties are satisfied.</p>
</li>
</ul>
<blockquote>
<p>For any group $(X, +)$, and any nonempty subsets $A$ and $B$ of $X$, we let<br>$$<br>A + B := {x + y : (x, y) ∈ A × B}<br>$$</p>
</blockquote>
<p><strong><font color=941100>Linear Space</font>, Vector Space</strong>: </p>
<ul>
<li>Let $X$ be a nonempty set. </li>
<li>The list $(X,+,·)$ is called a <font color=941100><strong>linear space</strong></font> (or <strong>vector space</strong>) if $(X, +)$ is an Abelian group, and if $·$ is a mapping that assigns to each $(λ,\mathbf{x}) ∈ \mathbb{R} ×X$ an element $λ·\mathbf{x}$ of $X $ such that, for all $α, λ ∈ \mathbb{R} $ and $\mathbf{x}, \mathbf{y} ∈ X$, we have<ol>
<li>(Closure) $λ\mathbf{x}\in X$</li>
<li>(Associativity) $α(λ\mathbf{x}) = (αλ)\mathbf{x}$;</li>
<li>(Distributivity) $(α+λ)\mathbf{x}=α\mathbf{x}+λ\mathbf{x}$ </li>
<li>(Distributivity) $λ(\mathbf{x}+\mathbf{y})=λ\mathbf{x}+λ\mathbf{y}$;</li>
<li>(Existence of an identity element) $1\mathbf{x} = \mathbf{x}$.</li>
</ol>
</li>
</ul>
<blockquote>
<p>Let $(X,+,·)$ be a linear space, and $A,B ⊆ X$ and $λ ∈ \mathbb{R} $. Then,<br>$$<br>A+B:={x+y:(x,y)∈A×B}\ \ \ \mbox{and}\ \ \ \ λA:={λx:x∈A}.<br>$$</p>
<p>For simplicity, we write $A + y$ for $A + {y}$, and similarly, $y + A := {y} + A$.</p>
</blockquote>
<blockquote>
<p>Linear spaces provide an ideal structure for a proper investigation of convex sets. </p>
</blockquote>
<blockquote>
<p>We use the notation $X$ instead of $(X,+,\cdot)$ for a linear space, but we should always keep in mind that what makes a linear space “linear” is the two operations defined on it. </p>
<p>Two different types of addition and scalar multiplication operations on a given set may well endow this set with different linear structures, and hence yield two very different linear spaces.</p>
</blockquote>
<p><strong>Addition, Scalar Multipulation, Origin, Zero, Vector</strong>: </p>
<ul>
<li>In a linear space $(X,+,·)$, the mappings $+$ and $·$ are called <strong>addition</strong> and <strong>scalar multiplication</strong> operations on $X$, respectively. </li>
<li>The identity element $\mathbf{0} $ is called the <strong>origin</strong> (or <strong>zero</strong>), and any member of $X$ is referred to as a <strong>vector</strong>. <ul>
<li>If $\mathbf{x} ∈ X\backslash {0}$, then we say that $x$ is a nonzero vector in $X$.</li>
</ul>
</li>
</ul>
<p><strong>Linear Subspace, Proper Linear Subspace</strong>: </p>
<ul>
<li><p>Let $X$ be a linear space and $∅ \neq Y ⊆ X$. </p>
</li>
<li><p>If $Y$ is a linear space with the same operations of addition and scalar multiplication as with $X$, then it is called a **linear subspace **of $X$. </p>
</li>
<li><p>If, further, $Y \neq X$, then $Y$ is called a <strong>proper linear subspace</strong> of $X$.</p>
</li>
</ul>
<p><strong>Affine Manifold, Hyperplane</strong>: </p>
<ul>
<li><p>A subset $S$ of a linear space $X$ is said to be an <strong>affine manifold</strong> of $X$ if $S=Z+\mathbf{x}^*$ for some linear subspace $Z$ of $X$ and some vector $\mathbf{x}^* ∈X$. </p>
</li>
<li><p>If $Z$ is a $⊇$-maximal proper linear subspace of $X, S$ is called a <strong>hyperplane</strong> in $X$. </p>
</li>
<li><p>Equivalently, a hyperplane is a $⊇$-maximal proper affine manifold.</p>
</li>
</ul>
<blockquote>
<p>A good way of thinking intuitively about the notion of affinity in linear analysis is this: affinity = linearity + translation.</p>
<p>Since we think of $\mathbf{0} $ as the origin of the linear space $X$ — this is a geometric interpretation; don’t forget that the definition of $\mathbf{0} $ is purely algebraic — it makes sense to view a linear subspace of $X$ as untranslated (relative to the origin of the space), for a linear subspace “passes through” $\mathbf{0} $. </p>
<p>The following simple but important observation thus gives support to our informal equation above: An affine manifold $S$ of a linear space $X$ is a linear subspace of $X$ iff $\mathbf{0}  ∈ S$. </p>
<p>An immediate corollary of this is: If $S$ is an affine manifold of X, then $S −x$ is a linear subspace of $X$ for any $x ∈ S$. </p>
<p>Moreover, this subspace is determined independently of $x$, because if $S$ is an affine manifold, then<br>$$<br>S−x=S−y\ \ \  \mbox{for any}\ \ \ x,y∈S.<br>$$</p>
</blockquote>
<p><strong><font color=FF2600>Linear Combination, Affine Combination, Positive Linear Combination, Convex Combination</font></strong>:  </p>
<ul>
<li>For any $m ∈ \mathbb{N} $, by a <strong><font color=FF2600>linear combination</font></strong> of the vectors $\mathbf{x}^1,…,\mathbf{x}^m$ in a linear space $X$, we mean a vector $\sum^m λ_i\mathbf{x}^i ∈ X$, where $λ_1, …, λ_m$ are any real numbers.</li>
<li>If we have $\sum^m\lambda_i=1$, then $\sum^m\lambda_i\mathbf{x}^i$ is referred to as an <strong><font color=FF2600>affine combination</font></strong> of the vectors $\mathbf{x}^1,…,\mathbf{x}^m$. </li>
<li>If $\lambda_i\geq0$ for each $i$, then $\sum^m\lambda_i\mathbf{x}^i$ is called a <strong><font color=FF2600>positive linear combination</font></strong> of $\mathbf{x}^1,…,\mathbf{x}^m$. </li>
<li>If $\lambda_i\geq0$ for each $i$ and $\sum^m\lambda_i=1$, then $\sum^m\lambda_i\mathbf{x}^i$ is called a <strong><font color=FF2600>convex combination</font></strong> of $\mathbf{x}^1,…,\mathbf{x}^m$. <ul>
<li>Equivalently, a linear (affine (convex)) combination of the elements of a nonempty finite subset $T$ of $X$ is $\sum_{x\in T}\lambda(\mathbf{x})\mathbf{x}$, where $\lambda\in \mathbb{R}^T$. (and $\sum_{\mathbf{x}\in T}\lambda(\mathbf{x})=1$ (and $\lambda(T)\subseteq \mathbb{R}_{+}$)).</li>
</ul>
</li>
</ul>
<p><strong><font color=941751>Span / Linear Hull</font></strong>: </p>
<ul>
<li>The set of all linear combinations of finitely many members of a nonempty subset $S$ of a linear space $X$ is called the <strong><font color=941751>span / linear Hull</font></strong> of $S$ in $X$, and is denoted by $span(S)$. </li>
<li>That is, for any $∅ \neq S ⊆ X$,</li>
</ul>
<p>$$<br>span(S):= \left\{λ_i\mathbf{x}^i :m∈\mathbb{N}\ \mbox{and}\ \ (\mathbf{x}^i,λ_i)∈S×\mathbb{R}, i=1,…,m \right\},<br>$$</p>
<ul>
<li><p>or equivalently,<br>$$<br>span(S)=\left\{\sum_{\mathbf{x}\in T}λ(\mathbf{x})\mathbf{x}:T ∈\mathcal{P}(S)\ \ \ \mbox{and}\ \ λ∈\mathbb{R}^T\right\}<br>$$</p>
</li>
<li><p>where $\mathcal{P}(S)$ is the class of all nonempty finite subsets of $S$. </p>
</li>
</ul>
<blockquote>
<p>By convention, we let $span(∅) = \{\mathbf{0} \}$.</p>
<p>$span(S)$ is the smallest (i.e. $ ⊇$-minimum) linear subspace of the mother space that contains $S$. </p>
<p>Especially when $S$ is finite, this linear subspace has a very concrete description in that every vector in it can be expressed as linear combination of all the vectors in $S$.</p>
</blockquote>
<p><strong><font color=941751>Affine Hull</font></strong>: </p>
<ul>
<li><p>The set of all affine combinations of finitely many members of a nonempty subset $S$ of a linear space $X$ is called the <strong><font color=941751>affine hull</font></strong> of $S$ (in $X$), and is denoted by $aff(S)$. </p>
</li>
<li><p>That is, for any $∅ \neq S ⊆ X$,<br>$$<br>aff(S):=\left\{\sum_{\mathbf{x}\in T}λ(\mathbf{x})\mathbf{x}:T ∈\mathcal{P}(S)\  \ \ \mbox{and}\ \ \ λ∈\mathbb{R}^T\ \ \mbox{with} \sum_{\mathbf{x}\in T}λ(\mathbf{x})=1\right\}<br>$$</p>
</li>
<li><p>where $\mathcal{P}(S)$ is the class of all nonempty finite subsets of $S$. </p>
</li>
</ul>
<blockquote>
<p>By convention, we let $aff(∅) = \{\mathbf{0} \}$.</p>
<p>By Proposition 1, $aff(S)$ is an affine manifold of $X$. </p>
<p>Moreover, again by Proposition 1 and the Principle of Mathematical Induction, any affine manifold of $X$ that contains $S$ also contains $aff (S)$. </p>
<p>Therefore, $aff (S)$ is the smallest affine manifold of $X$ that contains $S$. </p>
<p>Equivalently, this manifold equals the intersection of all affine manifolds of $X$ that contain $S$.</p>
<p>These observations help clarify the nature of the tight connection between the notions of span and affine hull of a set. Put precisely, for any nonempty subset $S$ of a linear space $X$, we have<br>$$<br>aff(S)=span(S−x)+x \mbox{ for any }x∈S.<br>$$</p>
</blockquote>
<p><strong>Linearly Dependent, Linearly Independent</strong>: </p>
<ul>
<li><p>Let $X$ be a linear space. </p>
</li>
<li><p>A subset $S$ of $X$ is <strong>linearly dependent</strong> in $X$ if it either equals ${0}$ or at least one of the vectors in $S$ can be expressed as a linear combination of finitely many vectors in $S\backslash\{\mathbf{x}\}$. </p>
</li>
<li><p>For any $m ∈ \mathbb{N} $, any distinct vectors $\mathbf{x}_1, …, \mathbf{x}_m ∈ X$ are <strong>linearly dependent</strong> if $\{\mathbf{x}_1, …, \mathbf{x}_m\}$ is linearly dependent in $X$.</p>
</li>
<li><p>A subset of $X$ is <strong>linearly independent</strong> in $X$ if no finite subset of it is linearly dependent in $X$. </p>
</li>
<li><p>For any $m ∈ \mathbb{N} $, the vectors $\mathbf{x}_1, …, \mathbf{x}_m ∈ X$ are called <strong>linearly independent</strong> if $\{\mathbf{x}_1, …, \mathbf{x}_m\}$ is linearly independent in $X$.</p>
</li>
</ul>
<blockquote>
<p>It follows from these definitions that any set $S$ in a linear space $X$ with $\mathbf{0} ∈ S$ is linearly dependent in $X$.</p>
<p>For any distinct $x, y ∈ X\backslash\{\mathbf{0}\}$, the set $\{x, y\}$ is linearly dependent in $X$ iff $y ∈ span(\{x\})$ iff $span(\{x\}) = span(\{y\})$. Hence, one says that two nonzero vectors $x$ and $y$ are linearly dependent iff they both lie on a line through the origin. More generally, a subset $S$ of $X\backslash\{\mathbf{0}\}$ is linearly dependent in $X$ iff there exists an $x ∈ S$ such that $x ∈ span(S\backslash\{x\})$.</p>
<p>A fundamental principle of linear algebra is that there cannot be more than $m$ linearly independent vectors in a linear space spanned by $m$ vectors. </p>
</blockquote>
<p><strong>Affinely Independent, Affinely Dependent</strong>: </p>
<ul>
<li><p>Let $X$ be a linear space. </p>
</li>
<li><p>A finite subset $T$ of $X$ is <strong>affinely independent</strong> in $X$ if $\{\mathbf{z} − \mathbf{x} : \mathbf{z} ∈ T\backslash\{\mathbf{x}\}\}$ is linearly independent in $X$ for any $\mathbf{x} ∈ T$. </p>
</li>
<li><p>An arbitrary nonempty subset $S$ of $X$ is <strong>affinely independent</strong> in $X$ if every finite subset of $S$ is affinely independent in $X$. </p>
</li>
<li><p>$S$ is <strong>affinely dependent</strong> in $X$ if it is not affinely independent in $X$.</p>
</li>
</ul>
<blockquote>
<p>Since, at most $n$ vectors can be linearly independent in $\mathbb{R}^n, n = 1, 2, …$, it follows that there can be at most $n + 1$ affinely independent vectors in $\mathbb{R}^n$. </p>
</blockquote>
<p><strong>Basis</strong>: </p>
<ul>
<li>A basis for a linear space $X$ is a $⊇$-minimal subset of $X$ that spans $X$. </li>
<li>That is, $S$ is a **basis **for $X$ iff,<ol>
<li>$X = span(S)$; and</li>
<li>If $X = span(T)$, then $T ⊂ S$ is false.</li>
</ol>
</li>
<li>If a linear space $X$ has a finite basis, then it is said to be finite dimensional, and its dimension, $dim(X)$, is defined as the cardinality of any one of its bases. </li>
<li>If $X$ does not have a finite basis, then it is called infinite dimensional, in which case we write $dim(X) = ∞$.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1</strong>: </p>
<ul>
<li>Let $X$ be a linear space and $∅ \neq S ⊆ X$. </li>
<li>Then $S$ is an affine manifold of $X $ if, and only if,</li>
</ul>
<p>$$<br>λx+(1−λ)y∈S\ \ \  \mbox{for any}\ \ \ x,y∈S\ \ \mbox{and} \ \ \ λ∈\mathbb{R}.<br>$$</p>
<p><strong>Proposition 2</strong>: </p>
<ul>
<li>Let $X$ be a linear space, and $A,B ⊆ X$. If $B$ is linearly independent in $X$ and $B ⊆ span(A)$, then $|B| ≤ |A|$.</li>
</ul>
<p><strong>Proposition 3</strong>: </p>
<ul>
<li>A subset $S$ of a linear space $X$ is a basis for $X$ if, and only if, $S$ is linearly independent and $X = span(S)$.</li>
</ul>
<p><strong>Corollary 1</strong>: </p>
<ul>
<li>Any two bases of a finite dimensional linear space have the same number of elements.</li>
</ul>
<p><strong>Remark 1</strong>: </p>
<ul>
<li>The dimension of an affine manifold $S$ in a linear space $X$, denoted by $dim(S)$, is defined as the dimension of the linear subspace of $X$ that is “parallel” to $S$. Put more precisely,</li>
</ul>
<p>$$<br>dim(S) := dim(S − x)<br>$$</p>
<ul>
<li> for any $x ∈ S$. </li>
</ul>
<p><strong>Corollary 2</strong>: </p>
<ul>
<li><p>Let $S$ be a basis for a linear space $X$. </p>
</li>
<li><p>Any nonzero vector $x ∈ X$ can be expressed as a linear combination of finitely many members of $S$ with nonzero coefficients in only one way.</p>
</li>
<li><p>If $X$ is finite dimensional, then every vector in $X$ can be uniquely written as a linear combination of all vectors in $S$.</p>
</li>
</ul>
<p><strong>Theorem 1</strong>: </p>
<ul>
<li>Every linear space has a basis.</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Examples</strong>:</p>
<ul>
<li>$(\mathbb{Z},+), (\mathbb{Q} ,+), (\mathbb{R} ,+), (\mathbb{R}^n,+)$ and $(\mathbb{R} \backslash\{0\},\cdot)$ are Abelian groups where $+$ and $ \cdot$ are the usual addition and multiplication operations. <ul>
<li>In the $(\mathbb{R} \backslash\{0\},\cdot)$ number $1$ plays the role of the identity element.</li>
</ul>
</li>
<li> $(\mathbb{R} , \cdot)$ is not a group because it does not have inverse elements. Similarly, $(\mathbb{Z} , \cdot)$ and $(\mathbb{Q} , \cdot)$ are not groups.</li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li><p>$\mathbb{R}^n_{++}$ is not a linear space since it does not contain the origin. </p>
<ul>
<li>This is of course not the only problem. </li>
<li>After all, $\mathbb{R}^n_+$ is not a linear space either (under the usual operations), for it does not contain the inverse of any nonzero vector.</li>
</ul>
</li>
<li><p>The real sequence space $\mathbb{R}^∞$ (which is none other than $\mathbb{R}^\mathbb{N} $) and the function spaces $\mathbb{R}^T$ and $\mathbf{B}(T)$ (for any nonempty set $T$) are linear spaces under usual operations. </p>
</li>
<li><p>$\ell^p$ (for any $1 ≤ p ≤ ∞$), along with the function spaces $\mathbf{CB}(T )$ and $\mathbf{C}(T )$ (for any metric space $T$), are linear spaces under usual operations.</p>
<ul>
<li>The same goes as well for other function spaces, such as $\mathbf{P}(T )$ or the space of all polynomials on $T$ of degree $m ∈ \mathbb{Z}_+$ (for any nonempty subset $T$ of $\mathbb{R} $).</li>
</ul>
</li>
<li><p>Since the negative of an increasing function is decreasing, the set of all increasing real functions on $\mathbb{R} $ (or on any compact interval $[a, b]$ with $a &lt; b$) is not a linear space under the usual operations. </p>
<ul>
<li>Less trivially, the set of all monotonic self-maps on $\mathbb{R} $ is not a linear space either. </li>
</ul>
</li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li><p>$[0,1]$ is not a linear subspace of $\mathbb{R} $ whereas $\{x ∈ \mathbb{R}^2 : x_1 +x_2 = 0\}$ is a proper linear subspace of $\mathbb{R}^2$.</p>
</li>
<li><p>For any $n ∈ \mathbb{N} $, $\mathbb{R}^{n×n}$ is a linear space under the usual operations. The set of all symmetric $n × n$ matrices, that is, $\{[a_{ij}]<em>{n×n} : a</em>{ij} = a_{ji}\mbox{ for each }i,j\}$ is a linear subspace of this space.</p>
</li>
<li><p>For any $n∈\mathbb{N} $ and linear $f:\mathbb{R}^n →\mathbb{R} $, $\{x∈\mathbb{R}^n :f(x)=0\}$ is a linear subspace of $\mathbb{R}^n$. </p>
</li>
<li><p>For any $m ∈ \mathbb{N} $, the set of constant functions on $[0, 1]$ is a proper linear subspace of the set of all polynomials on $[0, 1]$ of degree at most $m$. </p>
<ul>
<li>The latter set is a proper linear subspace of $\mathbf{P}[0, 1]$ which is a proper linear subspace of $\mathbf{C}[0, 1]$ which is itself a proper linear subspace of $\mathbf{B}[0, 1]$. </li>
<li>$\mathbf{B}[0, 1]$ is a proper linear subspace of $\mathbb{R}^{[0,1]}$.</li>
</ul>
</li>
<li><p>$\{x ∈ \mathbb{R}^2 : x_1 + x_2 = 1\}$ is not a linear subspace of $\mathbb{R}^2$, since this set does not contain the origin of $\mathbb{R}^2$. </p>
<ul>
<li>On the other hand, geometrically speaking, this set is very “similar” to the linear subspace $\{x ∈ \mathbb{R}^2 : x_1 + x_2 = 0\}$. </li>
</ul>
</li>
</ul>
<ul>
<li>Indeed, the latter is nothing but a parallel shift (translation) of the former set.</li>
</ul>
<p><strong>Examples</strong>:</p>
<ul>
<li>There is no hyperplane in the trivial space $\{\mathbf{0}\}$. <ul>
<li>Since the only proper linear subspace of $\mathbb{R} $ is $\{0\}$, any one-point set in $\mathbb{R} $ and $\mathbb{R} $ itself are the only affine manifolds in $\mathbb{R} $. </li>
<li>So, all hyperplanes in $\mathbb{R} $ are singleton sets. </li>
</ul>
</li>
<li>In $\mathbb{R}^2$, any one-point set, any line (with no endpoints) and the entire $ \mathbb{R}^2$ are the only affine manifolds. <ul>
<li>A hyperplane in this space is necessarily of the form $\{x ∈ \mathbb{R}^2 : a_1x_1 + a_2x_2 = b\}$ for some real numbers $a_1,a_2$ with at least one of them being nonzero, and some real number $b$. </li>
</ul>
</li>
<li>All hyperplanes are of the form of (infinitely extending) planes in $\mathbb{R}^3 $.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-27T09:45:57.000Z" title="9/27/2020, 5:45:57 PM">2020-09-27</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/">1.1.B Metric Spaces and Continuity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/27/Mathematics/Analysis/3%20Continuity/The-Space-of-Functions/">The Space of Functions</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p>$\mathbf{CB}(T)$: </p>
<ul>
<li><p>Let $T$ be any metric space, and $\mathbf{B}(T) $ is the set of all bounded functions defined on $T$, which is endowed with the sup metric $d_∞$ .</p>
</li>
<li><p>An important subspace of $\mathbf{B}(T)$ consists of all continuous and bounded functions on $T$, denoted by $\mathbf{CB}(T)$ or $\mathbf{BC}(T)$. </p>
</li>
<li><p>The set of all continuous functions on $T$ is denoted as $\mathbf{C}(T)$.</p>
<ul>
<li>While the sup-metric cannot be used to metrize $\mathbf{C}(T)$ in general (because a continuous function need not be bounded), this complication disappears when $T$ is compact. </li>
<li>For then, by Corollary 2, we have $\mathbf{CB}(T ) = \mathbf{C}(T )$. </li>
<li>When $T$ is compact, therefore, we can, and we will, think of $\mathbf{C}(T)$ as metrized by the sup-metric.</li>
</ul>
</li>
</ul>
<p><strong>Pointwise Convergence</strong>:</p>
<ul>
<li>Let $(f_n)_{n∈\mathbb{N}}$ be a sequence of functions in $\mathbf{B}(T)$.</li>
<li>We say that a function $f : X → \mathbb{R}$ is the <strong>pointwise limit</strong> of the sequence $(f_n)_{n∈\mathbb{N}}$ if, <u>for every</u> $x ∈ X$, $\lim_{n→∞} f_n(x) = f(x)$.</li>
</ul>
<p><strong>Uniformly Convergence</strong>:</p>
<ul>
<li>Let $(f_n)_{n∈\mathbb{N}}$ be a sequence of functions in $\mathbf{B}(T)$.</li>
<li>The sequence is said to <strong>converge uniformly</strong> to a function $f ∈ \mathbf{B}(T)$ provided that, given $ε &gt; 0$, $∃N_ε ∈ \mathbb{N}$ such that $\sup_{x∈X}|f_n(x) − f(x)| &lt; ε$ for $n ≥ N_ε$.</li>
</ul>
<blockquote>
<p>Uniform convergence is strictly more demanding than the perhaps more familiar notion of pointwise convergence of a sequence of functions </p>
<p>Uniform convergence, being a “global” (i.e. $x$-independent) statement, implies the pointwise convergence which is a “local” ($x$-dependent) condition.</p>
</blockquote>
<p><strong>Separate Points</strong>:</p>
<ul>
<li>Let $A$ be a collection of functions from a set $X$ to $F$. </li>
<li><u>The collection</u> $A$ <strong>separates points</strong> if, for every pair of distinct points $x_1,x_2 ∈ X$, there is a function $f ∈ A$ such that $f(x_1)\neq f(x_2)$.</li>
</ul>
<p><strong>Real Polynomial Function</strong>:</p>
<ul>
<li>A <strong>real polynomial function</strong> $f : \mathbb{R}^n → \mathbb{R}$ is a finite linear combination of expressions of the form $x_1^{m_1}, x_2^{m_2},\dots, x_n^{m_n} $  where $m_1, m_2 ,\dots , m_n$ , are non-negative integers. </li>
<li>The coefficients of a polynomial may be taken from $\mathbb{Z}, \mathbb{Q}$, or $\mathbb{R}$. </li>
<li>The resulting set of polynomials is denoted by $\mathbb{Z}[x_1, . . . , x_n], \mathbb{Q}[x_1, . . . , x_n]$, and $\mathbb{R}[x_1, . . . , x_n]$, respectively.</li>
</ul>
<p><strong>Lattice</strong>:</p>
<ul>
<li>Let $V$ be a vector space of real valued continuous functions on a metric space $X$. </li>
<li>$V$ is a <strong>lattice</strong> if $|f|∈V$ whenever $f∈V$.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Theorem 3.4.23</strong> : </p>
<ul>
<li>The spaces $\mathbf{B}(T)$ is a complete metric space.</li>
</ul>
<p><strong>Proposition 7 / Theorem 3.5.26</strong> :  </p>
<ul>
<li>For any metric space $T$, $\mathbf{CB}(T)$ is a complete metric subspace of $\mathbf{B}(T )$.</li>
</ul>
<p><strong>Dini’s Theorem</strong>: </p>
<ul>
<li><p>Let $T$ be a compact metric space, $φ ∈ \mathbf{C}(T)$ and $(φ_m)$ a sequence in $\mathbf{C}(T)$. </p>
</li>
<li><p>If $φ_m \searrow φ$ (or $φ_m\nearrow φ$), then $φ_m →φ$ uniformly.</p>
</li>
</ul>
<p><strong>Lemma 3.8.11</strong>: </p>
<ul>
<li>Let $A$ be an algebra of real-valued, continuous functions on a compact metric space $X$. </li>
<li>Then, for $f ∈ A$, $|f|$  is in the uniform closure of $A$. That is, there is a sequence $(f_n)_{n∈\mathbb{N}}$ in $A$ such that $(f_n)_{n∈\mathbb{N}}$ converges uniformly to $|f|$.</li>
</ul>
<p><strong>Lemma 3.8.14</strong>:</p>
<ul>
<li>Let $X$ be a compact metric space and $L$ a lattice of continuous functions on $X$. </li>
<li>Suppose that, for any $x,y∈X$ with $x\neq y$ and $a,b∈\mathbb{R}$ , there is a function $f_{xy} ∈L$ satisfying $f_{xy}(x)=a$ and $f_{xy}(y) = b$. </li>
<li>Then, for each $f ∈ \mathbf{C}(X)$, there is a sequence $(f_n)_{n∈\mathbb{N}}$ in $L$ such that $(f_n)_{n∈\mathbb{N}}$ converges uniformly to $f$.</li>
</ul>
<p><strong>Theorem 3.8.8</strong>: (Stone) </p>
<ul>
<li>Let $X$ be a compact metric space. Let $A$ be an algebra of continuous, real valued functions on $X$, and suppose that $A$ separates points. </li>
<li>Then $\overline{A}$, the closure of $A$ in $\mathbf{C}(X,\mathbb{R})$ under the sup metric, sometimes called the uniform closure of $A$, either coincides with $\mathbf{C}(X,\mathbb{R})$ or with $\mathbf{C}x_0 (X, \mathbb{R}) = {f ∈ \mathbf{C}(X,\mathbb{R}) | f(x_0) = 0}$, for some point $x_0 ∈ X$.</li>
</ul>
<p><strong>The Stone-Weierstrass Theorem</strong>. </p>
<ul>
<li>Take any compact metric space $T$ , and let $\mathcal{P}$ be a subset of $\mathbf{C}(T)$ with the following properties:<ol>
<li>$αφ + βψ ∈ \mathcal{P}$ for all $φ, ψ ∈ \mathcal{P}$ and $α, β ∈ \mathbb{R} $;</li>
<li>$φψ ∈ \mathcal{P}$ for all $φ,ψ ∈ \mathcal{P}$;</li>
<li>all constant functions belong to $\mathcal{P}$;</li>
<li>for any distinct $x, y ∈ T$, there exists a $φ$ in $\mathcal{P}$ such that $φ(x) \neq φ(y)$.</li>
</ol>
</li>
<li>Then $\mathcal{P}$ is dense in $\mathbf{C}(T)$.</li>
</ul>
<blockquote>
<p>Above two thoermes are the same.</p>
</blockquote>
<p><strong>Theorem 3.8.7</strong>: (Weierstrass) </p>
<ul>
<li>Let $A$ be a compact set in $\mathbb{R}^n$. Then every continuous function $f : A → \mathbb{R}$ is the uniform limit of a sequence of real polynomials in $\mathbb{R}[x_1 , . . . , x_n ]$.<ul>
<li>Proof:</li>
<li>$\mathbb{R}[x_1 , . . . , x_n ]$ on $A$ is an algebra that separate points, by the Stone’s Theorem, $\overline{\mathbb{R}[x_1 , . . . , x_n ]}=\mathbf{C}(A)$.</li>
<li>This means that for any $f\in \mathbf{C}(A)$, either $f\in \mathbb{R}[x_1 , . . . , x_n ]$ or $f$ is an accumulation point of $\mathbb{R}[x_1 , . . . , x_n ]$.</li>
<li>If $f$ is an accumulation point of $\mathbb{R}[x_1 , . . . , x_n ]$, then there is a sequence $(f_n)\in\mathbb{R}[x_1 , . . . , x_n ]$ such that $\lim_{n\to\infty} f_n=f$ in the sup norm, which means that $(f_n)$ converges to $f$ uniformly.</li>
<li>If $f\in \mathbb{R}[x_1 , . . . , x_n ]$, set $f_n=f$</li>
</ul>
</li>
</ul>
<p><strong>Corollary 4, Remark 3.8.9</strong>: </p>
<ul>
<li>Let $T$ be a metric space. If $T$ is compact, then $\mathbf{C}(T )$ is separable, or $\mathbf{C}(T )$ is dense in $\mathbb{R}$.</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example 3.8.2, Exercise 3.8.3</strong>: </p>
<ul>
<li>$\mathbf{C}[0,1]$ separates $X=[0,1]$. Indeed, $f(x)=x$ is continuous and injective so $f(x_1)\neq f(x_2)$ for all $x_1\neq x_2$. </li>
<li>In fact, even the set of polynomial functions separates points.</li>
</ul>
<p><strong>Example 3.8.5</strong>:</p>
<ul>
<li>$f= 4x_1x_2^2+5x_4^6-x_1x_3^3 \in \mathbb{Z}[x_1 ,x_2 ,x_3 ,x_4 ]$ </li>
<li>$f= \pi x_1^2x_3^3-12x_2^6+\sqrt{2}x_1^2x_5 \in \mathbb{R}[x_1 ,x_2 ,x_3 ,x_4, x_5]$ </li>
<li>$f= \sqrt{2}x_1^3 x_2x_3^2x_4 +πx_1 x_2^5 x_4^{15} −11x_1 x_4 \in \mathbb{R}[x_1 ,x_2 ,x_3 ,x_4 ]$ </li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-27T09:45:29.000Z" title="9/27/2020, 5:45:29 PM">2020-09-27</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/">1.1.B Metric Spaces and Continuity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/27/Mathematics/Analysis/3%20Continuity/Continuity-and-Compactness/">Continuity and Compactness</a></h1><div class="content"><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 3</strong>: </p>
<ul>
<li><p>Let $X$ and $Y$ be two metric spaces, and $f ∈ Y^X$ a continuous function. </p>
</li>
<li><p>If $S$ is a compact subset of $X$, then $f(S)$ is a compact subset of $Y $.</p>
</li>
</ul>
<blockquote>
<p>Proposition 3 is a very important observation and has many implications. First of all, it gives us a useful sufficient condition for the inverse of an invertible continuous function to be continuous.</p>
</blockquote>
<p><strong>The Homeomorphism Theorem</strong>: </p>
<ul>
<li>If $X$ is a compact metric space and $f ∈ Y^X$ is a continuous bijection, then $f$ is a homeomorphism.</li>
</ul>
<p><strong>Corollary 2</strong>: </p>
<ul>
<li>Any continuous real function $φ$ defined on a compact metric space $X$ is bounded.</li>
</ul>
<p><strong>Local-to-global Method</strong>: </p>
<ul>
<li>For any given metric space $X$, suppose $Λ$ is a property (that may or may not be satisfied by the open subsets of $X$) such that</li>
<li>$(\vartriangle)$ Λ is satisfied by an open neighborhood of every point in $X$; and</li>
<li>$(\blacktriangledown) $If Λ is satisfied by the open subsets $O$ and $U$ of $X$, then it is also satisfied by $O ∪ U$.</li>
<li>Then, if $X$ is compact, it must possess the property $Λ$. </li>
</ul>
<blockquote>
<p>By compactness, we can deduce that $Λ$ is satisfied globally from the knowledge that it is satisfied locally.</p>
</blockquote>
<p><strong>Weierstrass’ Theorem</strong>: </p>
<ul>
<li>If $X$ is a compact metric space and $φ ∈ \mathbb{R}^X$ is a continuous function, then there exist $x, y ∈ X$ with $φ(x) = \sup φ(X)$ and $φ(y) = \inf φ(X)$.</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-27T09:45:15.000Z" title="9/27/2020, 5:45:15 PM">2020-09-27</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/">1.1.B Metric Spaces and Continuity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/27/Mathematics/Analysis/3%20Continuity/Continuity-and-Separability/">Continuity and Separability</a></h1><div class="content"><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Theorem 3.6.30</strong>:</p>
<ul>
<li>Suppose $X$ and $X’$ are metric spaces with $X$ separable. </li>
<li>Let $f : X → X’$ be a continuous surjection. Then $X’$ is separable.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-27T09:45:00.000Z" title="9/27/2020, 5:45:00 PM">2020-09-27</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/">1.1.B Metric Spaces and Continuity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/27/Mathematics/Analysis/3%20Continuity/Continuity-and-Connectedness/">Continuity and Connectedness</a></h1><div class="content"><h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Theorem 3.6.36, Proposition 2</strong>: </p>
<ul>
<li><p>Let $X$ and $Y$ be two metric spaces, and $f ∈ Y^X$ a continuous function. </p>
</li>
<li><p>If $X$ is connected, then $f(X)$ is a connected subset of $Y$.</p>
</li>
</ul>
<p><strong>Corollary 3.6.37</strong>: (The Intermediate Value Theorem)</p>
<ul>
<li><p>Let $X$ be a connected metric space and $φ ∈ R^X$ a continuous function. </p>
</li>
<li><p>If $φ(x) ≤ α ≤ φ(y)$ for some $x, y ∈ X$, then there exists a $z ∈ X$ such that $φ(z) = α$.</p>
</li>
</ul>
<p><strong>Corollary 1</strong>: </p>
<ul>
<li>Given any $−∞ &lt; a ≤ b &lt; ∞$, any continuous self-map $f$ on $[a, b]$ has a fixed point.</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example</strong>:</p>
<ul>
<li>Proposition 2 and/or the Intermediate Value Theorem can sometimes be used to prove that a function is not continuous. <ul>
<li>For instance, Proposition 2 yields readily that there does not exist a continuous function that maps $[0, 1]$ onto $[0, 1]\setminus{ \frac{1}{2} }$.</li>
</ul>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-27T09:44:39.000Z" title="9/27/2020, 5:44:39 PM">2020-09-27</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/">1.1.B Metric Spaces and Continuity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/27/Mathematics/Analysis/3%20Continuity/Continuity-of-Functions/">Continuity of Functions</a></h1><div class="content"><p>If $(X, d_X )$ and $(Y, d_Y )$ are two metric spaces, and $f ∈ Y^X$ is any function, then, for any $x ∈ X$, the statement “the images of points nearby $x$ under $f$ are close to $f(x)$” can be formalized as follows: </p>
<p>However small an $ε &gt; 0$ one picks, if $y$ is a point in $X$ which is sufficiently close to $x$ (closer than some $δ &gt; 0$), then the distance between $f(x)$ and $f(y)$ is bound to be smaller than $ε$. </p>
<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=531B93> Countinuous</font></strong>:</p>
<ul>
<li><p>Let $(X, d)$ and $(X’, d’)$ be metric spaces. </p>
</li>
<li><p>A function $f:X\to X’$ is <strong><font color=531B93>continuous</font></strong> <u>at the point</u> $x_0\in X$ if, for all $\varepsilon&gt;0$, there exists a $\delta&gt;0$ such that if $x \in X $ with $d(x,x_0) &lt;\delta$, then $d’(f(x),f(x_0))&lt;\varepsilon$.</p>
</li>
<li><p>A function $f:X\to X’$ is <strong><font color=531B93>continuous</font></strong> at the point $x_0\in X$ if, <u>given any sequence</u> $(x_n)_{n\in\mathbb{N}}$ in $X$, if $(x_n)_{n\in\mathbb{N}}$ converges to $x_0$, then $\lim_{n\to\infty}f(x_n)=f(x_0)$.</p>
</li>
<li><p>$f$ is <strong><font color=531B93>continuous</font></strong> <u>at $x$</u> if, for any $ε &gt; 0$, there exists a $δ &gt; 0$ such that<br>$$<br>f(N_{δ,X}(x)) ⊆ N_{ε,X’} (f(x)).<br>$$</p>
</li>
<li><p>For any nonempty $S ⊆ X$, we say that $f $ is <strong><font color=531B93>continuous</font></strong> on $S$, if it is continuous at every $x ∈ S$. </p>
</li>
</ul>
<blockquote>
<p>Above three definitions are equivalent</p>
</blockquote>
<blockquote>
<p>The continuity of a function that maps a metric space to another depends intrinsically on the involved metrics, because the continuity of a function is conditional on the distance functions used to metrize the domain and codomain of the function.</p>
</blockquote>
<p><strong>Uniformly Continuous</strong>: </p>
<ul>
<li>Let $X$ and $Y$ be two metric spaces. </li>
<li>A function $f ∈ Y^X$ is <strong>uniformly continuous</strong> if, for all $ε &gt; 0$, there exists a $δ &gt; 0$ such that if $x,y ∈ X$ with $d(x,y)&lt;\delta$, then $d’(f(x),f(y))&lt;\varepsilon$.</li>
<li>A function $f ∈ Y^X$ is <strong>uniformly continuous</strong> if, for all $ε &gt; 0$, there exists a $δ &gt; 0$ such that $f(N_{δ,X}(x)) ⊆ N_{ε,Y} (f(x))$ <u>for all</u> $x ∈ X$.</li>
</ul>
<blockquote>
<p>The notion of continuity is an inherently local one.</p>
<p>Uniform continuity says something about the behavior of $f$ on its entire domain, not only in certain neighborhoods of the points in its domain. </p>
</blockquote>
<blockquote>
<p>Obviously, a uniformly continuous function is continuous. On the other hand, a continuous function need not be uniformly continuous. </p>
</blockquote>
<p><strong>Homeomorphism</strong>: </p>
<ul>
<li>If $f ∈ Y^X$ is a bijection, and both $f$ and $f^{−1}$ are continuous, then it is called a **homeomorphism **between $X$ and $Y$.</li>
</ul>
<blockquote>
<p>If there exists such a bijection, then we say that $X$ and $Y$ are homeomorphic </p>
</blockquote>
<blockquote>
<p>Two homeomorphic spaces are indistinguishable from each other insofar as their neighborhood structures are concerned. </p>
<p>If $X$ and $ Y $ are homeomorphic, then corresponding to each open set $O$ in $X$, there is an open set $f(O)$ in $Y $, and conversely, corresponding to each open set $U$ in $Y$, there exists an open set $f^{−1}(U)$ in $X$. </p>
<p>Therefore, loosely speaking, $Y$ possesses any property that $X$ possesses so long as this property is defined in terms of open sets. </p>
<p>Such a property is called a topological property.</p>
</blockquote>
<blockquote>
<p>Formally, a property for metric spaces is referred to as a topological property if it is invariant under any homeomorphism, that is, whenever this property is true for $X$, it must also be true for any other metric space that is homeomorphic to $X$.</p>
<p>Connectedness is a topological property. The same goes for separability and compactness as well.</p>
<p>Neither completeness nor boundedness are preserved by a homeomorphism — these are not topological properties.</p>
<p>Thus, there are important senses in which two homeomorphic metric spaces may be of different character.</p>
</blockquote>
<p><strong>Isometry</strong>: </p>
<ul>
<li>$f ∈ Y^X$ is an <strong><font color=941100>isometry</font></strong> if $f$ is a homeomorphism that preserves the distance between any two points, that is, if<br>$$<br>d_Y (f(x),f(y)) = d(x,y)\ \ \ \mbox{for all} \ \ \ \ x,y ∈ X,<br>$$</li>
</ul>
<blockquote>
<p>Then we may conclude that the spaces $(X, d)$ and $(Y, d_Y )$ are indistinguishable as metric spaces — one is merely a relabelling of the other. </p>
<p>In this case, $X$ and $Y$ are said to be isometric.</p>
</blockquote>
<p><strong>Hölder Continuous</strong></p>
<ul>
<li>For any $α &gt; 0$, a function $f ∈ Y^X$ is said to be $α$-Hölder continuous, if there exists a $K &gt; 0$ such that</li>
</ul>
<p>$$<br>d_Y (f(x),f(y)) ≤ Kd(x,y)^α \ \ \ \ \ \mbox{for all}\ \ \ \  x,y ∈ X.<br>$$</p>
<ul>
<li>It is called **Hölder continuous **if it is $α$-Hölder continuous for some $α &gt; 0$.</li>
</ul>
<p><strong>Lipschitz Continuous, Lipschitz Constant</strong></p>
<ul>
<li><p><strong>Lipschitz continuous</strong> is $\alpha$-Hölder continuous with $\alpha=1$, that is, if there exists a $K &gt; 0$ such that<br>$$<br>d_Y (f(x),f(y)) ≤ Kd(x,y)\ \ \ \ \ \mbox{for all}\ \ \ \  x,y ∈ X.<br>$$</p>
</li>
<li><p>The smallest such $K$ is called the Lipschitz constant of $f$. </p>
</li>
</ul>
<p><strong>Contraction, Nonexpansive</strong>:</p>
<ul>
<li><p><strong>Contraction</strong> (or a contractive map) is Lipschitz Continuous with $0 &lt; K &lt; 1$ , that is, if there exists a $0&lt;K&lt;1$ such that<br>$$<br>d_Y (f(x),f(y)) ≤ Kd(x,y)\ \ \ \ \ \ \mbox{for all}\ \ \ \ x,y ∈ X,<br>$$</p>
</li>
<li><p>and <strong>nonexpansive</strong>if<br>$$<br>d_Y (f(x),f(y)) ≤ d(x,y)\ \ \ \ \  \mbox{for all}\ \ \ \  x,y ∈ X.<br>$$</p>
</li>
</ul>
<blockquote>
<p>The latter two definitions generalize the corresponding contraction which applied only to self-maps.</p>
</blockquote>
<blockquote>
<p>For a self-map$f$ on $\mathbb{R}$, $f$ is Lipschitz continuous if its derivative is bounded, </p>
<p>It is nonexpansive if $\sup{|f’(t)| : t ∈ \mathbb{R} } ≤ 1$</p>
<p>It is a contraction if $\sup{|f’(t)| : t ∈ \mathbb{R} } ≤ K &lt; 1$ for some real number $K$.<br>$$<br>\begin{aligned}<br>\mbox{contraction}&amp;\Rightarrow\mbox{nonexpansiveness}<br>\Rightarrow\mbox{Lipschitz continuity}<br>\Rightarrow\mbox{Hölder continuity}<br>\Rightarrow\mbox{uniform continuity}<br>\Rightarrow\mbox{continuity}<br>\end{aligned}<br>$$</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Theorem 3.5.5, Proposition 1</strong>: </p>
<ul>
<li>For any metric spaces $X$ and $Y$ , and $ f ∈ Y^X$ , the following statements are equivalent:<ol>
<li>$f$ is continuous;</li>
<li>For every open subset $O$ of $Y$, the set $f^{−1}(O)$ is open in $X$;</li>
<li>For every closed subset $S$ of $Y$, the set $f^{−1}(S)$ is closed in $X$;</li>
<li>For any $x∈X$ and $(x_m )∈X ,x^m→x$ implies $f(x^m )→f(x)$.</li>
</ol>
</li>
</ul>
<p><strong>Corollary 3.5.6</strong>: </p>
<ul>
<li>Suppose that $(X,d),\ (X’,d’),$ and $(X’’,d’’)$ are metric spaces, and $f : X → X’$ and $g:X’→X’’$ are continuous. Then $g◦f:X→X’’$ is continuous.</li>
</ul>
<p><strong>Theorem 3.5.16</strong>: </p>
<ul>
<li>Suppose $1 ≤ p &lt; q ≤ ∞$. Then the identity map $I(x) = x$ from $\ell^p_n(\mathbb{R})$ to $\ell^q_n(\mathbb{R})$ is a homeomorphism.</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example</strong>: </p>
<ul>
<li>Given two metric spaces $X$ and $Y$, if $f ∈ Y^X$ is continuous and $S$ is a metric subspace of $X$, then $f|_S$ is a continuous function.</li>
</ul>
<p><strong>Example 1</strong>:</p>
<ul>
<li>The identity function $id_X$ on a metric space $X$ is continuous</li>
<li> A constant function on any metric space is continuous.</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>Any function defined on a discrete space is continuous.</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>Any polynomial function on $\mathbb{R}$ with the usual metric is continuous.</li>
<li>For every $n ∈ \mathbb{N}$, any concave or convex function defined on an open subset of $\mathbb{R}^n$ is continuous.</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>The function $\varphi : \ell^∞ → \mathbb{R}_+$ defined by $\varphi((x_m)) := \sup{|x_m| : m ∈ \mathbb{N}}$ is continuous.</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>The function $L ∈ \mathbb{R}^{\mathbf{C}[0,1]}$ defined by $L(f):=\int^1_0f(t)dt$ is continuous.</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>The continuous function $f : \mathbb{R}_{++} →\mathbb{R}_{++} $ defined by $f(x) := \frac{1}{x}$ is continuous but not uniform continuous.</li>
<li>For the continuous function $f : \mathbb{R} →\mathbb{R} $. If $f’(x)$ exists, and it is bounded, then $f(x)$ is uniform continuous.</li>
<li>$f(x) = \sin(x)$ is uniformly continuous on $\mathbb{R}$.</li>
<li>A polynomial function $p(x)$ on $\mathbb{R}$ is uniformly continuous iff $\deg(p(x)) &lt; 2$.</li>
<li>Any linear map from $\mathbb{R}^n$ to $\mathbb{R}^m$ is uniformly continuous.</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>$[0,1)$ is homeomorphic to $\mathbb{R}_+$. $x \mapsto \frac{x}{1-x}$ is a homeomorphism between $[0, 1)$ and $\mathbb{R}^+$.</li>
<li>$\mathbb{R} $ and $(\mathbb{R}, d’)$ are homeomorphic, where $d’(x, y) :=\frac{|x−y|}{1+|x-y|} $. The identity map is a homeomorphism.</li>
<li> $\mathbb{R}^{n,p}$ and $\mathbb{R}^{n,q}$ are homeomorphic for any $n ∈ \mathbb{N} $ and $1 ≤ p, q ≤ ∞$.</li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>$\overline{\mathbb{R}}$ and $[−1, 1]$ are isometric </li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>$f(x)=\frac{1}{2}(x+\frac{2}{x})$, $x\in[1,\infty)$ is a contraction. <ul>
<li>Consider its derivative, which is always less than 1.</li>
</ul>
</li>
</ul>
<p><strong>Exercise 3.7.3</strong>:</p>
<ul>
<li>Let $f : \mathbb{R} → \mathbb{R}$ be a polynomial function. $f$ is a contraction if $0&lt;|a_1|&lt;1$.<ul>
<li>Consider the derivative, which is always less than 1 if $0&lt;|a_1|&lt;1$.</li>
</ul>
</li>
</ul>
<p><strong>Exercise 3.7.2, Exercise 3.7.9</strong>:</p>
<ul>
<li>A contraction mapping is continuous.<ul>
<li>Proof:</li>
<li>Let $X$ be a metric space and $f$ a map from $X$ to $X$ and  $d(f(x),f(y)) ≤ αd(x,y)$, $x,y\in X$, $0 &lt; α &lt; 1$.</li>
<li>For any $\varepsilon&gt;0$, for any $x_0,x\in X$, let $\delta=\frac{\varepsilon}{\alpha}$, if $d(x_0,x)&lt;\delta$, then $d(f(x_0),f(x))\leq\alpha d(x_0,x)&lt;\alpha\frac{\varepsilon}{\alpha}=\varepsilon$.</li>
<li>Thus any contraction mapping is continuous.</li>
</ul>
</li>
<li>A Lipschitz continuious function is continuous.<ul>
<li>Proof:</li>
<li>Let $X$ be a metric space and $f$ a map from $X$ to $X$ and  $d(f(x),f(y)) ≤ Cd(x,y)$, $x,y\in X$.</li>
<li>For any $\varepsilon&gt;0$, for any $x_0,x\in X$, let $\delta=\frac{\varepsilon}{C}$, if $d(x_0,x)&lt;\delta$, then $d(f(x_0),f(x))\leq C d(x_0,x)&lt;C\frac{\varepsilon}{C}=\varepsilon$.</li>
<li>Thus any Lipschitz continuous function is continuous.</li>
</ul>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-18T09:45:44.000Z" title="9/18/2020, 5:45:44 PM">2020-09-18</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/">1.1.B Metric Spaces and Continuity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/18/Mathematics/Analysis/2%20Metric%20Spaces/The-Completion-of-Metric-Spaces/">The Completion of Metric Spaces</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p> <strong>$\tilde{X}$, Equivalence Class in $X’$</strong>:</p>
<ul>
<li>Let $X’$ be the set of all Cauchy sequences in $X$. </li>
<li>We define an <strong>equivalence relation</strong> on $X’$ by saying that $(x_n)\sim(y_n)$ if $\lim_{n\to\infty} d(x_n,y_n)=0$</li>
<li>Let $\tilde{X}$ denote the set of <strong>equivalence classes</strong> of $X’$<ul>
<li>The equivalent class of $(x_k)_{k∈\mathbb{N}}$ is denoted by $[x_k]$.</li>
</ul>
</li>
</ul>
<p><strong>$\tilde{d}$, Metric on</strong> $\tilde{X}$:</p>
<ul>
<li><p>Let $[x_n],[y_n]$ be in $\tilde{X}$. Note that $(d(x_n,y_n))_{n\in\mathbb{N}}$ is a Cauchy sequence in $\mathbb{R}$. </p>
<ul>
<li><p>Indeed, let $\varepsilon&gt;0$ be give, then there is $N\in\mathbb{N}$ such that , for all $m,n\geq N$, we have $d(x_n,x_m),d(y_n,y_m)&lt;\frac{\varepsilon}{2}$.</p>
</li>
<li><p>Then<br>$$<br>\begin{aligned}<br>|d(x_n,y_n)-d(x_m,y_m)|&amp;=|d(x_n,y_n)-d(x_n,y_m)|+|d(x_n,y_m)-d(x_m,y_m)|\<br>&amp;\leq d(y_n,y_m)+d(x_n,x_m)\<br>&amp;\leq \varepsilon<br>\end{aligned}<br>$$</p>
</li>
</ul>
</li>
<li><p>Since $\mathbb{R}$ is complete, there is $d\geq0$ such that $\lim_{n\to\infty}d(x_n,y_n)=d$</p>
</li>
<li><p>We define $\tilde{d}([x_n],[y_n])=d$ as a <strong>metric</strong> on $\tilde{X}$</p>
</li>
</ul>
<p><strong>The Completion of $X$</strong>:</p>
<ul>
<li>We call $\tilde{X}$ the <strong>completion</strong> <strong>of $X$.</strong></li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Theorem 3.9.1</strong>:</p>
<ul>
<li>Let $(X,d)$ be a metric space. Then there exists a complete metric space $(X,d)$, and an injection $φ : X → \tilde{X}$, such that<ol>
<li>$φ:X→φ(X)$ is an isometry, and </li>
<li>$φ(X)$ is dense in $\tilde{X}$.</li>
</ol>
</li>
</ul>
<blockquote>
<p>Define $φ:X→\tilde{X}$ by $φ(x)=[x_n]$ where $x_n=x$ for all $n\in\mathbb{N}$</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-18T09:44:23.000Z" title="9/18/2020, 5:44:23 PM">2020-09-18</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/">1.1.B Metric Spaces and Continuity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/18/Mathematics/Analysis/2%20Metric%20Spaces/The-Contraction-Mapping-Theorem/">The Contraction Mapping Theorem</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Contraction</strong>: </p>
<ul>
<li>Let $X$ be any metric space. </li>
<li>A self-map $Φ$ on $X$ is said to be a <strong>contraction</strong> if there exists a real number $0 &lt; K &lt; 1$ such that</li>
</ul>
<p>$$<br>d(Φ(x), Φ(y)) ≤ Kd(x, y)\ \ \ \ \ \mbox{for all}\ \ \  x, y ∈ X.<br>$$</p>
<ul>
<li>The infimum of the set of all such $K$ is called the <strong>contraction coefficient</strong> of $Φ$.</li>
</ul>
<blockquote>
<p>In general, it may be quite difficult to check whether or not a given self-map is contractive. </p>
<p>In most applications, however, one works with self-maps that are defined on relatively well-behaved metric spaces. </p>
</blockquote>
<p><strong>$m$th Iteration</strong>: </p>
<ul>
<li><p>Let $X$ be a nonempty set, and $Φ$ a self-map on $X$. </p>
</li>
<li><p>Let $Φ^1 := Φ$, and define $Φ^{m+1} :=Φ◦Φ^m$ for any $m=1,2,…. $ </p>
</li>
<li><p>The self-map $Φ^m$ is called the <strong>$m$th iteration</strong> of $Φ$.</p>
</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Lemma 3</strong>: (Blackwell’s Contraction Lemma) </p>
<ul>
<li>Let $T$ be a nonempty set, and $X $ a nonempty subset of $\mathbf{B}(T )$ which is closed under addition by positive constant functions. Assume that $Φ$ is an increasing self-map on $X$. If there exists a $0&lt;\delta&lt;1$ such that</li>
</ul>
<p>$$<br>Φ(f + α) ≤ Φ(f) + δα\ \ \ \  \mbox{for all}\ \ \  (f, α) ∈ X × \mathbb{R}_+,<br>$$</p>
<ul>
<li>then $Φ$ is a contraction.</li>
</ul>
<p><strong>Theorem 3.7.7</strong>: (The Contraction Mapping Theorem / The Banach Fixed Point Theorem)</p>
<ul>
<li>Let $X$ be a complete metric space. </li>
<li>If $Φ ∈ X^X$ is a contraction, then there exists a unique $x^∗ ∈ X$ such that $Φ(x^∗) = x^∗$.</li>
</ul>
<p><strong>Corollary 2</strong>: </p>
<ul>
<li><p>Let $X$ be a complete metric space. </p>
</li>
<li><p>For any self-map $Φ$ on $X$ such that $Φ^m$ is a contraction for some $m ∈ \mathbb{N} $, there exists a unique $x^∗ ∈ X$ such that $Φ(x^∗) = x^∗$.</p>
</li>
</ul>
<p><strong>The Generalized Banach Fixed Point Theorem 1</strong>. (Merryfield-Stein Jr.) </p>
<ul>
<li><p>Let $X$ be a complete metric space, and $Φ$ a self-map on $X$. </p>
</li>
<li><p>If there exist a $(K,M) ∈ (0, 1) × \mathbb{N} $ such that</p>
</li>
</ul>
<p>$$<br>\min{d(Φ^m(x),Φ^m(y)) : m = 1,…,M} ≤ Kd(x,y)\ \ \mbox{for all}\ \  x,y ∈ X,<br>$$</p>
<ul>
<li>then there exists an $x ∈X$ such that $Φ(x)=x$.</li>
</ul>
<p><strong>The Generalized Banach Fixed Point Theorem 2</strong>: (Matkowski) </p>
<ul>
<li>Let $X$ be a complete metric space, and $f : \mathbb{R}_+ → \mathbb{R}_+$ an increasing function with $\lim f^m(t) = 0$ for all $t≥0$. If $Φ∈X^X$ satisfies</li>
</ul>
<p>$$<br>d(Φ(x), Φ(y)) ≤ f (d(x, y))\ \ \ \  \mbox{for all}\ \ \  x, y ∈ X,<br>$$</p>
<ul>
<li>then there exists a unique $x ∈X$ such that $Φ(x)=x$.</li>
</ul>
<p><strong>The Banach Fixed Point Property</strong>: </p>
<ul>
<li>Every contraction $Φ ∈ S^S$, where $S$ is a nonempty closed subset of $X$, has a fixed point.</li>
</ul>
<blockquote>
<p>A metric space is complete iff it has the Banach Fixed Point Property.</p>
</blockquote>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Picard’s Existence Theorem</strong>: (The Local Version) </p>
<ul>
<li>Let $−∞ &lt; a &lt; b &lt; ∞$, and $(x_0,y_0) ∈ (a,b)^2$ . If $H : [a,b]^2→ \mathbb{R} $ is continuous, and </li>
</ul>
<p>$$<br>|H(x,y)−H(x,z)|≤α|y−z|,\ \ \ \  a≤x,y,z≤b,<br>$$</p>
<ul>
<li>for some $α &gt; 0$, then there is a $δ &gt; 0$ such that there exists a unique differentiable real function $f$ on $[x_0 − δ, x_0 + δ]$ with $y_0 = f(x_0)$ and</li>
</ul>
<p>$$<br>f’(x)=H(x,f(x)),\ \ \  x_0−δ≤x≤x_0+δ.<br>$$</p>
<p><strong>Picard’s Existence Theorem</strong>: (The Global Version) </p>
<ul>
<li>Let $−∞ &lt; a &lt; b &lt; ∞$, and $(x_0,y_0)∈[a,b]×\mathbb{R} $. If $H:[a,b]×\mathbb{R} →\mathbb{R} $ is continuous,and</li>
</ul>
<p>$$<br>|H(x,y)−H(x,z)|≤α|y−z|,\ \ \ \ \ \ \  a≤x≤b,\ \ \ \ \  −∞&lt;y,z&lt;∞,<br>$$</p>
<ul>
<li>for some $α &gt; 0$, then there exists a unique differentiable real function $f$ on $[a, b]$ with $y_0 = f(x_0)$ and<br>$$<br>f’(x)=H(x,f(x)),\ \ \  a≤x≤b.<br>$$</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-18T09:44:05.000Z" title="9/18/2020, 5:44:05 PM">2020-09-18</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/">1.1.B Metric Spaces and Continuity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/18/Mathematics/Analysis/2%20Metric%20Spaces/Completeness/">Completeness</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=941751>Complete</font></strong>: </p>
<ul>
<li>A metric space $X$ is said to be <strong><font color=941751>complete</font></strong> if <u>every Cauchy sequence</u> in $X$ converges to <u>a point in $X$.</u></li>
</ul>
<blockquote>
<p>Completeness is a stronger condition than closedness in a metric subspace.</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 6</strong>: </p>
<ul>
<li>Let $(x^m)$ be a sequence in a metric space $X$.<ol>
<li>If $(x^m)$ is convergent, then it is Cauchy.</li>
<li>If $(x^m)$ is Cauchy, then ${x_1,x_2,…}$ is bounded, but $(x_m)$ need not converge in $X$.</li>
<li>If $(x^m)$ is Cauchy and has a subsequence that converges in $X$, then it converges in $X$ as well.</li>
</ol>
</li>
</ul>
<p><strong>Proposition 7</strong>: </p>
<ul>
<li>Let $X$ be a metric space, and $Y$ a metric subspace of $X$. <ul>
<li>If $Y$ is complete, then it is closed in $X$.</li>
<li>If $Y$ is closed in $X$ and $X$ is complete, then $Y$ is complete.</li>
</ul>
</li>
</ul>
<p><strong>Corollary 1</strong>: </p>
<ul>
<li>A metric subspace of a complete metric space $X$ is complete iff, it is closed in $X$.</li>
</ul>
<p>**Theorem 1.1.6, Theorem 1.1.8 **: (The Least Upper Bound Property in $\mathbb{R}$, The Completeness Axiom)</p>
<ul>
<li>Every nonempty subset in $\mathbb{R}$ that is bounded above has a least upper bound.</li>
</ul>
<blockquote>
<p>This can be viewed as a definition or an axiom.</p>
</blockquote>
<p><strong>Lemma 1.6.13</strong>: </p>
<ul>
<li>Every sequence in $\mathbb{R}$ has a monotonic subsequence.</li>
</ul>
<blockquote>
<p>Due to the infinitiness of sequences.</p>
</blockquote>
<p><strong>Lemma 1.6.14</strong>: (Monotone Convergence Theorem in $\mathbb{R}$)</p>
<ul>
<li>Every bounded monotonic sequence in $\mathbb{R}$ converges to an element in $\mathbb{R}$.</li>
</ul>
<p><strong>The Cantor-Fréchet Intersection Theorem</strong>: </p>
<ul>
<li>A metric space $X$ is complete iff, for any sequence $(S_m)$ of nonempty closed subsets of $X$ with</li>
</ul>
<p>$$<br>S_1 ⊇S_2 ⊇··· \mbox{and}\ \mbox{diam}(S_m)→0,<br>$$</p>
<ul>
<li>we have $\cap^∞S_i \neq∅$ (so that $|\cap^∞S_i|=1$).</li>
</ul>
<p><strong>Lemma 1.6.15, Theorem 1.6.31:</strong> (Bolzano–Weierstrass Theorem in $\mathbb{R}$)</p>
<ul>
<li>Every bounded sequence in $\mathbb{R}$ has a convergent subsequence.</li>
<li>Let $S$ be a bounded, infinite subset of $\mathbb{R}$. Then $S$ has an accumulation point in $\mathbb{R}$.</li>
</ul>
<blockquote>
<p>Above two statements are equivalent, due to the definition of accumulation point.</p>
<p>Note that only infinite sets have accumulation points. Finite sets can only consist of isolated points.</p>
</blockquote>
<p><strong>Lemma 3.4.12, Theorem 3.4.14</strong>: (Bolzano–Weierstrass Theorem in $\mathbb{R}^n$)</p>
<ul>
<li><p>Every bounded sequence in $\mathbb{R}^n$ with the usual metric has a convergent subsequence.</p>
</li>
<li><p>Let $S$ be a bounded, infinite subset of $\mathbb{R}^n$. Then $S$ has an accumulation point in $\mathbb{R}^n$.</p>
</li>
</ul>
<blockquote>
<p>Above two statements are equivalent, due to the definition of accumulation point.</p>
<p>Note that only infinite sets have accumulation points. Finite sets can only consist of isolated points.</p>
</blockquote>
<p><strong>Theorem 1.6.24</strong>: (Cauchy Criterion in $\mathbb{R}$)</p>
<p>A sequence in $\mathbb{R}$ is convergent iff it is Cauchy.</p>
<p><strong>Theorem 3</strong>: </p>
<ul>
<li>A metric space is compact iff, it is complete and totally bounded.</li>
</ul>
<blockquote>
<p>A closed subset of a complete metric space is compact iff it is totally bounded.</p>
<p>A subset $S$ of a metric space $X$ is totally bounded iff every sequence in $S$ has a Cauchy subsequence.</p>
</blockquote>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Example</strong>:</p>
<ul>
<li><p>$(0,1]$ is not complete as a metric subspace of $\mathbb{R} $, but would be complete with the discrete metric.</p>
</li>
<li><p>$\mathbb{Q} $ is an incomplete metric space as a metric subspace of $\mathbb{R} $. </p>
</li>
<li><p>$\mathbb{R} $ is complete but not compact. </p>
<ul>
<li>In fact, even a complete and bounded metric space need not be compact, the gap between completeness and compactness disappears if we strengthen the boundedness hypothesis here to total boundedness.</li>
</ul>
</li>
<li><p>$\mathbb{R}^{n,p},\ \ell^p$ is complete.</p>
</li>
<li><p>$\mathbf{B}(T)$ is complete for any nonempty set $T$.</p>
</li>
<li><p>$\mathbf{C}[0, 1]$ is a closed subset of $\mathbf{B}[0, 1]$, so it is complete.</p>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-18T09:43:55.000Z" title="9/18/2020, 5:43:55 PM">2020-09-18</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/">1.1.B Metric Spaces and Continuity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/18/Mathematics/Analysis/2%20Metric%20Spaces/Compactness/">Compactness</a></h1><div class="content"><p>The power of <strong><font color=941751>compactness</font></strong>: providing a finite structure for infinite sets. </p>
<p>In many problems where finiteness makes life easier (such as in optimization problems), compactness does the same thing.</p>
<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=F27200>Cover, Open Cover</font></strong>: </p>
<ul>
<li><p>Let $X$ be a metric space and $S ⊆ X$. </p>
</li>
<li><p>A <u>class</u> $\mathcal{O}$ of subsets of $X$ is said to <strong><font color=F27200>cover</font></strong> $S$ if $S⊆\cup\mathcal{O}$. </p>
</li>
<li><p>If all <u>members</u> of $\mathcal{O}$ are open in $X$, then we say that $\mathcal{O}$ is an <strong><font color=F27200>open cover</font></strong> of $S$.</p>
</li>
</ul>
<p><strong><font color=941751>Compact</font></strong>: </p>
<ul>
<li>A metric space $X$ is said to be <font color=941751><strong>compact</strong></font> if <u>every open cover</u> of $X$ has a finite subset that also covers $X$. </li>
<li>A subset $S$ of $X$ is said to be <font color=941751><strong>compact</strong></font> in $X$ if <u>every open cover</u> of $S$ has a finite subset that also covers $S$.</li>
</ul>
<blockquote>
<p>If $S$ is a compact subset of a metric space $Y$, and $Y$ is a metric subspace of $X$, then $S$ is compact in $X$.</p>
<p>$(0, 1)$ is not a compact subset of $\mathbb{R} $ (or that $(0, 1)$ is not a compact metric space).</p>
<p>A finite subset of any metric space is necessarily compact. Much less trivially, $[0,1]$ is a compact subset of $\mathbb{R} $.</p>
</blockquote>
<p><strong>Sequentially Compact</strong>: </p>
<ul>
<li>A subset $S$ of a metric space $X$ is <strong>sequentially compact</strong> iff, <u>every sequence</u> in $S$ has a subsequence that converges to a point in $S$.</li>
</ul>
<blockquote>
<p>Since the closedness property can be characterized in terms of convergent se- quences (Proposition 1). </p>
<p>Given that every compact space is closed, it makes sense to ask if it is possible to characterize compactness in the same terms as well. So it comes to the sequentially compactness.</p>
<p>Sequentially compactness is a stronger condition than closedness. If a subset is sequentially compact, then it is closed.</p>
<p>Sequentially compactness is a stronger condition than completeness. If a subset is sequentially compact, then it is complete.</p>
</blockquote>
<p><strong>Totally Bounded, Precompact</strong>: </p>
<ul>
<li>A set $S$ in a metric space $X$ <strong>totally bounded</strong> (or <strong>precompact</strong>) if, <u>for any</u> $ε &gt; 0$, there exists a finite subset $T$ of $S$ such that $S ⊆ {N_{ε,X}(x) : x ∈ T}$.</li>
</ul>
<blockquote>
<p>This property is strictly more demanding than boundedness.</p>
<p>Every totally bounded subset of a metric space is bounded.</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 4</strong>: </p>
<p>Any closed subset of a compact metric space $X$ is compact.</p>
<p><strong>Theorem 3.6.4 / Proposition 5</strong>: </p>
<p>Any compact subset of a metric space $X$ is closed and bounded.</p>
<blockquote>
<p>Compactness is, in general, a stronger property than closedness and boundedness put together; it often introduces significantly more structure to the analysis. </p>
</blockquote>
<p><strong>Corollary 3.6.5</strong> </p>
<p>If $A$ is a compact set in a metric space $X$, then every infinite subset of $A$ has an accumulation point in $A$.</p>
<p><strong>Corollary 3.6.6</strong> </p>
<p>Let $A$ be a compact set in a metric space. Then, every infinite sequence in $A$ has a subsequence that converges to a point in $A$.</p>
<blockquote>
<p>Corollary 3.6.5 is equivalent to corollary 3.6.6.</p>
</blockquote>
<p><strong>Lemma 1</strong>: </p>
<ul>
<li>Every sequentially compact subset of a metric space $X$ is totally bounded.</li>
</ul>
<p><strong>Lemma 3.6.16</strong>: </p>
<p>Let $X$ be a metric space. If $A ⊂ X$ has the property that every infinite subset of $A$ has an accumulation point in $X$, then there exists a countable collection of open sets ${U_i | i ∈ \mathbb{N}}$ such that, if $V$ is any open set in $X$ and $x ∈ A ∩ V$ , then there is some $U_i$ such that $x ∈ U_i ⊂ V$ .</p>
<p><strong>Lemma 2</strong>: </p>
<ul>
<li>Let $X$ be a metric space. If $A ⊂ X$ has the property that every infinite subset of $A$ has an accumulation point in $X$, then for every open cover of $A$, there is a finite subcover.</li>
<li>Let $S$ be a sequentially compact subset of a metric space $X$, and $\mathcal{O}$ an open cover of $S$. Then there exists an $ε&gt;0$ such that, for any $x∈S$, we can find an $O_x ∈\mathcal{O}$ with $N_{ε,X}(x) ⊆ O_x$.</li>
</ul>
<p><strong>Theorem 3.6.19 / Theorem 2</strong>: </p>
<p>A subset $S$ of a metric space $X$ is compact iff it is sequentially compact.</p>
<blockquote>
<p>The properties of compactness and sequential compactness are equivalent for any given metric space. </p>
</blockquote>
<p><strong>The Heine-Borel Theorem</strong>: </p>
<p>For any $−∞ &lt; a &lt; b &lt; ∞$, the $n$-dimensional cube $[a,b]^n$ is compact.</p>
<p><strong>Theorem 3.6.21 / Theorem 1</strong>: </p>
<p>Given any $n ∈ \mathbb{N} $, a subset of $\mathbb{R}^n$ is compact iff, it is closed and bounded.</p>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Exercise 3.6.8</strong>:</p>
<ul>
<li>A finite subset of any metric space is necessarily compact. </li>
</ul>
<p><strong>Example</strong>:</p>
<ul>
<li>$(0, 1)$ is not a compact subset of $\mathbb{R} $. </li>
<li>$(0, 1)$ is not a compact metric space.</li>
<li>$[0,1]$ is a compact subset of $\mathbb{R} $.</li>
<li>$\mathbb{R} $ is obviously not compact.</li>
<li>$\overline{\mathbb{R}}$ is sequentially compact, and hence, compact.</li>
</ul>
<p><strong>Example:</strong></p>
<ul>
<li>The empty set is compact in any metric space.</li>
</ul>
<p><strong>Exercise 3.6.8</strong>:</p>
<ul>
<li>The intersection of two compact sets is compact.</li>
<li>A finite union of compact sets is compact.</li>
<li>A countable union of compact sets does not need to be compact.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-18T09:43:46.000Z" title="9/18/2020, 5:43:46 PM">2020-09-18</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/">1.1.B Metric Spaces and Continuity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/18/Mathematics/Analysis/2%20Metric%20Spaces/Separability/">Separability</a></h1><div class="content"><p><strong><font color=941751>Separability</font></strong>, identifies those metric spaces that have relatively “few” open sets.</p>
<p>Intuitively speaking, one may think of a separable metric space as a space which is “not very large.” After all, in such a space, there is a countable set which is “almost” equal to the entire space.</p>
<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=F27200>Dense</font>, <font color=941751>Separable</font></strong>: </p>
<ul>
<li>Let $X$ be a metric space and $Y ⊆ X$.  </li>
<li>If $cl_X(Y) = X$, then $Y$ is said to be <strong><font color=F27200>dense</font></strong> in $X$  .</li>
<li>$X$ is said to be <font color=941751><strong>separable</strong></font> if it contains <u>a countable dense set</u>.</li>
</ul>
<blockquote>
<p>A set $Y ⊆ X$ is dense in a metric space $X$ iff, any point in $X$ can be approached by means of a sequence that is contained entirely in $Y$. </p>
<p>So, $X$ is a separable metric space iff, it contains a countable set $Y$ such that $x ∈ X$ iff there exists a $(y_m) ∈ Y^∞$ with $y_m → x$</p>
<p>This is also equivelant to say $Y\subseteq X$ and $X$ contains all the accumulation points of $Y$.</p>
</blockquote>
<blockquote>
<p>In any metric space $X$, $X$ is dense in $X$.</p>
</blockquote>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Example 6</strong>: </p>
<ul>
<li>Any metric subspace of a separable metric space is separable.</li>
</ul>
<p><strong>Theorem 3.6.29</strong>: </p>
<ul>
<li>If $(X, d)$ is a compact metric space, then $X$ is separable.<ul>
<li>Proof: </li>
<li>For each $n ∈ \mathbb{N}$, consider the collection of open balls $\mathcal{O}={B_{\frac{1}{n}}(x) | x ∈ X}$. This is an open covering of $X$, and hence, there is a finite subcovering $\mathcal{U}$. </li>
<li>Set $X_n={x\in X:B_{\frac{1}{n}}(x)\in \mathcal{U}}$, and define $S=\cup_{n\in\mathbb{N}}X_n$, then we claim that $S$ is dense in $X$.</li>
<li>Since $\overline{S}\subseteq X$, we need to prove $X\subseteq \overline{S}$.</li>
<li>Take any $x\in X,\ x\notin S$, there is $y\in S$ such that $x\in B_r(y)$. </li>
<li>Prove that $x$ is the accumulation point of $S$ by showing that $y\in {B_r(y)\setminus {y}}\cap S$.</li>
</ul>
</li>
</ul>
<p><strong>The Weierstrass Approximation Theorem</strong>: </p>
<ul>
<li>The set of all polynomials defined on $[a, b]$ is dense in $\mathbf{C}[a, b]$. That is,<br>$$<br>cl_{\mathbf{C}[a,b]}(\mathbf{P}[a, b]) = \mathbf{C}[a, b].<br>$$</li>
</ul>
<p><strong>Proposition 3</strong>: </p>
<ul>
<li>Let $X$ be metric space. If $X$ is separable, then there exists a countable class $\mathcal{O}$ of open subsets $X$ such that</li>
</ul>
<p>$$<br>U= {O∈\mathcal{O}:O⊆U}<br>$$</p>
<ul>
<li>for any open subset $U$ of $X$</li>
</ul>
<blockquote>
<p>One major reason for why separable metric spaces areseful is that in such spaces all open sets can be described in terms of a countable set of open sets.</p>
<p>Keep in mind that many interesting properties of a metric space are defined through the notion of an open set. </p>
<p>For instance, if we know all the open subsets of a metric space, then we know all the closed and/or connected sets in this space.  </p>
<p>Similarly, if we know all the open subsets of a metric space, then we know which sequences in this space are convergent and which ones are not. </p>
<p>Consequently, a lot can be learned about the general structure of a metric space by studying the class of all open subsets of this space. But the significance of this observation would be limited, if, in a sense, we had “too many” open sets lying around. </p>
<p>In the case of a separable metric space this is not the case, for such a space has only a countable number of open sets “that matter” in the sense that all other open subsets of this space can be described using only these (countably many) open sets. </p>
<p>This is the gist of Proposition 3.</p>
</blockquote>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Exercise 3.6.26, Example 6</strong>: </p>
<ul>
<li>The only dense subset of $X$ is $X$ itself iff, $X$ is a discrete metric space. </li>
<li>A discrete space is separable iff it is countable.</li>
</ul>
<p><strong>Example 3.6.25, Example 6</strong>:</p>
<ul>
<li>$\mathbb{R}^{n,p}$ is separable for any $1 ≤ p ≤ ∞$.<ul>
<li>$\mathbb{Q}^n$ is dense in $\mathbb{R}^n$ in the usual metric.</li>
<li>The “dyadic numbers,” that is, the set $D = {\frac{a}{2^n}∈\mathbb{Q}|a,n ∈ \mathbb{Z}}$, are also dense in $\mathbb{R}$ in the usual metric.</li>
</ul>
</li>
</ul>
<p><strong>Example 6</strong>: </p>
<ul>
<li>$\ell_p$ is separable for any $1≤p&lt;∞$.</li>
</ul>
<p><strong>Example 6</strong>: </p>
<ul>
<li>$\mathbf{C}[a, b]$ is separable for any $−∞ &lt; a ≤ b &lt; ∞$.<ul>
<li>A corollary of the Weierstrass Approximation Theorem</li>
</ul>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-18T09:43:34.000Z" title="9/18/2020, 5:43:34 PM">2020-09-18</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/">1.1.B Metric Spaces and Continuity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/18/Mathematics/Analysis/2%20Metric%20Spaces/Connectedness/">Connectedness</a></h1><div class="content"><p><strong><font color=941751>Connectedness</font>,</strong> gives one a glimpse of how one would study the geometry of an arbitrary metric space.</p>
<p>Intuitively speaking, a connected subset of a metric space is one that cannot be partitioned into two (or more) separate pieces, it is rather in one whole piece. </p>
<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=941751>Connected</font></strong>:</p>
<ul>
<li>A metric space $X$ is <strong><font color=941751>connected</font></strong> if there do not exist two <u>nonempty</u> and <u>disjoint open</u> subsets $O$ and $U$ of $X$ such that $ O ∪ U = X$. </li>
<li>A subset $S ⊆ X$ is <strong><font color=941751>connected</font></strong> in $X$ iff it cannot be written as a <u>disjoint</u> union of two <u>nonempty</u> sets that are <u>open</u> in $S$.</li>
</ul>
<p><strong>Disconnected</strong>:</p>
<ul>
<li>Let $X$ be a metric space and $A ⊂ X$. </li>
<li>$A$ is <strong>disconnected</strong> if there exist open sets $U, V ⊂ X$ such that<ul>
<li> $ U∩A\neq∅$ and $V ∩A\neq∅$</li>
<li>$ (U ∩ A) ∩ (V ∩ A) = ∅$</li>
<li>$A = (U ∩ A) ∪ (V ∩ A)$</li>
</ul>
</li>
<li>Let $X$ be a metric space and $A ⊂ X$. $A$ is <strong><font color=941751>connected</font></strong> if $A$ is not disconnected.</li>
</ul>
<blockquote>
<p>Above definitions are equivalent to each other.</p>
</blockquote>
<blockquote>
<p>To show disconnected, find a good pair of sets. </p>
<p>To show connected, check every potential pair of sets.</p>
</blockquote>
<p><strong>Connected Component, Totally Disconnected</strong>:</p>
<ul>
<li><p>If $X$ is a metric space and $x_0\in X$</p>
</li>
<li><p>The <strong>connected component</strong> of $x_0$ in $X$ is the union of the connected sets that contain $x_0$.</p>
</li>
<li><p>A metric space $X$ is <strong>totally disconnected</strong> if the connected component of each point is the point itself.</p>
</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 2</strong>: </p>
<ul>
<li>Let $X$ be a metric space. $X$ is connected iff, the only clopen subsets of $X$ are $∅$ and $X$.<ul>
<li>Proof:<ul>
<li>Suppose $S\notin {\emptyset ,X}$ is a clopen subset of $X$, then $X\setminus S$ can be an open subset such that <ul>
<li>$S\cap X = S\neq \emptyset$ and $(X\setminus S)\cap X= (X\setminus S)\neq \emptyset$</li>
<li>$S\cap(X\setminus S)=\emptyset$</li>
<li>$X = S \cup (X\setminus S)$</li>
<li>So $X$ is not connected, contradiction!</li>
</ul>
</li>
<li>Suppose that $X$ is not connected, then we can find nonempty and disjoint open subsets $O\neq X$ and $X\setminus O$ such that $O \cup (X\setminus O) = X$. <ul>
<li>Since $O$ is open, then $X\setminus O$ is closed, so it is a clopen set.</li>
<li>Since the only clopen subsets of $X$ are $\emptyset$ and $X$, contradiction!</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Exercise 3.6.35, Example 3.6.45</strong>:</p>
<ul>
<li>A discrete metric space $X$ is totally disconnected.</li>
<li>A subset of a discrete metric space is connected iff its cardinality is at most 1. <ul>
<li>Discrete metric spaces only have isolated points, if its cardinality is 2, then take two points $a,\ b$, and consider $B_\varepsilon(a),\ B_\varepsilon(b)$, where $\varepsilon&lt;1$.</li>
<li>According to the proposition 2, a discrete space is not connected unless it contains only one element, because any subset of the discrete space is clopen.</li>
</ul>
</li>
</ul>
<p><strong>Exercise 3.6.35</strong>:</p>
<ul>
<li>A finite subset of any metric space is connected iff its cardinality is at most 1.<ul>
<li>If its cardinality is 2, then take two points $a,\ b$, and consider $B_\varepsilon(a),\ B_\varepsilon(b)$, where $\varepsilon=\frac{d(a,b)}{2}$ or $\varepsilon=\frac{d(a,b)}{3}$. </li>
</ul>
</li>
</ul>
<p><strong>Exercise 3.6.35</strong>:</p>
<ul>
<li>$\mathbb{Q}$ is not connected in $\mathbb{R}$<ul>
<li>Consider $U= (−∞,\sqrt{2} )$ and $V=(\sqrt{2}, ∞)$ or directly $\mathbb{Q}=(−∞,\sqrt{2} )\cup(\sqrt{2}, ∞)$.</li>
</ul>
</li>
<li>A subset $A$ of $\mathbb{R}$ in the usual metric is connected iff $A$ is an interval.<ul>
<li>Proof<ul>
<li>We first prove that any interval $I$ is connected in $\mathbb{R}$. <ul>
<li>Suppose $ I = O \cup U$ and $O$ and $U$ are nonempty and disjoint open subsets of $I$.</li>
<li>Let $a\in O$ and $b\in U$ and  $a &lt; b $. </li>
<li>Let $c=\sup{t\in O:t&lt;b}$, note that $a\leq c\leq b$ so we have $c\in I$.</li>
<li>If $c\in O$, then $c\notin U$ and $c \neq b$, so $c&lt;b$.</li>
<li>Since $c$ is the least upper bound, then for any $\varepsilon &gt; 0$ we have $b \in B_\varepsilon(c) $, then there exists an $\varepsilon&gt;0$ such that $B_\frac{\varepsilon}{2}(b)\in B_\varepsilon(c)\subseteq O$. Contradiction!</li>
</ul>
</li>
<li>We then prove that any connected set in $\mathbb{R}$ is an interval.<ul>
<li>Suppose $I$ is a nonempty connected set in $\mathbb{R}$ but not an interval.</li>
<li>Then there exist two points $a$ and $b$ in $I$ such that $(a,b)\setminus I\neq\emptyset$. </li>
<li>Let $c\in (a,b)\setminus I$, $O= I \cap (-\infty,c)$ and $U= I \cap (c, \infty)$.</li>
<li>Then $O$ and $U$ are nonempty and disjoint open subsets of $I$ such that $O \cup U = I$. Contradiction!</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Exercise 3.6.35</strong>:</p>
<ul>
<li><p>A convex subset of $\mathbb{R}^n$ with the usual metric is a connected set.</p>
<ul>
<li><p>Proof:</p>
<ul>
<li><p>Assume that a convex set $A⊂\mathbb{R}^n$ is disconnected. Then for some open sets $O,U$, we have $O∪U=A$. </p>
<ul>
<li>Consider some $o∈O$ and $u∈U$. Define $L$ as the line segment connecting $u$ and $v$. Thus, $u∈O∩L≠∅$ and $v∈U∩L≠∅$. </li>
</ul>
</li>
<li><p>Since $A$ is convex, $L⊂A$. Then $(O∩L)∩(V∩L)⊂O∩U=∅$, so $(O∩L)∩(U∩L)=∅$. </p>
<ul>
<li>Furthermore, $(O∪U)∩L=A∩L=L$, so $(O∩L)∪(U∩L)=L$. </li>
</ul>
</li>
<li><p>As a metric subspace $L$ is open, since $O,\ U$ are both open, so $O∩L, U∩L$ are  nonempty disjoint open sets. Thus, $L$ is disconnected. </p>
</li>
</ul>
</li>
<li><p>Or assume that a convex set $A⊂\mathbb{R}^n$ is disconnected. Then for some open sets $U,V$, we have, $U∩A≠∅$ and $V∩A≠∅$, $(U∩A)∩(V∩A)=∅$, and $(U∩A)∪(V∩A)=A$. </p>
<ul>
<li>Consider some $u∈(U∩A)$ and $v∈(V∩A)$. Define $L$ as the line segment connecting $u$ and $v$. Thus, $u∈U∩L≠∅$ and $v∈V∩L≠∅$. <ul>
<li>Since $A$ is convex, $L⊂A$. Then $(U∩L)∩(V∩L)⊂(U∩A)∩(V∩A)=∅$, so $(U∩L)∩(V∩L)=∅$. </li>
</ul>
</li>
<li>Furthermore, $((U∩A)∪(V∩A))∩L=A∩L=L$, so $((U∩A)∩L)∪((V∩A)∩L)=(U∩L)∪(V∩L)=L$. Thus, $L$ is disconnected. </li>
</ul>
</li>
<li><p>Choose $f:\mathbb{R}→\mathbb{R}^n$ defined as $f(t)=(1−t)u+tv$ (where $u$ and $v$ are the previously chosen points in $\mathbb{R}^n$). </p>
<ul>
<li><p>Let $\varepsilon&gt;0$ and let $a∈\mathbb{R}$. Choose $δ=\frac{\varepsilon}{||v−u||}$ which is greater than 0. Then if $|t−a|&lt;δ$, we have </p>
</li>
<li><p>$$<br>  \begin{aligned}<br>||f(t)−f(a)||&amp;=||(1−t)u+tv−(1−a)u−av||\&amp;=||(a−t)u+(t−a)v||\&amp;=||(t−a)v−(t−a)u||\&amp;=|t−a|⋅||v−u||\&amp;&lt;δ||v−u||\&amp;=\frac{\varepsilon}{||v−u||}||v−u||\&amp;=\varepsilon<br>  \end{aligned}<br>$$</p>
<ul>
<li><p>Thus, $f$ is continuous at $a$. </p>
</li>
<li><p>Since $a$ was arbitrary, $f$ is continuous. </p>
</li>
<li><p>Since $[0,1]$ is an interval, it is connected. Thus, since $f$ is continuous and $f([0,1])=L$, $L$ is connected. Contradiction. </p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Exercise 3.6.41</strong>:</p>
<ul>
<li>Connected components are closed.<ul>
<li>Proof:<ul>
<li>Suppose $A$ is connected, $\overline{A}$ is a disconnected, then $\overline{A}$ can be written as two disjoint nonempty sets $O$ and $U$ that are open in $\overline{A}$ .<ul>
<li>Suppose $O\cap A\neq \emptyset$ and $A\nsubseteq O$, then $A$ can be written as two disjoint nonempty sets $O\cap A$ and $U\cap A$ which are open in $A$, contradiction!</li>
<li>Thus $O\cap A=\emptyset$ or $A\subseteq O$.</li>
<li>Since $O\cap U= \emptyset$ and $O\cup U=A\cup A’$.</li>
<li>Hence if $O\cap A=\emptyset$, we have $A\subseteq U$, if $A\subseteq O$, we have $U\cap A=\emptyset$, we only need to show one of these cases is impossible.</li>
<li>Suppose $U\cap A=\emptyset$ and $A\subseteq O$, then $U\subseteq A’$, take any $a_0\in U\subseteq A’$, then $a_0$ is an accumulation point of $A$. </li>
<li>Since $U$ is open, we have for some $r&gt;0$, $B_r(a_0)\subseteq U\subseteq A’$</li>
<li>Since $A\subseteq O$, $O\cap U=\emptyset$, so $U\cap A=\emptyset $</li>
<li>$a_0\notin A$, then for some $r&gt;0$, ${B_r(a_0)\setminus a_0}\subseteq U\subseteq A’$ and ${B_r(a_0)\setminus a_0}\cap A=\emptyset $ contradicts with the definition of accumulation points.</li>
<li>Thus $\overline{A}$ is connected.</li>
<li>Since $A$ and $\overline{A}$ are both the connected subsets containing $x_0$, their union is $\overline{A}$ which is closed.</li>
</ul>
</li>
<li>Or directly the connected component is itself a metric subspace, which is closed.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-18T09:43:18.000Z" title="9/18/2020, 5:43:18 PM">2020-09-18</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/">1.1.B Metric Spaces and Continuity</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/18/Mathematics/Analysis/2%20Metric%20Spaces/Topology-of-Metric-Spaces/">Topology of Metric Spaces</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=F27200>Open ball</font>, Closed ball</strong>:</p>
<ul>
<li>Let $(X, d)$ be a metric space and $x_0 ∈ X$. If $r ∈ \mathbb{R}$, with $r &gt; 0$,</li>
<li>The <strong><font color=F27200>open ball</font></strong> of radius $r$ around $x_0$ is $B_r(x_0) = {x ∈ X | d(x,x_0) &lt; r}$. </li>
<li>The closed ball of radius $r$ around $x_0$ is $B_r(x_0)={x∈X|d(x,x_0)≤r}$.</li>
</ul>
<blockquote>
<p>The open ball is never empty, for it contains $x$.</p>
</blockquote>
<p><strong>Neighborhood</strong>:</p>
<ul>
<li>For any $x ∈ X$ and $ε &gt; 0$, the <strong>$ε$-neighborhood</strong> of $x$ in $X$ </li>
</ul>
<p>$$<br>N_{ε,X} (x) := {y ∈ X : d(x, y) &lt; ε}.<br>$$</p>
<ul>
<li>A <strong>neighborhood</strong> of $x$ in $X$ is any subset of $X$ that contains at least one $ε$-neighborhood of $x$ in $X$.</li>
</ul>
<blockquote>
<p>It is the same with the open ball.</p>
<p>The $ε$-neighborhood of $x$ in a metric space $X$ depends on $ε$ and $x$. And it also depends on the set $X$ and the distance function $d$ used to metrize this set. </p>
</blockquote>
<p><strong><font color=FF2600>Open Set</font>, <font color=FF2600>Closed Set</font>, <font color=FF2600>Bounded Set</font></strong>: </p>
<ul>
<li>A subset $S$ of $X$ is said to be <font color=FF2600><strong>open</strong></font> in $X$ if, for each $x ∈ S$, <u>there exists</u> an $ε &gt; 0$ such that $N_{ε,X}(x) ⊆ S$. </li>
<li>A subset $S$ of $X$ is said to be <font color=FF2600><strong>closed</strong></font> in $X$ if $X\setminus S$ is open in $X$.</li>
<li>A subset $S$ of $X$ is said to be <font color=FF2600><strong>bounded</strong></font> in $X$ if there is a point $x \in X$ and $r &gt; 0$ such that $S\subseteq B_r(x)$ or $X = \emptyset$.</li>
</ul>
<blockquote>
<p>The open ball is inherently connected to the underlying metric space, so does the notions of open and closed sets.</p>
<p>Changing the metric on a given set, or concentrating on a metric subspace of the original metric space, would in general yield different classes of open (and hence closed) sets.</p>
</blockquote>
<blockquote>
<p>For any $x ∈ X$ the open ball is open, and the set ${x}$ is closed.</p>
</blockquote>
<blockquote>
<p>It is possible for a set in a metric space to be neither open nor closed. The structure of the mother metric space is crucial.</p>
<p>$\mathbb{Q}$ is neither closed nor open in $(\mathbb{R},d)$ while it is noth closed and open in $(\mathbb{Q},d)$</p>
</blockquote>
<p><strong>Clopen</strong>: </p>
<ul>
<li>The sets which are both open and closed are sometimes called <strong>clopen</strong>.</li>
</ul>
<blockquote>
<p>In any metric space $X$, the sets $X$ and $∅$ are both open and closed.</p>
</blockquote>
<blockquote>
<p>Any subset of a discrete space is clopen.</p>
</blockquote>
<p><strong><font color=FF2600>Accumulation Point</font>, <font color=FF2600>Isolated Point</font>, <font color=FF2600>Boundary Point</font></strong>:</p>
<ul>
<li><p>Let $X$ be a metric space, $x_0 \in X$, $S \subseteq X$.</p>
</li>
<li><p>A point $x_0 ∈ X$ is an <font color=FF2600><strong>accumulation point</strong></font> of $S$ if, <u>for any</u> $r&gt;0$, we have $(B_r(x_0)\setminus {x_0})\cap S\neq\emptyset$.</p>
</li>
<li><p>A point $x_0 ∈ X$ is an <font color=FF2600><strong>isolated point</strong></font> of $S$ if, <u>there exists</u> an $r &gt; 0$ such that $B_r(x_0) \cap S = {x_0}$.</p>
</li>
<li><p>A point $x_0 ∈ X$ is an <font color=FF2600><strong>boundary point</strong></font> of $S$ if, <u>for any</u> $r &gt; 0$, we have $B_r(x_0)\cap S \neq\emptyset$ and $B_r(x_0)\cap S^c \neq \emptyset$.</p>
</li>
</ul>
<ul>
<li><p>A point $x_0 ∈ X$ is an <font color=FF2600><strong>accumulation point</strong></font> of $S$ if, <u>there exists</u> a sequence ${x_k}<em>{k\in\mathbb{N}}\subseteq S$ <u>with all different points,</u> such that<br>$$<br>\lim</em>{k\to\infty}d(x_k,x_0)=0<br>$$</p>
</li>
<li><p>A point $x_0 ∈ X$ is an <font color=FF2600><strong>isolated point</strong></font> of $S$ if, $x_0\in S$ but $x_0$ is not an accumulation point of $S$.</p>
</li>
</ul>
<blockquote>
<p>Above definitions are equivalent to each other.</p>
</blockquote>
<p><strong>Interior, Closure, Boundary</strong>: </p>
<ul>
<li><p>Let $X$ be a metric space and $S ⊆ X$. </p>
</li>
<li><p><u>The largest open set</u> in $X$ that is <u>contained</u> in $S$ is the <strong>interior</strong> of $S $ (relative to $X$), and is denoted by $int_X(S)$.<br>$$<br>int_X(S)=\bigcup{O∈\mathcal{O}_X :O⊆S},<br>$$</p>
<ul>
<li>where $\mathcal{O}_X$ is the class of all open subsets of $X$</li>
</ul>
</li>
<li><p>The <u>smallest closed set</u> in $X$ that <u>contains</u> $S$ is the <strong>closure</strong> of $S$ (relative to $X$), and is denoted by $cl_X(S)$.<br>$$<br>cl_X(S)=\bigcap{C\in\mathcal{C}_X:S\subseteq C}<br>$$</p>
<ul>
<li>where $\mathcal{C}_X$ is the class of all closed subsets of $X$</li>
</ul>
</li>
<li><p>The <strong>boundary</strong> of $S$ (relative to $X$), denoted by $bd_X(S)$, is defined as<br>$$<br>bd_X (S) := cl_X (S)\setminus int_X (S).<br>$$</p>
</li>
</ul>
<blockquote>
<p>Let $X$ be a metric space, and $Y$ a metric subspace of $X$. </p>
<p>For any subset $S$ of $Y$,  $int_X(S)$, could be different from $int_Y(S)$. </p>
<p>Consider $\mathbb{R}$ and $\mathbb{R}^2$.</p>
<p>The same with the closure and the boundary operators.</p>
</blockquote>
<blockquote>
<p>It is obvious that a set $S$ in a metric space $X$ is closed iff $cl_X(S) = S$. </p>
<p>$S$ is open iff $int_X(S) = S$. </p>
<p>$x ∈ bd_X(S)$ iff $S ∩ N_{ε,X}(x)$ and $(X\setminus S) ∩ N_{ε,X}(x)$ are nonempty for any $ε &gt; 0$.</p>
</blockquote>
<p><strong>Diameter</strong>: </p>
<ul>
<li>Let $A$ be a nonempty subset of a metric space $X$. The diameter of $A$ is</li>
</ul>
<p>$$<br>\mbox{diam}(A)= \sup_{x,y\in A} d(x,y).<br>$$</p>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Exercise 6</strong>: </p>
<ul>
<li>Suppose that $X$ is a metric space, $Y$ is a metric subspace of $X$, and take any $S\subseteq Y$.</li>
<li>$S$ is open in $Y$ iff $S=O\cap Y$ for some open subset $O$ of $X$</li>
<li>$S$ is closed in $Y$ iff $S=C\cap Y$ for some closed subset $C$ of $X$.</li>
</ul>
<p><strong>Theorem 3.3.14, Theorem 3.3.21</strong>:</p>
<ul>
<li>Suppose ${A_j}_{j\in J}$ is a family of open sets in a metric space<ul>
<li>${A_j}<em>{j\in J}$ open $\Longrightarrow \cup</em>{j\in J} A_j$ open.</li>
<li>$A_1, A_2,\dots,A_n$ open $\Longrightarrow \cap_{j=1}^n A_j$ open.</li>
<li>${A_j}<em>{j\in J}$ closed $\Longrightarrow \cap</em>{j\in J} A_j$ closed.</li>
<li>$A_1, A_2,\dots,A_n$ closed $\Longrightarrow \cup_{j\in J} A_j$ closed.</li>
</ul>
</li>
</ul>
<p><strong>Lemma 1.6.29</strong>:</p>
<ul>
<li>Suppose that $X$ is a metric space, $x\in X$ and $S\subseteq X$.</li>
<li>$x$ is an accumulation point of $S$ iff <u>every neighborhood o</u>f $x$ contains infinitely many <u>points of $S$</u>.</li>
</ul>
<blockquote>
<p>Lemma 1.6.29 is equivalent to the definitions of the accumulation point.</p>
</blockquote>
<p><strong>Proposition 1</strong>: </p>
<ul>
<li>A set $S$ in a metric space $X$ is closed iff, <u>every sequence</u> in $S$ that converges in $X$ converges to <u>a point in</u> $S$.</li>
</ul>
<p><strong>Theorem 3.3.27</strong>:</p>
<ul>
<li>A set $S$ in a metric space $X$ is closed iff, $S$ contains <u>all its accumulation points</u>.</li>
</ul>
<blockquote>
<p>Proposition 1 is equivlent to Theorem 3.3.27</p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R^n}$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-09T09:42:56.000Z" title="9/9/2020, 5:42:56 PM">2020-09-09</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-A-Preliminaries-of-Real-Analysis/">1.1.A Preliminaries of Real Analysis</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/09/Mathematics/Analysis/1%20Preliminaries%20of%20Real%20Analysis/Constructing-Complex-Numbers/">Constructing Complex Numbers</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Complex Numbers</strong>:</p>
<ul>
<li><p>Consider the Cartesian product $\mathbb{R} × \mathbb{R}$ with addition and multiplication defined by<br>$$<br>(a, b) + (c, d) = (a + c, b + d)\<br>(a, b)(c, d) = (ac − bd, bc + ad)<br>$$</p>
</li>
<li><p>The field of <strong>complex numbers</strong> is the set $\mathbb{C} = \mathbb{R} × \mathbb{R}$ with the operations of addition and multiplication defined above.</p>
</li>
<li><p>The additive identity is $(0,0)$ and the multiplicative identity is $(1,0)$.</p>
</li>
<li><p>For $(a,b)$ the additive inverse is $(-a,-b)$. If $(a,b)\neq(0,0)$, thhe multiplicative inverse is $(a,b)^{-1}=(\frac{a}{a^2+b^2},\frac{b}{a^2+b^2})$.</p>
</li>
<li><p>We denote $z=(a,b)$ by $z=a+bi$ and $\overline{z}=a-bi$.</p>
</li>
</ul>
<blockquote>
<p>$\mathbb{C}$ is a field.</p>
</blockquote>
<p><strong>The Abosulute Value of Complex Numbers</strong>:</p>
<ul>
<li>If $z=a+bi$ with $a,b∈\mathbb{R}$, the <strong>absolute value</strong> of $z$ is $|z|=(z\overline{z})^\frac{1}{2} =(a^2+b^2)^\frac{1}{2}$, where, of course, we mean the nonnegative square root in $\mathbb{R}$.</li>
<li>If $z,w\in\mathbb{C}$, then $d(z,w)=|z-w|$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-09T09:42:39.000Z" title="9/9/2020, 5:42:39 PM">2020-09-09</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-A-Preliminaries-of-Real-Analysis/">1.1.A Preliminaries of Real Analysis</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/09/Mathematics/Analysis/1%20Preliminaries%20of%20Real%20Analysis/Constructing-Real-Numbers/">Constructing Real Numbers</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Upper Bound, Lower Bound, Bounded</strong>:</p>
<ul>
<li>Let $F$ be an ordered field. Let $A$ be a nonempty subset of $F$. </li>
<li>$A$ is <strong>bounded above</strong> if there is an element $M ∈ F$ with the property that if $x ∈ A$, then $x ≤ M$. We call $M$ an <strong>upper bound</strong> for $A$. </li>
<li>$A$ is <strong>bounded below</strong> if there is an element $m ∈ F$ such that if $x ∈ A$, then $m ≤ x$. We call $m$ a <strong>lower bound</strong> for $A$. </li>
<li>We say that $A$ is <strong>bounded</strong> if $A$ is bounded above and $A$ is bounded below.</li>
</ul>
<p><strong>Least Upper​ Bound, Greatest Lower Bound</strong>:</p>
<ul>
<li>Let $F$ be an ordered field, and let $A$ be a nonempty subset of $F$ which is bounded above.</li>
<li>$L ∈ F$ is a <strong>least upper bound</strong> for $A$ if: <ul>
<li>$L$ is an upper bound for $A$;</li>
<li>if $M$ is any upper bound for $A$, then $L≤M$.</li>
</ul>
</li>
<li>The definition of a <strong>greatest lower bound</strong> is similar with all of the inequalities reversed.</li>
</ul>
<p><strong>The Greatest Lower Bound Property, The Least Upper Bound Property</strong>:</p>
<ul>
<li><p>An ordered field $F$ has <strong>the greatest lower bound property</strong> if every nonempty subset $A$ of $F$ that is bounded below has a greatest lower bound. </p>
</li>
<li><p>That is, there exists an element $l$ of $F$ such that: </p>
<ol>
<li>$l$ is a lower bound for $A$;</li>
<li>if $m$ is any lower bound for $A$ , then $m≤l$.</li>
</ol>
</li>
<li><p>The definition of <strong>the least upper bound property</strong> is similar.</p>
</li>
</ul>
<p><strong><font color=941100>Real Numbers, $\mathbb{R}$ / Complete Ordered Field</font></strong>:</p>
<ul>
<li>The <strong><font color=941100>real numbers</font></strong> are the unique, up to order isomorphism ordered field that satisfies the least upper bound property. </li>
<li>We will denote this field by $\mathbb{R}$.</li>
<li>$\mathbb{R}$ is obtained from $\mathbb{Q}$ by filling the “holes” in $\mathbb{Q}$ to obtain an ordered field that satisfies the Completeness Axiom. We thus say that $\mathbb{R}$ is a complete ordered field.</li>
</ul>
<p>$\mathbf{R}$, <strong>Equivalence Classes in</strong> $\mathcal{C}$:</p>
<ul>
<li>Let $(a_k)_{k∈\mathbb{N}}$, and $(b_k)_{k∈\mathbb{N}}$ be Cauchy sequences in $\mathbb{Q}$. </li>
<li>$(a_k)<em>{k∈\mathbb{N}}$ is equivalent to $(b_k)_{k∈\mathbb{N}}$, denoted by $(a_k)_{k∈\mathbb{N}} ∼ (b_k)_{k∈\mathbb{N}}$, if $(c_k)_{k∈\mathbb{N}}= (a_k − b_k)</em>{k∈\mathbb{N}}$ is in $\mathcal{I}$.<ul>
<li>The set $\mathcal{I}$ consists of Cauchy sequences that converge to 0.</li>
</ul>
</li>
<li>$∼$ defines an <strong>equivalence relation</strong> on $\mathcal{C}$.<ul>
<li>The set $\mathcal{C}$ denote the set of all Cauchy sequences of rational numbers. </li>
</ul>
</li>
<li>$\mathbf{R}$ is defined as the set of <strong>equivalence classes</strong> in $\mathcal{C}$.<ul>
<li>The equivalent class of $(a_k)_{k∈\mathbb{N}}$ is denoted by $[a_k]$.</li>
</ul>
</li>
</ul>
<p><strong>Order Relation on</strong> $\mathbf{R}$:</p>
<ul>
<li>Let $a = [a_k]$, $b = [b_k]$ be distinct elements of $\mathbf{R}$. </li>
<li>We define $a &lt; b$ if $a_k &lt; b_k$ eventually and $b &lt; a$ if $b_k &lt; a_k$ eventually, which is an <strong>order relation</strong> on $\mathbf{R}$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R^n}$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-09T09:42:20.000Z" title="9/9/2020, 5:42:20 PM">2020-09-09</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-A-Preliminaries-of-Real-Analysis/">1.1.A Preliminaries of Real Analysis</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/09/Mathematics/Analysis/1%20Preliminaries%20of%20Real%20Analysis/Elements-of-Metric-Spaces/">Elements of Metric Spaces</a></h1><div class="content"><p>A function $f$ on $\mathbb{R}$ is continuous at a given point $a ∈ \mathbb{R}$ iff the image of a point which is close to $a$ is itself close to $f(a)$. </p>
<ul>
<li>The “geometric” way of thinking about continuity depends intrinsically on the “distance” between two points. </li>
<li>For the continuity of functions defined on more complicated sets, the meaning of the term “close” is not transparent.</li>
<li>Therefore, need the conception of distance between two elements of an arbitrary set</li>
</ul>
<p>By way of abstraction, the notion of distance function is built only on three properties. </p>
<ul>
<li>These properties are strong enough to introduce to an arbitrary nonempty set a geometry rich enough to build a satisfactory theory of continuous functions.</li>
</ul>
<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong><font color=F27200>Metric</font></strong>: </p>
<ul>
<li>Let $X$ be any nonempty set. Let $X$ be any nonempty set. </li>
<li>A function $d : X × X → \mathbb{R}_+$ that satisfies the following properties is called a <strong>distance function</strong> or a <strong><font color=F27200>metric</font></strong> on $X$: </li>
<li>For any $x, y, z ∈ X$, <ol>
<li>(Positive Definite) $d(x,y)=0$ iff $x=y$, </li>
<li>(Symmetry) $d(x, y)=d(y, x)$,</li>
<li>(Triangle Inequality) $d(x, y) ≤ d(x, z) + d(z, y)$.</li>
</ol>
</li>
</ul>
<p><strong><font color=941100>Metric Space</font></strong>: </p>
<ul>
<li>If $d$ is a distance function on $X$, we say that $(X, d)$ is a <strong><font color=941100>metric space</font></strong>, and refer to the elements of $X$ as points in $(X, d)$. </li>
</ul>
<p><strong>Metric Subspace</strong>: </p>
<ul>
<li>If $(X,d)$ is a metric space and $∅\neq Y ⊂X$.</li>
<li>$(Y, d|_{Y ×Y} )$, or simply $Y$, is a <strong>metric subspace</strong> of $X$</li>
<li>$d$ is called <strong>the inherited metric</strong>.</li>
</ul>
<p><strong><font color=F27200>Sequence</font></strong>: </p>
<ul>
<li>A <strong><font color=F27200>sequence</font></strong> in a nonempty set $X$ is a function $f : \mathbb{N }→ X$, </li>
<li>This function is represented as $(x_1,x_2,…)$ where $x_i := f(i)$ for each $i ∈ \mathbb{N}$. </li>
<li>The set of all sequences in $X$ is $X^{\mathbb{N}}$ or $X^∞$. </li>
</ul>
<p><strong>Subsequence</strong>: </p>
<ul>
<li>A <strong>subsequence</strong> of a sequence $f ∈ X^\mathbb{N}$ is a function of the form $f ◦σ$ where $σ :\mathbb{N} → \mathbb{N}$ is strictly increasing </li>
<li>That is, $σ(k) &lt; σ(l)$ for any $k,l ∈ N$ with $k &lt; l$. </li>
<li>This function is represented as the array $(x_{m_1},x_{m_2},…)$ with  $m_k = σ(k)$ and $x_{m_k} = f(m_k)$ for each $k = 1,2,….$ </li>
</ul>
<p><strong>Increasing, Decreasing, Monotonic</strong>: </p>
<ul>
<li>A real sequence $(x_m)$ is said to be <strong>increasing</strong> if $x_m ≤ x_{m+1}$ for each $m ∈\mathbb{N}$, and <strong>strictly increasing</strong> if $x_m &lt; x_{m+1}$ for each $m ∈\mathbb{N}$. </li>
<li>It is said to be (<strong>strictly</strong>) <strong>decreasing</strong> if $(−x_m)$ is (strictly) increasing. </li>
<li>A real sequence which is either increasing or decreasing is referred to as a <strong>monotonic</strong> sequence. </li>
</ul>
<p><strong><font color=FF2600>Convergent Sequence</font></strong>:</p>
<ul>
<li><p>A sequence $(a_n)_{n\in\mathbb{N}}$ of points is a <strong><font color=FF2600>convergent sequence</font></strong> in a metric spaces $X$ if, <u>given any</u> real number $\varepsilon &gt; 0$, there exists an integer $N_\varepsilon\in\mathbb{N}$ such that if $n \geq N_\varepsilon$, then $d(a_n, L) &lt; \varepsilon$.</p>
</li>
<li><p>$L$ is called the <strong>sequitial limit</strong>.</p>
</li>
</ul>
<p><strong><font color=FF2600>Cauchy Sequence</font></strong>:</p>
<ul>
<li>A sequence $(a_n)_{n\in\mathbb{N}}$ of real numbers is a <strong><font color=FF2600>Cauchy sequence</font></strong> in a metric spaces $X$ if, <u>given any</u> real number $\varepsilon &gt; 0$, there exists an integer $N_\varepsilon\in\mathbb{N}$ such that if $n, m \geq N_\varepsilon$, then $d(a_n, a_m) &lt; \varepsilon$.</li>
</ul>
<blockquote>
<p>Intuitively speaking, by a Cauchy sequence we mean a sequence the terms of which eventually get arbitrarily close to one another. </p>
</blockquote>
<p><strong>Subsequential Limit</strong>: </p>
<ul>
<li>Given any $(x_m) ∈ \mathbb{R}^∞$, we say that $x ∈ \overline{\mathbb{R}} $ is a <strong>subsequential limit</strong> of $(x_m)$ if there exists a subsequence $(x_{m_k} )$ with $x_{m_k} → x$ as $k → ∞$ </li>
<li>That is, if, for any $ε&gt;0$, there exists a $K∈\mathbb{N} $ such that $ |x_m −x|&lt;ε$ for all $k≥K$.</li>
</ul>
<p><strong>Limit Supremum, Limit Infimum</strong>:</p>
<ul>
<li>Let $(a_k)_{k∈\mathbb{N}}$ be a bounded sequence of real numbers. </li>
<li>For each $n ∈ \mathbb{N}$, define $b_n = \sup{a_k | k ≥ n}$, and $c_n = \inf{a_k | k ≥ n}$. </li>
<li>We define the <strong>limit supremum</strong> of the sequence $(a_k)<em>{k∈\mathbb{N}}$ to be $\lim</em>{n→∞} b_n$, and we denote this by $\lim \sup_{k→∞} a_k$. </li>
<li>We define the <strong>limit infimum</strong> of $(a_k)<em>{k∈\mathbb{N}}$ similarly: $\lim \inf</em>{k→∞} a_k = \lim_{n→∞} c_n$.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Young’s Inequality</strong>:</p>
<ul>
<li><p>Suppose $a,b\geq 0$ in $\mathbb{R}$. </p>
</li>
<li><p>Suppose $p,q\in \mathbb{R}$ with $p,q&gt;1$ such that $\frac{1}{p}+\frac{1}{q} = 1$. Then<br>$$<br>ab\leq\frac{a^p}{p}+\frac{b^q}{q}<br>$$</p>
</li>
</ul>
<p><strong>Theorem 3.2.7</strong>: (Hölder’s Inequality)</p>
<ul>
<li><p>Suppose $x = (x_1,x_2,…,x_n) ∈ \mathbb{R}^n$ and $y = (y_1,y_2,…,y_n) ∈ \mathbb{R}^n$</p>
</li>
<li><p>Suppose $p,q\in \mathbb{R}$ with $p,q&gt;1$ such that $\frac{1}{p}+\frac{1}{q} = 1$. Then<br>$$<br>\sum^n_{k=1}|x_k-y_k|\leq(\sum^n_{k=1}|x_k|^p)^\frac{1}{p}(\sum^n_{k=1}|y_k|^q)^\frac{1}{q}<br>$$</p>
</li>
</ul>
<p><strong>Minkowski’s Inequality 1</strong>:</p>
<ul>
<li>For any $n ∈ \mathbb{R} $, $a_i,b_i ∈\mathbb{R}, i = 1,…,n$, and any $1 ≤ p &lt;∞$,</li>
</ul>
<p>$$<br>  \left(\sum^n_{i=1}|a_i +b_i|^p \right)^{\frac1{p}}\leq\left(\sum^n_{i=1}|a_i |^p \right)^{\frac1{p}}+\left(\sum^n_{i=1}|b_i|^p \right)^{\frac1{p}}<br>$$</p>
<p><strong>Minkowski’s Inequality 2</strong>:</p>
<ul>
<li>For any $(x_m),(y_m) ∈ \mathbb{R}^\infty $, and $1\leq p&lt;\infty $,</li>
</ul>
<p>$$<br>\left(\sum^n_{i=1}|x_i +y_i|^p \right)^{\frac1{p}}\leq\left(\sum^n_{i=1}|x_i |^p \right)^{\frac1{p}}+\left(\sum^n_{i=1}|y_i|^p \right)^{\frac1{p}}<br>$$</p>
<h2 id="Examples-and-Exercises"><a href="#Examples-and-Exercises" class="headerlink" title="Examples and Exercises"></a>Examples and Exercises</h2><p><strong>Discrete Metric Space</strong>:</p>
<ul>
<li><p>Let $X$ be any nonempty set and, for $x_1,x_2 ∈ X$, define<br>$$<br>d(x_1,x_2)=\left{\begin{matrix}0&amp;if\ x_1=x_2\ 1&amp; if\ x_1\neq x_2\end{matrix}\right.<br>$$</p>
</li>
<li><p>This is called the discrete metric, the pair $(X,d)$ is referred to as a discrete metric space. </p>
</li>
</ul>
<blockquote>
<p>It is designed to disabuse people of the notion that every metric looks like the usual metric on $\mathbb{R}^n$. </p>
<p>The discrete metric is very handy for producing counterexamples.</p>
</blockquote>
<p>$(\mathbb{R}^n, d_p) =\mathbb{R}^{n,p}=\ell^p_n(\mathbb{R})$</p>
<ul>
<li><p>For each $1≤p≤∞$, where $d_p :\mathbb{R}^n×\mathbb{R}^n →\mathbb{R}<em>+$ is defined by<br>$$<br>d_p(x, y) := |x-y|_p=\left(\sum^n</em>{i=1}|x_i − y_i|^p \right)^{\frac1{p}}\ \ \ \ \mbox{for}\ \  1 ≤ p &lt; ∞,<br>$$</p>
</li>
<li><p>and</p>
</li>
</ul>
<p>$$<br>d_p(x,y):=\max{|x_i −y_i|:i=1,…,n}\ \ \ \  \mbox{for}\ \ p=∞.<br>$$</p>
<blockquote>
<p>$(\mathbb{R}^n, d_p)$ is not a metric space for $p &lt; 1$.</p>
</blockquote>
<p>$(\ell^p, d_p)$:</p>
<ul>
<li><p>For any $1 ≤ p &lt; ∞$, we define<br>$$<br>\ell^p :={(x_m)∈\mathbb{R}^∞ : \sum^∞_{i=1} |x_i|^p &lt;∞}.<br>$$</p>
</li>
<li><p>and $d_p : \ell^p × \ell^p → \mathbb{R} _+$ is defined by<br>$$<br>d_p ((x_m) , (y_m)) := \left(\sum^\infty_{i=1}|x_i − y_i|^p\right)^\frac1{p} .<br>$$</p>
</li>
</ul>
<blockquote>
<p>Any $\ell^p$ space is smaller than the set of all real sequences $\mathbb{R}^∞$ since the members of such a space are real sequences that are either bounded or that satisfy some form of a summability condition (that ensures that $d_p$ is real-valued). </p>
<p>Indeed, no $d_p$ defines a distance function on the entire $\mathbb{R}^∞$.</p>
</blockquote>
<p>$(\ell^\infty, d_\infty)$: </p>
<ul>
<li><p>$\ell^∞$ is the set of all bounded real sequences,<br>$$<br>\ell^∞ :={(x_m)∈\mathbb{R}^∞ :\sup{|x_m|:m∈\mathbb{N}}&lt;∞}.<br>$$</p>
</li>
<li><p>and $d_∞ : \ell^∞ ×\ell^∞ → \mathbb{R} _+$ is defined by</p>
</li>
</ul>
<p>$$<br>d_∞ ((x_m) , (y_m)) := |x_m − y_m|_∞=\sup{|x_m − y_m| : m ∈ \mathbb{N}}.<br>$$</p>
<ul>
<li>This metric is called the sup-metric on the set of all bounded real sequences.</li>
</ul>
<p>$(\mathbf{B}(T), d_\infty)$: </p>
<ul>
<li><p>Let $T$ be any nonempty set. </p>
</li>
<li><p>$\mathbf{B}(T)$ is the set of all bounded real functions defined on $T$,<br>$$<br>\mathbf{B}(T):={f ∈\mathbb{R}^T :\sup{|f(x)|:x∈T}&lt;∞}.<br>$$</p>
</li>
<li><p>and $d_∞ : \mathbf{B}(T) ×\mathbf{B}(T ) → \mathbb{R}_+$ is defined by<br>$$<br>d_∞ (f, g) := |f − g|_∞=\sup{|f (x) − g(x)| : x ∈ T }.<br>$$</p>
</li>
</ul>
<blockquote>
<p>An $n$-vector or a sequence can always be thought of as special functions</p>
<p>$\mathbf{B}({1, …, n})$ coincides with $\mathbb{R}^{n,∞}$ (for any $n ∈ \mathbb{N}$) </p>
<p>$\mathbf{B}(\mathbb{N})$ coincides with $\ell^∞$.</p>
</blockquote>
<blockquote>
<p>Every continuous function on $[a, b]$ is bounded, and hence $\mathbf{C}[a, b] ⊆\mathbf{B}[a, b]$. </p>
<p>Consequently, we can consider $\mathbf{C}[a, b]$ as a metric subspace of $\mathbf{B}[a, b]$. </p>
</blockquote>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><p>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</p>
</li>
<li><p>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</p>
</li>
<li><p>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R^n}$ 1;</p>
</li>
<li><p>Terence Tao, 2006, “Analysis”; </p>
</li>
<li><p>周民强，2008，“实变函数论”；</p>
</li>
<li><p>周民强，2007，“实变函数解题指南”；</p>
</li>
<li><p>周民强，2010，“数学分析习题演练”；</p>
</li>
<li><p>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</p>
</li>
<li><p>张筑生，1990，”数学分析新讲“；</p>
</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-09T09:42:04.000Z" title="9/9/2020, 5:42:04 PM">2020-09-09</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-A-Preliminaries-of-Real-Analysis/">1.1.A Preliminaries of Real Analysis</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/09/Mathematics/Analysis/1%20Preliminaries%20of%20Real%20Analysis/Countability/">Countability</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Countably Infinite, Countable, Uncountable</strong>: </p>
<ul>
<li>A set $X$ is called <strong>countably infinite</strong> if there exists a bijection $f$ that maps $X$ onto the set $\mathbb{N} $ of natural numbers. </li>
<li>$X$ is called <strong>countable</strong> if it is either finite or countably infinite. </li>
<li>$X$ is called <strong>uncountable</strong> if it is not countable.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 1</strong>: </p>
<ul>
<li>Every subset of a countable set is countable.</li>
</ul>
<p><strong>Proposition 2</strong>: </p>
<ul>
<li>A countable union of countable sets is countable.</li>
</ul>
<p><strong>Corollary 1</strong>: (Cantor)</p>
<ul>
<li>$\mathbb{Q} $ is countable.</li>
</ul>
<p><strong>Proposition 4</strong>: (Cantor) </p>
<ul>
<li>$\mathbb{R} $ is uncountable.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R}^n$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-09-09T09:41:52.000Z" title="9/9/2020, 5:41:52 PM">2020-09-09</time></span><span class="level-item"><a class="link-muted" href="/categories/1-Mathematics/">1 Mathematics</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/">1.1 Analysis</a><span> / </span><a class="link-muted" href="/categories/1-Mathematics/1-1-Analysis/1-1-A-Preliminaries-of-Real-Analysis/">1.1.A Preliminaries of Real Analysis</a></span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/09/09/Mathematics/Analysis/1%20Preliminaries%20of%20Real%20Analysis/Elements-of-Set-Theory/">Elements of Set Theory</a></h1><div class="content"><h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Subset, Proper Subset</strong>:</p>
<ul>
<li><p>If every member of $A$ is also a member of $B$, then $A$ is a <strong>subset</strong> of $B$ and write $A⊆B$. </p>
<ul>
<li>$ A=B$ holds iff both $A⊆B$ and $B⊆A$ hold. </li>
</ul>
</li>
<li><p>If $A⊆B$ but $A\neq B$, then $A$ is  a <strong>proper subset</strong> of $B$, and write $A ⊂ B$。</p>
</li>
</ul>
<p><strong>Finite, Cardinality, Singleton, Infite</strong>: </p>
<ul>
<li>For any set $S$ that contains finitely many elements (in which case we say $S$ is <strong>finite</strong>), we denote by $|S|$ the total number of elements that $ S$ contains, and refer to this number as the <strong>cardinality</strong> of $S$. <ul>
<li>$S$ is a <strong>singleton</strong> if $|S| = 1$. </li>
</ul>
</li>
<li>If $S$ contains infinitely many elements (in which case we say $S$ is <strong>infinite</strong>), then we write $|S| = ∞$.</li>
</ul>
<p><strong>Empty Set</strong>:</p>
<ul>
<li><p>The set that contains no elements is defined as an <strong>empty set</strong>, that is,<br>$$<br>∅ := {x : x \neq x},<br>$$</p>
<ul>
<li>We have $∅ ⊆ S$ for any set $S$, it implies that $∅$ is unique.</li>
<li>If $S \neq ∅$, we say that $S$ is nonempty.</li>
<li>${∅}$ is a nonempty set.</li>
</ul>
</li>
</ul>
<p><strong>Power Set</strong>: </p>
<ul>
<li>We define the class of all subsets of a given set $S$ as</li>
</ul>
<p>$$<br>2^S :={T :T ⊆S},<br>$$</p>
<ul>
<li>which is called the <strong>power set</strong> of $S$. </li>
</ul>
<p><strong>Union, Intersection, Difference</strong>: </p>
<ul>
<li>Given any two sets $A$ and $B$, </li>
<li>The <strong>union</strong> of $A$ and $B$, denoted as $A ∪ B$, is defined as the set ${x : x ∈ A\ \mbox{or} \ x ∈ B}$.</li>
<li>The <strong>intersection</strong> of $A$ and $B$, denoted as $A ∩ B$, is defined as the set ${x : x ∈ A\ \mbox{and}\ x ∈ B}$. <ul>
<li>If $A ∩ B = ∅$, we say that $A$ and $B$ are disjoint. </li>
<li>If $A ⊆ B$, then $A∪B = B$ and $A∩B = A$. </li>
<li>$∅ ∪ S = S$ and $∅ ∩ S = ∅$ for any set $S$.</li>
</ul>
</li>
<li>The difference between $A$ and $B$, denoted as $A\setminus B$, is defined as the set ${ x : x ∈ A\ \mbox{and}\ x ∈/ B } $. </li>
</ul>
<p><strong>Class, Family, Member</strong>: </p>
<ul>
<li><p>“<strong>Class</strong>” or “<strong>family</strong>” refer to a nonempty collection of sets. </p>
</li>
<li><p>If $\mathcal{A}$ is a class, then $\mathcal{A} \neq ∅$ and any member $A ∈ \mathcal{A}$ is a set. </p>
</li>
<li><p>The union of all <strong>members</strong> of this class,  $\bigcup \mathcal{A}$, $\bigcup_{A∈\mathcal{A}}A$, is defined as the set ${x : x ∈ A\ \mbox{for some}\ A ∈ \mathcal{A}}$. </p>
<ul>
<li>To specify a class $\mathcal{A}$, we can designate a set $I$ as a set of indices</li>
<li>If $I={k,k+1,\dots, K}$ for some integers $k$ and $K$ with $k&lt;K$, then we often write $\bigcup^K A_i$, $\bigcup_{i=k}^KA_i$ for $\bigcup_{i\in I}A_i$.</li>
<li>If $I={k,k+1,\dots,}$ for some integer $k$, then we may write $\bigcup^\infty A_i$, $\bigcup_{i=k}^\infty A_i$ for $\bigcup_{i\in I}A_i$</li>
</ul>
</li>
<li><p>The intersection of all sets in $\mathcal{A}$,  $\bigcap \mathcal{A}$ , $\bigcap_{A∈\mathcal{A}}A$,  is defined as the set ${x : x ∈ A\ \mbox{for each}\ A ∈ \mathcal{A}}$. </p>
</li>
</ul>
<p><strong><font color=0096FF>Ordered Pair</font></strong>: </p>
<ul>
<li>An **<font color=0096FF>ordered pair </font>**is an ordered list $(a,b)$ consisting of two objects $a$ and $ b$. </li>
</ul>
<p><strong><font color=0096FF>(Cartesian) Product</font></strong>:</p>
<ul>
<li>The <strong><font color=0096FF>(Cartesian) product</font></strong> of $n$ sets $A_1, …, A_n$, is then defined as</li>
</ul>
<p>$$<br>A_1 ×···×A_n :={(a_1,…,a_n):a_i∈A_i, i=1,…,n}.<br>$$</p>
<p><strong><font color=0096FF>(Binary) Relation</font></strong>: </p>
<ul>
<li><p>Let $X$ and $Y$ be two nonempty sets. </p>
</li>
<li><p>A subset $R$ of $X × Y$ is called a <strong><font color=0096FF>(binary) relation</font></strong> from $X$ to $Y$. </p>
</li>
</ul>
<p><strong>Reflexive, Transitive, Symmetric</strong>, <strong>Antisymmetric, Complete</strong>: </p>
<ul>
<li>A relation $R$ on a nonempty set $X$ is said to be <ul>
<li><strong>reflexive</strong> if $xRx$ for each $x ∈ X$ </li>
<li>**transitive **if $xRy$ and $yRz$ imply $xRz$ for any $x,y,z ∈ X.$</li>
<li>**symmetric **if, for any $x, y ∈ X$, $xRy$ implies $yRx$,  </li>
<li><strong>antisymmetric</strong> if, for any $x, y ∈ X$, $xRy$ and $yRx$ imply $x = y$. </li>
<li><strong>complete</strong> if either $xRy$ or $yRx$ holds for each $x,y ∈ X$. </li>
</ul>
</li>
</ul>
<p><strong><font color=0096FF>Equivalence Relation, Equivalence Class</font></strong>: </p>
<ul>
<li><p>A relation $∼$ on a nonempty set $X$ is called an <strong><font color=0096FF>equivalence relation</font></strong> if it is reflexive, symmetric and transitive. </p>
</li>
<li><p>For any $x ∈ X$, the <strong>equivalence class</strong> of $x$ relative to $∼$ is defined as the set<br>$$<br>[x]_∼ :={y∈X:y∼x}.<br>$$</p>
</li>
<li><p>Let $X$ be a set. </p>
</li>
<li><p>An <strong>equivalence relation</strong> on $X$ is a relation $R$ on $X$ such that </p>
<ol>
<li>(Reflexive) For all $x ∈ X$, $x ∼ x$. </li>
<li>(Symmetric) For $x,y∈X$, if $x∼y$ then $y∼x$. </li>
<li>(Transitive) For $s,y,z∈X$, if $x∼y$ and $y∼z$, then $x∼z$. </li>
</ol>
</li>
</ul>
<p><strong><font color=0096FF>Preorder, Partial order, Linear order</font></strong>: </p>
<ul>
<li><p>A relation $\succsim$ on a nonempty set $X$ is called a **<font color=0096FF>preorder </font>**on $X$ if it is transitive and reflexive. </p>
</li>
<li><p>$\succsim$ is said to be a **<font color=0096FF>partial order </font>**on $X$ if it is an antisymmetric preorder on $X$. </p>
</li>
<li><p>$\succsim$ is called a **<font color=0096FF>linear order </font>**on $X$ if it is a partial order on $X$ which is complete.</p>
</li>
<li><p>By a <strong>preordered set</strong> we mean a list $(X, \succsim)$ where $X$ is a nonempty set and $\succsim$ is a preorder on $X$. </p>
</li>
<li><p>If $\succsim$ is a partial order on $X$, then  $(X, \succsim)$ is called a <strong>poset</strong> (short for partially ordered set)</p>
</li>
<li><p>If $\succsim$ is a linear order on $X$, then $(X, \succsim)$ is called either a chain or a <strong>loset</strong> (short for linearly ordered set).</p>
</li>
</ul>
<p><strong><font color=0096FF>Function / Map</font></strong>: </p>
<ul>
<li><p>By a <strong><font color=0096FF>function / map</font></strong> $f$ that maps $X$ into $Y$, denoted as $f:X→Y$, we mean a relation $f∈X×Y$ such that</p>
<ol>
<li>For every $x∈X$, there exists a $y∈Y$ such that $xfy$,</li>
<li>For every $y,z∈Y$ with $xfy$ and $xfz$, we have $y=z$.</li>
</ol>
</li>
<li><p>$X$ is the <strong>domain</strong> of $f$ and $Y$ the <strong>codomain</strong> of $f$. </p>
</li>
<li><p>The <strong>range</strong> of $f$ is<br>$$<br>f(X):={y∈Y :xfy\ \mbox{for some}\ x∈X}.<br>$$</p>
</li>
<li><p>The set of all functions that map $X$ into $Y$ is denoted by $Y^X$ or $f : X → Y$ .</p>
</li>
<li><p>When $ f(x) = y$, we refer to $y$ as the <strong>image</strong> or <strong>value</strong> of $x$ under $f$. </p>
</li>
</ul>
<p><strong>Composition</strong>: </p>
<ul>
<li><p>Given functions $f : X → Z$ and $g : Z → Y$, </p>
</li>
<li><p>The <strong>composition</strong> of $f$ and $g$ is the function $g◦f : X → Y$, where $g◦f(x) := g(f(x))$. </p>
</li>
<li><p>$(g ◦ f )(x)$ is usually written as $g ◦ f (x$).</p>
</li>
</ul>
<p><strong>Surjection, Injection, Bijection</strong>: </p>
<ul>
<li>If its range equals its codomain, that is, if $f(X) = Y$, then one says that $f$ maps $X$ onto $Y$, and refers to it as a <strong>surjection</strong>. </li>
<li>If $f$ maps distinct points in its domain to distinct points in its codomain, that is, if $x\neq y$ implies $f(x) \neq f(y)$ for all $x,y ∈ X$, then we say that $f$ is an <strong>injection</strong>.</li>
<li>If $f$ is both injective and surjective, then it is called a <strong>bijection</strong>.</li>
</ul>
<p><strong>Self-map, Identity Function</strong>: </p>
<ul>
<li>A function whose domain and codomain are identical, that is, a function in $X^X$, is called a <strong>self-map</strong> on $X$. </li>
<li>The <strong>identity function</strong> is a self-map on $X$, denoted as $\mbox{id}_X$, and defined as $\mbox{id}_X (x) := x$ for all $x ∈ X$. </li>
</ul>
<blockquote>
<p>$\mbox{id}_X$ is a bijection.</p>
</blockquote>
<p><strong><font color=0096FF>Binary Operation</font></strong>: </p>
<ul>
<li>Let $X$ be any nonempty set. </li>
<li>We refer to a function of the form $• : X ×X → X$ as a <strong><font color=0096FF>binary operation</font></strong> on $X$, and write $x•y$ instead of $•(x,y)$ for any $x, y ∈ X$.</li>
</ul>
<p><strong><font color=005493>Field</font></strong>: </p>
<ul>
<li><p>Let $X$ be any nonempty set, let $+$ and $·$ be two binary operations on $X$. </p>
</li>
<li><p>The list $(X, +, ·)$ is called a <strong><font color=005493>field</font></strong> if the following properties are satisfied:</p>
<ol>
<li>(Closure) $x+y\in X$ and $xy\in X$ for all $x,y∈X$;</li>
<li>(Commutativity) $x+y=y+x$ and $xy=yx$ for all $x,y∈X$;</li>
<li>(Associativity) $(x+y)+z = x+(y+z)$ and $(xy)z = x(yz)$ for all $x,y,z ∈ X$;</li>
<li>(Existence of Identity Elements) There exist elements $0$ and $1$ in $X$ such that $0 + x = x = x + 0$ and $1x = x = x1$ for all $x ∈ X$;</li>
<li>(Existence of Inverse Elements) <ul>
<li>(the additive inverse of $x$) For each $x ∈ X$, there exists an element $−x$ in $X$ such that $x+−x = 0 = −x+x$, and </li>
<li>(the multiplicative inverse of $x$) For each $x ∈ X\backslash{0}$, there exists an element $x^{−1}$ in $X$ such that $xx^{−1} = 1 = x^{−1}x$.</li>
</ul>
</li>
<li>(Distributivity) $x(y + z) = xy + xz$ for all $x, y, z ∈ X$;</li>
<li>(Cancellation) If $xz=yz$ and $z\neq0$ then $x=y$ for all $x,y,z ∈ X$;</li>
</ol>
</li>
<li><p>A field without the multiplicative inverse of $x$ and cancellation rule for the multiplication is <strong>a commutative ring with 1</strong>.</p>
</li>
</ul>
<p><strong><font color=005493>Ordered Field</font></strong>:</p>
<ul>
<li>The list $(X,+,·,≥)$ is called an <strong><font color=005493>ordered field</font></strong> if $(X,+,·)$ is a field, and if $≥$ is a partial order on $X$ such that<ol start="3">
<li>(Addition) If $x,y,z\in X$, and $x\leq y$, then $x+z\leq y+z$</li>
<li>(Multiplication by nonnegative elements) If $x,y,z\in X$, and $x\leq y,\ 0\leq z$ then $xz\leq yz$</li>
</ol>
</li>
<li>The list $(X,+,·,&lt;)$ is called an **<font color=005493>ordered field</font>** if $(X,+,·)$ is a field, and if $&lt;$ is a relation on $X$ such that<ol>
<li>(Trichotomy) If $x,y\in X$, then $x&lt;y,\ x=y$ or $x&gt;y$</li>
<li>(Transitivity) If $x,y\in X$, and $x&lt;y,\ y&lt;z$, then $x&lt;z$</li>
<li>(Addition) If $x,y,z\in X$, and $x&lt;y$, then $x+z&lt;y+z$</li>
<li>(Multiplication by postive elements) If $x,y,z\in X$, and $x&lt;y,\ 0&lt;z$ then $xz&lt;yz$</li>
</ol>
</li>
<li>We adopt the following notation:</li>
</ul>
<p>$$<br>X_+ :={x∈X :x≥0}\ and\ X_{++} :={x∈X :x&gt;0},<br>$$</p>
<p>$$<br>X_− :={x∈X :x≤0}\ and\ X_{−−} :={x∈X :x&lt;0}.<br>$$</p>
<p><strong>Absolute Value, The Triangle Inequality</strong>: </p>
<ul>
<li>Let $(X, +, ·, ≥)$ be an ordered field. </li>
<li>The function $|·| : X → X$ defined by</li>
</ul>
<p>$$<br>|x|:=\left{\begin{matrix}x,&amp;if\ x\geq0\-x,\ &amp;if\ x &lt;0\end{matrix}\right.<br>$$</p>
<ul>
<li>is called the <strong>absolute value</strong> function. </li>
<li>The following is called the <strong>triangle inequality</strong>:</li>
</ul>
<p>$$<br>|x+y|≤|x|+|y|\ \ \  for\ all\ x,y∈X.<br>$$</p>
<ul>
<li>It is valid within any ordered field.</li>
</ul>
<h2 id="Propositions-and-Theorems"><a href="#Propositions-and-Theorems" class="headerlink" title="Propositions and Theorems"></a>Propositions and Theorems</h2><p><strong>Proposition 2</strong>: </p>
<ul>
<li>Let $X$ and $Y$ be two nonempty sets. A function $f ∈ Y^X$ is invertible if, and only if, it is a bijection.</li>
</ul>
<p><strong>Proposition 3</strong>: </p>
<ul>
<li>Let $X$ and $Y$ be two nonempty sets. A function $f ∈ Y^X$ is invertible if, and only if, there exists a function $g∈X^Y$ such that $g◦f=\mbox{id}_X$ and $f◦g=\mbox{id}_Y$.</li>
</ul>
<p><strong>Proposition 6</strong>: </p>
<ul>
<li>(The Archimedean Property) For any $(a, b) ∈ \mathbb{R}_{++} × \mathbb{R}$, there exists an $m ∈\mathbb{N}$ such that $b &lt; ma$.</li>
<li>For any $a, b ∈ \mathbb{R}$ such that $a &lt; b$, there exists a $q ∈\mathbb{Q}$ such that $a &lt; q &lt; b$.</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li>Efe A. Ok, 2005, “Real Analysis with Economic Applications”;</li>
<li>John Boller and Paul J. Sally, Jr., “A Textbook for Advanced Calculus”;</li>
<li>Lecture notes and discussions at Piazza, Math 20300 51, Autumn 2020, Analysis in $\mathbb{R^n}$ 1;</li>
<li>Terence Tao, 2006, “Analysis”; </li>
<li>周民强，2008，“实变函数论”；</li>
<li>周民强，2007，“实变函数解题指南”；</li>
<li>周民强，2010，“数学分析习题演练”；</li>
<li>夏道行，吴卓人，严绍宗，舒五昌，2010，“实变函数论与泛函分析“；</li>
<li>张筑生，1990，”数学分析新讲“；</li>
</ol>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="hqin"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">hqin</p><p class="is-size-6 is-block">A Student in Economics</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Zhengzhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">53</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">17</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">5</p></a></div></div></nav></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:24:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Optimal-Control-The-Maximum-Principle/">Optimal Control: The Maximum Principle</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-C-Optimal-Control-Theory/">1.4.C Optimal Control Theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:23:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Constrained-Problems/">Constrained Problems</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:22:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Infinite-Planning-Horizon/">Infinite Planning Horizon</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:19:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Second-Order-Conditions/">Second-Order Conditions</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-01-03T14:17:22.000Z">2021-01-03</time></p><p class="title"><a href="/2021/01/03/Mathematics/Dynamic%20Optimization/2%20The%20Calculus%20of%20Variations/Transversality-Conditions-for-Variable-Endpoint-Problems/">Transversality Conditions for Variable-Endpoint Problems</a></p><p class="categories"><a href="/categories/1-Mathematics/">1 Mathematics</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/">1.4 Dynamic Optimization</a> / <a href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/">1.4.B The Calculus of Variations</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">September 2020</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li></ul></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://ygnmax.github.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Guangnan Yang</span></span><span class="level-right"><span class="level-item tag">ygnmax.github.io</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Abstract-Algebra/"><span class="tag">Abstract Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Control-Theory/"><span class="tag">Control Theory</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linear-Algebra/"><span class="tag">Linear Algebra</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Measure-Theoretic-Probability/"><span class="tag">Measure Theoretic Probability</span><span class="tag">20</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Real-Analysis/"><span class="tag">Real Analysis</span><span class="tag">24</span></a></div></div></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/1-Mathematics/"><span class="level-start"><span class="level-item">1 Mathematics</span></span><span class="level-end"><span class="level-item tag">53</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/"><span class="level-start"><span class="level-item">1.1 Analysis</span></span><span class="level-end"><span class="level-item tag">44</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-A-Preliminaries-of-Real-Analysis/"><span class="level-start"><span class="level-item">1.1.A Preliminaries of Real Analysis</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-B-Metric-Spaces-and-Continuity/"><span class="level-start"><span class="level-item">1.1.B Metric Spaces and Continuity</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-C-Linear-Spaces-and-Convexity/"><span class="level-start"><span class="level-item">1.1.C Linear Spaces and Convexity</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-D-Metric-Linear-Spaces-and-Normed-Linear-Spaces/"><span class="level-start"><span class="level-item">1.1.D Metric Linear Spaces and Normed Linear Spaces</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-E-Probability-via-Measure-Theory/"><span class="level-start"><span class="level-item">1.1.E Probability via Measure Theory</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-F-Expectation-Lebesgue-Integral-and-Stieltjes-Integral/"><span class="level-start"><span class="level-item">1.1.F Expectation, Lebesgue Integral, and Stieltjes Integral</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-G-Weak-Convergence-and-Probability-Limit/"><span class="level-start"><span class="level-item">1.1.G Weak Convergence and Probability Limit</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-1-Analysis/1-1-H-Stochastic-Independence-and-Dependence/"><span class="level-start"><span class="level-item">1.1.H Stochastic Independence and Dependence</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-2-Algebra/"><span class="level-start"><span class="level-item">1.2 Algebra</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-2-Algebra/1-2-A-Abstract-Algebra/"><span class="level-start"><span class="level-item">1.2.A Abstract Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-2-Algebra/1-2-B-Linear-Algebra/"><span class="level-start"><span class="level-item">1.2.B Linear Algebra</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/"><span class="level-start"><span class="level-item">1.4 Dynamic Optimization</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-A-Preliminaries-of-Dynamic-Optimization/"><span class="level-start"><span class="level-item">1.4.A Preliminaries of Dynamic Optimization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-B-The-Calculus-of-Variations/"><span class="level-start"><span class="level-item">1.4.B The Calculus of Variations</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/1-Mathematics/1-4-Dynamic-Optimization/1-4-C-Optimal-Control-Theory/"><span class="level-start"><span class="level-item">1.4.C Optimal Control Theory</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">hqin</a><p class="is-size-7"><span>&copy; 2020 - 2021 hqin</span>  </p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'folded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>